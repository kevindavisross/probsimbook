# The Language of Probability and Simulation {#simulation}

<!-- \newcommand{\IP}{\textrm{P}} -->

<!-- \newcommand{\IQ}{\textrm{Q}} -->

<!-- \newcommand{\E}{\textrm{E}} -->

<!-- \newcommand{\Var}{\textrm{Var}} -->

<!-- \newcommand{\SD}{\textrm{SD}} -->

<!-- \newcommand{\Cov}{\textrm{Cov}} -->

<!-- \newcommand{\Corr}{\textrm{Corr}} -->

<!-- \newcommand{\Xbar}{\bar{X}} -->

<!-- \newcommand{\Ybar}{\bar{X}} -->

<!-- \newcommand{\xbar}{\bar{x}} -->

<!-- \newcommand{\ybar}{\bar{y}} -->

<!-- \newcommand{\ind}{\textrm{I}} -->

<!-- \newcommand{\dd}{\text{DDWDDD}} -->

<!-- \newcommand{\ep}{\epsilon} -->

<!-- \newcommand{\reals}{\mathbb{R}} -->

A phenomenon is **random** if there are multiple potential *outcomes*, and there is *uncertainty* about which outcome will occur.
This chapter introduces the fundamental terminology and objects of random phenomena, including

-   Possible **outcomes** of the random phenomenon
-   Related **events** that could occur
-   **Random variables** which measure numeric quantities based on outcomes
-   **Probability measures** which assign likelihoods to events in a logically coherent way and reflect assumptions about the random phenomenon
-   **Distributions** of random variables which describe their pattern of variability, and can be summarized by **percentiles**, **expected values**, **standard deviations** (and **variances**), and **correlations** (and **covariances**).
-   **Conditioning**, which involves revising probabilities and distributions to reflect new information
-   **Probability models** which put it all together

A probability model of a random phenomenon consists of a sample space of possible outcomes, associated events and random variables, and a probability measure which specifies probabilities of events and determines distributions of random variables according to the assumptions of the model.
We will see techniques for computing probabilities, distributions, and related characteristics, but in many situations explicit computation is difficult.
In this chapter we will also introduce simulation, a powerful tool for investigating probability models and solving complex problems.

**Simulation** involves using a probability model to artificially recreate a random phenomenon, usually using a computer.
Given a probability model, we can simulate outcomes, occurrences of events, and values of random variables, according to the specifications of the probability measure.
Simulation can be used to approximate probabilities of events, distributions of random variables, long run averages, and other characteristics.

Throughout this chapter we will illustrate ideas with three examples.

```{example dice-sum-max-intro, name = "Total or best?"}

In many sports, a competitors' final ranking is based on the results of multiple attempts.
Competitors in Olympic bobsled, for example, make four separate timed runs on the same course and their ranking is based on their *total* time.
Competitors in Olympic shot put make six throws, but their ranking is based on their *best* throw.
In sports with multiple attempts, how do the rankings compare if they are based on the total (or average) over all attempts (as in bobsled) or on the best attempt (as is shot put)?

We'll start to investigate this idea with an overly simplified example: roll a four-sided^[Why four-sided?  Simply to make the number of possibilities a little more manageable (e.g., for in-class simulation activities).
Rolling a four-sided die twice yields 16 possible pairs, while rolling a six-sided die yields 36 possible pairs.] die twice and consider the sum and the larger of the two rolls (or the common roll if a die).

```

```{example matching-intro, name = "Matching problem"}
The "matching problem" is one well known probability problem.
The general setup involves $n$ distincts objects labeled $1, \ldots, n$ which are placed in $n$ distinct boxes labeled $1, \ldots, n$, with exactly one object placed in each box.
The problem appears in many contexts; the following version is from [FiveThirtyEight](https://fivethirtyeight.com/features/everythings-mixed-up-can-you-sort-it-all-out/).

A geology museum in California has $n$ different rocks sitting in a row on a shelf, with labels on the shelf telling what type of rock each is.
An earthquake hits and the rocks all fall off the shelf and get mixed up.
A janitor comes in and, wanting to clean the floor, puts the rocks back on the shelf in random order. 
We might be interested in things like whether all the rocks are put back in the correct spot, or how many are, or if the heaviest rock is put back in the correct spot.

```

```{example meeting-intro, name = "Meeting problem"}
Regina and Cady plan to meet for lunch but they are not sure of their arrival times.
We might be interested in questions involving whether they arrive within 15 minutes of one another, who arrives first and at what time, or how long the first person to arrive needs to wait for the second.

```

## Outcomes {#sec-language-outcomes}

Probability models can be applied to any situation in which there are multiple potential outcomes and there is uncertainty about which outcome will occur.
Due to the wide variety of types of random phenomena, an **outcome** can be virtually anything:

-   the result of a coin flip
-   the results of a sequence of coin flips
-   a shuffle of a deck of cards
-   the weather conditions tomorrow in your city
-   the path of a particular Atlantic hurricane
-   the daily closing price of a certain stock over the next 30 days
-   a noisy electric signal
-   the result of a diagnostic medical test
-   a sample of car insurance polices
-   the customers arriving at a store
-   the result of an election
-   the next World Series champion
-   a play in a basketball game

And on and on.
In particular, an outcome does *not* have to be a number.

Before the random phenomenon occurs it is unknown which outcome will be the result.
When the phenomenon takes place, a particular outcome is observed.
The first step in defining a probability model for a random phenomenon is to identify the *possible* outcomes.
The **sample space** is the collection of all possible outcomes of a random phenomenon.

In simple examples we can describe sample space by listing all possible outcomes.
However, constructing a list of all possible outcomes is rarely done in practice.
We do so here only to provide some concrete examples of sample spaces.
While a random phenomenon always has a corresponding sample space, in most situations the sample space of outcomes is at best only vaguely specified and can not be feasibly enumerated.

```{example dice-outcome}
Roll a four-sided die twice, and record the result of each roll in sequence.
For example, a 3 on the first roll and a 1 on the second is not the same outcome as a 1 on the first roll and a 3 on the second.

1.  Identify the sample space.

2.  We might be interested in the sum of the two rolls.
Explain why it is still advantageous to define the sample space as in the previous part, rather than as just $2, \ldots, 8$.
```

```{solution dice-outcome-sol}
to Example \@ref(exm:dice-outcome)

```

```{asis, fold.chunk = TRUE}

1. We simply enumerate all the possible outcomes: first roll is a 1 and second roll is a 1, first roll is a 1 and second roll is a 2, etc.
The sample space consists of 16 possible ordered pairs of rolls, which we can display in a list or table; see Table \@ref(tab:dice-outcome-sol-table)  

1. Yes, we might be interested in the sum of the two dice.
But we might also be interested in other things, like the larger of the two rolls, or if at least one 3 was rolled, or the result of the first roll.
Knowing just the sum of the rolls does not provide as much information about the outcome of the random phenomenon as the sequence of individual rolls does.

```

```{r dice-outcome-sol-table, echo = FALSE}
# | label: tbl-dice-outcome
# | echo: false
# | tbl-cap: "Table representing the sample space of two rolls of a four-sided die. Each row represents an outcome."

u1 = sort(rep(1:4, 4))
u2 = rep(1:4, 4)

kbl(
  data.frame(u1, u2),
  col.names = c("First roll", "Second roll"),
  caption = 'Table representing the sample space of two rolls of a four-sided die. Each row represents an outcome.'
) %>%
  kable_styling(fixed_thead = TRUE)

```

In the previous example, there was a single sample space whose outcomes represented the result of the pair of rolls.
In particular, there was not a separate sample space for each of the individual rolls.

Here's another concrete examples where we can list all the outcomes in the sample space.
However, keep in mind that enumerating the sample space is rarely done in practice.

```{example matching-outcome}

Consider the matching problem (Example \@ref(exm:dice-outcome)) with $n=4$.
Label the objects 1, 2, 3, 4, and the spots 1, 2, 3, 4, with spot 1 the correct spot for object 1, etc. 
Identify an appropriate sample space.

```

```{solution matching-outcome-sol}
to Example \@ref(exm:matching-outcome)

```

```{asis, fold.chunk = TRUE}


We can consider each outcome to be a particular placement of objects in the spots.
For example, object 3 is placed in spot 1, object 2 in spot 2, object 1 in spot 3, and object 4 in spot 4.
So the sample space consists of all the possible arrangments of the object labels into the 4 spots.
There are 24 outcomes^[There are 4 objects that could potentially go in spot 1, then 3 objects that could potentially go in spot 2, 2 to spot 3, and 1 left for spot 4.  This results in $4\times3\times2\times1=4! = 24$ possible outcomes.  We will see more counting rules later.]; see Table \@ref(tab:matching-outcome-table) 
Recording outcomes in this way provides more information than if we had chosen the sample space to correspond to, for example, the number of objects that were placed in the correct spot.

```

```{r matching-outcome-table, echo = FALSE}
# | label: tbl-matching-outcome
# | echo: false
# | tbl-cap: "Table representing the sample space in the matching problem. Each row represents an outcome."

n = 4

permutations(n , n,  1:n, repeats.allowed = FALSE) %>%
  as.data.frame() %>%
  kable(col.names = paste("Spot ", 1:n),
        caption = "Table representing the sample space in the matching problem. Each row represents an outcome.")

```

In the previous examples, the sample space was **discrete**, in the sense that the outcomes could be enumerated in a list (though it could be a very long list).
But in many cases, it is not possible to enumerate outcomes in a list, even in principle.

For example, consider the circular spinner (like from a kids game) in Figure \@ref(fig:uniform-spinner).
Imagine a needle anchored at the center of the circle which is spun and eventually lands pointing at a number on the outside of the circle.
The values in the picture are rounded to two decimal places, but consider an idealized model where the spinner is infinitely precise and the needle infinitely fine so that any real number between 0 and 1 is a possible outcome.
The sample space corresponding to a single spin of this spinner is the interval [0, 1].
There are uncountably many numbers in [0, 1] so it would not be possible to enumerate them in a list.
The interval [0, 1] is an example of a **continuous** sample space.

(ref:cap-uniform-spinner) A continuous [0, 1] spinner. The values in the picture are rounded to two decimal places, but in the idealized model the spinner is infinitely precise so that any real number between 0 and 1 is a possible outcome.

```{r uniform-spinner, echo=FALSE, fig.cap="(ref:cap-uniform-spinner)", fig.height=12, fig.align='center'}

knitr::include_graphics("_graphics/uniform-spinner.png")

```

```{example meeting-outcome}
Consider a version of the meeting problem (Example \@ref(exm:meeting-intro)) where Regina and Cady will definitely arrive between noon and 1, but their exact arrival times are uncertain.
Rather than dealing with clock time, it is helpful to represent noon as time 0 and measure time as minutes after noon, so that arrival times take values in the continuous interval [0, 60].

Describe an appropriate sample space.
Hint: it might be easiest to draw a picture.

```

```{solution meeting-outcome-sol}

to Example \@ref(exm:meeting-outcome)

```

```{asis, fold.chunk = TRUE}

We can represent an outcome as a (Regina, Cady) pair of arrival times, each in [0, 60].
For example, the outcome (30, 45) represents Regina arriving at 12:30 and Cady at 12:45, while (45, 0) represents Regina arriving at time (12:45) and Cady at noon.
The sample space is the set of all possible pairs.
We can visualize the sample space^[Mathematically we can write the sample space as $[0,60]\times [0,60]=[0,60]^2$, the Cartesian product $\{(x, y): x \in [0, 60], y \in [0, 60]\}$, the set of ordered pairs whose components take values in $[0, 60]$.] as the set of points within the colored square with [0, 60] sides in Figure \@ref(fig:meeting-outcome-plot).

```

(ref:cap-meeting-outcome) The square represents the sample space in Example \@ref(exm:meeting-outcome). Each point within the square is a (Regina, Cady) pair of arrival times in [0, 60].

```{r, echo = FALSE}
# | echo: false

unit_grid = seq(0, 1, 0.001) * 60

unit_box <- expand_grid(x = unit_grid, y = unit_grid)

waiting_corr = 0.7
waiting_sd = 10
waiting_mean = 30

dbvn <- function(x, y, mu_x, mu_y, sigma_x, sigma_y, rho) {
  z_x = (x - mu_x) / sigma_x
  z_y = (y - mu_y) / sigma_y
  exp(-0.5 * (z_x ** 2 + z_y ** 2 - 2 * rho * z_x * z_y) / (1 - rho ** 2)) /
    (2 * pi * sigma_x * sigma_y * sqrt(1 - rho ** 2))
}


unit_box_prob <- unit_box %>%
  mutate(independent_uniform = (1 / 60) ^ 2,
         independent_normal = dnorm(x, waiting_mean, waiting_sd) * dnorm(y, waiting_mean, waiting_sd),
         bivariate_normal = dbvn(x, y, waiting_mean, waiting_mean, waiting_sd, waiting_sd, waiting_corr))

unit_box_max_prob = unit_box_prob %>%
  dplyr::select(!(1:2)) %>%
  pivot_longer(cols = everything()) %>%
  summarize(max(value)) %>%
  pull()

```

```{r meeting-outcome-plot, echo = FALSE, warning = FALSE, message = FALSE, fig.cap="(ref:cap-meeting-outcome)"}
# | echo: false
# | label: fig-meeting-independent-uniform
# | fig-cap: "add"
# | fig-alt: "add"

ggplot(unit_box_prob,
       aes(x = x,
           y = y)) +
  geom_raster(aes(fill = independent_uniform), interpolate = TRUE) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_fill_viridis(limits = c(0, unit_box_max_prob)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)") +
  coord_fixed() +
  theme_classic() +
  theme(legend.position = "none")

```

In the previous example, outcomes were measured on a continuous scale; any real number between 0 and 60 was a possible arrival time.
In practice we might round the arrival time to the nearest minute or second, but in principle and with infinite precision any real number in the continuous interval $[0, 60]$ is possible.

Furthermore, even in situations where outcomes are inherently discrete, it is often more convenient to model them as continuous.
For example, if an outcome represents the annual salary in dollars of a randomly selected U.S. household, it would be more convenient to model the sample space as the continuous interval[^simulation-1] $[0, \infty)$ rather than discrete intervals like $\{0, 1, 2, \ldots\}$ or $\{0, 0.01, 0.02, \ldots\}$.
Continuous models are often more tractable mathematically than discrete models.

[^simulation-1]: We could also try $[0, m]$ where $m$ is some large dollar amount providing an upper bound on the maximum possible salary.
    But we would need to be sure that $m$ is large enough so that all possible outcomes are in the sample space $[0, m]$.
    Without knowing this bound in advance, it is convenient to just choose the unbounded interval $[0, \infty)$.
    There is really no harm in making the sample space bigger than it needs to be, but you can run into problems if you make it too small.

### Summary

The sample space is the set of all possible outcomes of a random phenomenon.
Outcomes can take a wide variety of forms.
In particular, outcomes do not need to be numbers.
Whenever possible, a sample space outcome should be defined to provide the maximum amount of information about the outcome of random phenomenon.

In practice we rarely enumerate the sample space as we did for some of the examples in this section.
Nonetheless, there is always some underlying sample space corresponding to all possible outcomes of the random phenomenon.
Even though the sample space often is at best vaguely defined (e.g., "tomorrow's weather conditions") and plays a background role, it is important to first consider what is *possible* before determining what is *probable*.
The sample space essentially defines the denominator in probability calculations.
Considering the sample space can help distinguish between "what is the probability this happens to me?" and "what is the probability this happens to someone somewhere sometime?" (as discussed in Section \@ref(probofwhat).)

## Events {#sec-language-events}

An event is something that might happen.
For example, if we're interested in the weather conditions in our city tomorrow, events include

-   the high temperature is 75°F
-   the high temperature is above 75°F
-   it rains
-   it does not rain
-   it rains and the high temperature is above 75°F

An outcome consists of all the information about tomorrow's weather conditions, while an *event* is a collection of outcomes that satisfy some criteria.\
The sample space is the collection of all possible outcomes; an event represents only those outcomes which satisfy some criteria.
Events are typically denoted with capital letters near the start of the alphabet, with or without subscripts (e.g. $A$, $B$, $C$, $A_1$, $A_2$).
Mathematically, events are sets, so events can be composed from others using [basic set operations](https://en.wikipedia.org/wiki/Set_(mathematics)#Basic_operations) like unions ($A\cup B$), intersections ($A \cap B$), and complements ($A^c$).

-   Complements. Read $A^c$ as "not $A$", the outcomes that do not satisfy $A$
-   Intersections. Read $A\cap B$ as "$A$ and $B$", the outcomes that satisfy both $A$ and $B$
-   Unions. Read $A \cup B$ as "$A$ or $B$", the outcomes that satify $A$ or $B$. Note that unions ($\cup$, "or") are always inclusive: $A\cup B$ occurs if $A$ occurs but $B$ does not, $B$ occurs but $A$ does not, or both $A$ and $B$ occur.

If the outcomes of a sample space are represented by rows in a table, then events are collections (or subsets) or rows which satisfy some criteria.

```{example dice-event}
Roll a four-sided die twice, and record the result of each roll in sequence.
Using the sample space from Example \@ref(exm:dice-outcome), identify the following events.

1. $A$, the event that the sum of the two dice is 4.
1. $B$, the event that the sum of the two dice is at most 3.
1. $C$, the event that the larger of the two rolls (or the common roll if a tie) is 3.
1. $A\cap C$ (identify and interpret).
1. $D$, the event that the first roll is a 3.
1. $E$, the event that the second roll is a 3.
1. $D \cap E$ (identify and interpret).
1. $D \cup E$ (identify and interpret).
1. If the outcome is $(1, 3)$, which of the events above occurred?
```

```{solution dice-event-sol}
to Example \@ref(exm:dice-event)
```

```{asis, fold.chunk = TRUE}

Remember that the sample space consists of 16 possible ordered pairs of rolls, (first roll, second roll); see \@ref(tab:dice-outcome-sol-table).
All events must be defined as subsets of this sample space.

1. $A$ consists of the outcomes (1, 3), (2, 2), and (3, 1).
Mathematically, event $A$ is the set $\{(1, 3), (2, 2), (3, 1)\}$.
This event is highlighted in \@ref(tab:dice-event-sol-table).
1. $B$ consists of the outcomes (1, 1), (1, 2), and (2, 1).
1. $C$ consists of the outcomes (1, 3), (2, 3), (3, 1), (3, 2), and (3, 3).
1. $A\cap C$, which consists of the outcomes (1, 3) and (3, 1), is the event that both the sum of the two dice is 4 and the larger of the two rolls is 3.
1. Each outcome in the sample space consists of a pair of rolls, so we must account for both rolls in defining events, even if the event of interest involves just the first roll.
(Remember, there is always a single sample space upon which all events are defined.)
So $D$ consists of the outcomes (3, 1), (3, 2), (3, 3), and (3, 4).
1. $E$ consists of the outcomes (1, 3), (2, 3), (3, 3), and (4, 3).
Note that this is not the same event as $D$.
1. $D \cap E$, which consists only of the outcome (3, 3), is the event that both rolls result in a 3.
While an event is always a set, it can be a set consisting of a single outcome (or the empty set).
1. $D \cup E$, which consists of the outcomes (3, 1), (3, 2), (3, 3), (3, 4), (1, 3), (2, 3), and (4, 3) is the event that at least one of the two rolls results in a 3.
Notice that the union is inclusive: $(3, 3)$, the outcome that satisfies both $D$ and $E$, is an element of $D\cup E$.
But also notice that the outcome $(3, 3)$ only appears once.
1. If the outcome is $(1, 3)$ then events $A$, $C$, $A\cap C$, $E$, $D\cup E$ all occur.
Events $B,$ $D$ and $D\cap E$ do not occur.
```

```{r dice-event-sol-table, echo = FALSE}
# | label: tbl-dice-event
# | echo: false
# | tbl-cap: "Table representing the sample space of two rolls of a four-sided die. The outcomes in orange comprise the event $A$, the sum is equal to 4."

u1 = sort(rep(1:4, 4))
u2 = rep(1:4, 4)
event_dice = if_else(u1 + u2 == 4, "yes", "no")

kbl(
  data.frame(u1, u2, event_dice),
  align = 'r',
  col.names = c("First roll", "Second roll", "Sum is 4?"),
  caption = 'Table representing the sample space of two rolls of a four-sided die. Each row represents an outcome.  The outcomes in orange comprise the event $A$, the sum is equal to 4.'
) %>%
  kable_styling(fixed_thead = TRUE) %>%
    row_spec(which(event_dice == "yes"), bold = T, color = "white", background = "#FFA500")

```

We reiterate (again!) that there is a single sample space, upon which all events are defined.
In the above example, events that involved only the first or second roll such as $D$ and $E$ were still defined in terms of pairs of rolls.
An outcome in a sample space should be defined to record as much information as possible so that the occurrence or non-occurrence of all events of interest can be determined.

Some events consist of a single outcome, or no outcomes at all ($\emptyset$).
Events $A_1, A_2. A_3, \ldots$ are *disjoint* (a.k.a. mutually exclusive) if they have no outcomes in common; that is, if $A_i \cap A_j = \emptyset$ for all $i\neq j$.
Roughly, disjoint events do not "overlap".
In the previous example, events $B$ and $C$ are disjoint; there are no outcomes for which both the sum of the dice is at most 3 and the larger roll is a 3.
(Mathematically, $B\cap C \ emptyset$, the empty set.)

```{example matching-event}
objects labeled 1, 2, 3, 4, are placed at random in spots labeled 1, 2, 3, 4, with spot 1 the correct spot for object 1, etc.
Using the sample space from Example \@ref(exm:matching-outcome), identify the following events.
```

1.  $A$, the event that all objects are put in the correct spot.
2.  $B$, the event that no objects are put in the correct spot.
3.  $C$, the event that exactly 3 objects are put in the correct spot.
4.  $A_3$, the event that object 3 is put (correctly) in spot 3.

```{solution matching-event-sol}
to Example \@ref(exm:matching-event)
```

```{asis, fold.chunk = TRUE}

Recall that each outcome is a particular placement of objects in the spots.
For example, the outcome (3, 2, 1, 4) --- which we'll shorten to 3214 --- signifies that object 3 is put in spot 1, object 2 in spot 2, object 1 in spot 3, and object 4 in spot 4.

1. There is only one outcome, 1234, for which all objects are put in the correct spot, so $A=\{1234\}$.
Remember that an event is always a set, but it can be a set consisting of a single outcome.
1. For each outcome in the sample space check to see if the criteria holds to identify $B=\{2143, 2341, 2413, 3142, 3412, 3421, 4123, 4312, 4321\}$ as the event that no objects are put in the correct spot.
1. There are no outcomes in which exactly 3 objects are put in the correct spot so $C=\emptyset$.
(If three objects are in their correct spots, then the remaining object must be in its correct spot too.)
1. $A_3=\{1234, 1432, 2134, 2431, 4132, 4231\}$ is the event that object 3 is put (correctly) in spot 3.
This event is highlighted in \@ref(tab:matching-event-table).
Even though event $A_3$ only concerns object 3, since the sample space consists of the placements of each of the objects then all events must be expressed in terms of these outcomes.


```

```{r matching-event-table, echo = FALSE}
# | label: tbl-matching-event
# | echo: false
# | tbl-cap: "Table representing the sample space in the matching problem. Each row represents an outcome."

n = 4

matching_event = permutations(n , n,  1:n, repeats.allowed = FALSE) %>%
  as.data.frame() %>%
  mutate(event_matching = if_else(V3 == 3, "yes", "no"))

matching_event %>%
  kbl(col.names = c(paste("Spot ", 1:n), "Object 3 in spot 3?"),
      align = 'r',
      caption = "Table representing the sample space in the matching problem. Each row represents an outcome.  The outcomes in orange comprise the event $A_3$, object 3 in spot 3.") %>%
  kable_styling(fixed_thead = TRUE) %>%
  row_spec(which(matching_event$event_matching == "yes"), bold = T, color = "white", background = "#FFA500")

```

When more than just a few events are of interest, subscripts are commonly used to identify different events.
In the previous example, we might also be interested in $A_1$, the event that object 1 is placed in spot 1; \$A_2, the event that object 2 is placed in spot 2; and so on.

Remember that intervals of real numbers such as $(a,b), [a,b], (a,b]$ are also sets, and so can also be events. For example, if an outcome is the result of a single spin of the spinner in \@ref(fig:uniform-spinner), events include

-   $[0, 0.5]$, the result is between 0 and 0.5 (the needle lands in the right half of the spinner)
-   $[0.75, 1]$, the result is between 0.75 and 1 (the needle lands in the northwest quarter of the spinner)
-   $[0.595, 0.605)$, the result rounded to two decimal places is 0.6
-   $\{0.6\}$, the result is 0.6 exactly (the needle points exactly at 0.60000000$\ldots$)

It is often helpful to conceptualize and visualize events (sets) with pictures, especially when dealing with continuous sample spaces.

```{example meeting-event}

Using the sample space from Example \@ref(exm:meeting-outcome), identify the following events using pictures.
(Hint: start with Figure \@ref(fig:meeting-outcome-plot).)

1. Identify $A$, the event that Regina arrives after Cady.
1. Identify $B$, the event that either Regina or Cady arrives before 12:30.
1. Identify $C$, the event that Cady arrives first and Regina arrives at most 15 minutes after Cady.
1. Identify $D$, the event that Regina arrives before 12:24.
```

```{solution meeting-event-sol}
to Example \@ref(exm:meeting-event)
```

(ref:cap-meeting-event-shade) The square represents the sample space in Example \@ref(exm:meeting-outcome). Each point within the square is a (Regina, Cady) pair of arrival times in [0, 60]. Events from \@ref(exm:meeting-event) are shaded in orange.

```{r meeting-event-fig, fold.chunk = TRUE, echo = FALSE, fig.cap = "(ref:cap-meeting-event-shade)"}
#| label: fig-uniform-event
#| echo: false

dfA <- data.frame(x = c(0, 1, 1, 1) * 60,
                 y = c(0, 0, 1, 1) * 60,
                 v = c(1, 1, 1, 1) * 60)

pA <- ggplot(data = dfA, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  labs(x = "Regina's arrival time",
       y = "Cady's arrival time") +
  ggtitle("Event A is shaded orange") +
  theme(plot.title = element_text(hjust = 0.5))


dfB <- data.frame(x = c(0.5, 1, 1, 0.5) * 60,
                  y = c(0.5, 0.5, 1, 1) * 60,
                  v = c(1, 1, 1, 1) * 60)

pB <- ggplot(data = dfB, aes(x = x, y = y)) +
  geom_polygon(fill = "white", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(panel.background = element_rect(fill = "orange",
                                    colour = "orange")) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  labs(x = "Regina's arrival time",
       y = "Cady's arrival time") +
  ggtitle("Event B is shaded orange") +
  theme(plot.title = element_text(hjust = 0.5))
  



dfC <- data.frame(x = c(0, 0.25, 1, 1) * 60,
                 y = c(0, 0, 0.75, 1) * 60,
                 v = c(1, 1, 1, 1) * 60)

pC <- ggplot(data = dfC, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  labs(x = "Regina's arrival time",
       y = "Cady's arrival time") +
  ggtitle("Event C is shaded orange") +
  theme(plot.title = element_text(hjust = 0.5))

dfD <- data.frame(x = c(0, 0.4, 0.4, 0) * 60,
                  y = c(0, 0, 1, 1) * 60,
                  v = c(1, 1, 1, 1) * 60)

pD <- ggplot(data = dfD, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  labs(x = "Regina's arrival time",
       y = "Cady's arrival time") +
  ggtitle("Event D is shaded orange") +
  theme(plot.title = element_text(hjust = 0.5))



ggarrange(pA, pB, pC, pD, ncol = 2, nrow = 2)

```

In the previous example, the sample space consists of (Regina, Cady) pairs of arrival times so any event must be expressed as a collection of pairs.
Even though the criteria for event $D$ involves only Regina's arrival time, the event is not simply [0, 24]; we need to consider all (Regina, Cady) pairs for which the Regina component is in the interval [0, 24].

In many situations it is not possible to explicitly define a sample space in a compact way, and so outcomes and events are often only vaguely defined.
Nevertheless, there is always a sample space in the background representing possible outcomes, and collections of these outcomes represent events of interest.

### Summary

-   The sample space is the set of all possible outcomes.
-   An event is a collection of outcomes that satisfy some particular criteria. That is, an event is a subset of the sample space,\
-   All events of interest are defined in terms of a single sample space.
-   An event can be a set consisting of a single outcome, or no outcomes at all (the empty set).
-   Events can be composed from others using basic set operations like intersections ($A \cap B$, "$A$ and $B$"), unions ($A \cup B$, "$A$ or $B$"), and complements ($A^c$, "not $A$")
-   Tables, lists, and pictures can be used to conceptualize and visualize events.

## Random variables {#rv}

Statisticians use the terms *observational unit* and *variable*.
Observational units are the people, places, things, etc., for which data is observed.
Variables are the measurements made on the observational units.
For example, the observational units in a study could be college students, while variables could be age, high school GPA, college GPA, SAT score, years in college, number of Statistics courses taken, etc.

In probability, an *outcome* of a random phenomenon plays a role analogous to an observational unit in statistics.
The sample space of outcomes is often only vaguely defined.
In many situations we are less interested in detailing the outcomes themselves and more interested in whether or not certain events occur, or with measurements that we can make for the outcomes.
For example, if the random phenomenon corresponds to randomly selecting a sample of students at a college, an outcome could be the list of students selected for the sample.
But we are less interested in who the students are, and more interested in questions which involve variables, such as: what is the distribution of SAT scores?
What is the relationship between high school GPA and college GPA?
What is the average number of years before graduation?
In probability, *random variables* play a role analogous to variables in statistics.

Roughly, a **random variable** assigns a number measuring some quantity of interest to each outcome of a random phenomenon.
For example, if we're interested in the weather conditions in our city tomorrow, random variables include

-   high temperature (°F)
-   amount of precipitation (inches)
-   Humidity (%)
-   Maximum wind speed (mph)

A random variable is variable in the sense that it can take different values (that is, it varies) and the value it takes is uncertainty (that is, random)

```{example dice-rv}
Roll a four-sided die twice, and record the result of each roll in sequence.
Recall the sample space from Example \@ref(exm:dice-outcome).
Let $X$ be the sum of the two dice, and let $Y$ be the larger of the two rolls (or the common value if both rolls are the same).  

```

1.  Construct a table identifying the values of $X$ and $Y$ for each outcome in the sample space. (Hint: add columns to Table \@ref(tab:dice-outcome-sol-table).)
2.  Identify the possible values of $X$.
3.  Identify the possible values of $Y$.
4.  Identify the possible values of the pair $(X, Y)$.

```{solution dice-rv-sol}
to Example \@ref(exm:dice-rv)
```

```{asis, fold.chunk = TRUE}


1. See Table \@ref(tab:dice-rv-sol-table).
The first column corresponds to sample space outcomes, and there is a column for each random variable. 
1. The possible values of $X$ are $2, \ldots, 8$
1. The possible values of $Y$ are $1, 2, 3, 4$
1. The possible values of the pair $(X, Y)$ are: (2, 1), (3, 2), (4, 2), (4, 3), (5, 3), (5, 4), (6, 3), (6, 4), (7, 4), (8,4).
Notice that while, for example, 8 is a possible value of $X$ and 1 is a possible value of $Y$, (8, 1) is not a possible value of the pair $(X, Y)$; it's not possible for the larger of the two dice to be 1 but their sum to be 8.

```

```{r, dice-rv-sol-table, echo = FALSE}
u1 = sort(rep(1:4, 4))
u2 = rep(1:4, 4)
x = u1 + u2
y = pmax(u1, u2)

kbl(
  data.frame(paste("(", u1, ", ", u2, ")", sep=""), x, y),
  col.names = c("Outcome (First roll, second roll)", "X (sum)", "Y (max)"),
  booktabs = TRUE,
  caption = 'Table representing the sum ($X$) and larger ($Y$) of two rolls of a four-sided die'
)   %>%
  kable_styling(fixed_thead = TRUE)

```

In statistics, data is often stored in a spreadsheet or data table with rows corresponding to observational units and columns to variables.
Likewise, in probability it helps to conceptualize or visualize a table with rows corresponding to outcomes and columns to random variables.
Each outcome is associated with a value of the random variable.
Since the outcome is uncertain, the value the random variable takes is also uncertain.
Mathematically, a random variable is a *function* that takes an outcome in the sample space as input and returns a number as output.

One of the main reasons for modeling a sample space as the set of possible outcomes rather than the set of all possible values of some random variable is that **we often want to define many random variables on the same sample space, and study relationships between them**.
As a statistics analogy, you would not be able to study the relationship between SAT scores and college GPA unless you measured both variables for the same set of students.

Random variables are typically denoted by capital letters near the end of the alphabet, with or without subscripts: e.g. $X$, $Y$, $Z$, or $X_1$, $X_2$, $X_3$, etc.
The random variable itself is typically denoted with a capital letter ($X$); possible values of that random variable are denoted with lower case letters ($x$).
Think of the capital letter $X$ as a label standing in for a formula like "the sum of two rolls of a four-sided die" and $x$ as a dummy variable standing in for a particular value like 3.

```{example matching-indicator}
Objects labeled 1, 2, 3, 4, are placed at random in spots labeled 1, 2, 3, 4, with spot 1 the correct spot for object 1, etc.
Recall the sample space from Example \@ref(exm:matching-outcome).
Let the random variable $X$ count the number of objects that are put back in the correct spot.
Let $I_1$ be equal to 1 if object 1 is placed (correctly) in spot 1, and define $I_2, I_3, I_4$ similarly.

```

1.  Construct a table identifying the value of $X, I_1, \ldots, I_4$ for each outcome in the sample space.
2.  Identify the possible values of $X$.
3.  What is the relationship between $I_3$ and event $A_3$ from Example \@ref(exm:matching-event)?
4.  How can you express $X$ is terms of $I_1, \ldots, I_4$?

```{solution matching-indicator-sol}
to Example \@ref(exm:matching-indicator)
```

```{asis, fold.chunk = TRUE}

1. See Table \@ref(tab:matching-indicator-tab).
Each random variable corresponds to a different column in the table.
1. $X$ can take values 0, 1, 2, and 4, but 3 is not a possible value of $X$.
1. $I_3$ is equal to 1 only for outcomes that satisfy $A_3$, the event that object 3 is placed in spot 3; $I_3$ is equal 0 for outcomes that do not satisfy event $A_3$.
In this way, the value of the random variable $I_3$ indicates whether or not the event $A_3$ occurs. 
1. For every outcome (row), the value of $X$ is equal to the sum of the values of $I_1$, $I_2$, $I_3$, $I_4$.
That is, $X = I_1+I_2+I_3+I_4$.
For example, for outcome 2134, $X$ is equal to 2, $I_1$ and $I_2$ are equal to 0, and $I_3$ and $I_4$ are equal to 1, and $2 = 0 + 0 + 1 + 1$.
The relationship $X = I_1+I_2+I_3+I_4$ is true for every outcome (row).
The spot-by-spot indicators provide a way to incrementally count the total number of matches.
```

```{r, matching-indicator-tab, echo = FALSE}
n = 4
temp = expand.grid(1:4, 1:4, 1:4, 1:4)
us <- temp[apply(temp, 1, function(x) {length(unique(x)) == 4}),]
us <- us %>% unite(u, 1:4, sep="", remove = FALSE) %>% arrange(u)

I1 = as.numeric(us[, 1 + 1] == 1)
I2 = as.numeric(us[, 2 + 1] == 2)
I3 = as.numeric(us[, 3 + 1] == 3)
I4 = as.numeric(us[, 4 + 1] == 4)
y = I1 + I2 + I3 + I4


kbl(
  data.frame(us[, 1], y, I1, I2, I3, I4),
  col.names = c("Outcome", "X", "I~1~", "I~2~", "I~3~",
                "I~4~"),
  booktabs = TRUE,
  caption = 'Indicators for each item and total number of matches in the matching problem.'
) %>%
  kable_styling(fixed_thead = TRUE)

```

Random variables that only take two possible values, 0 and 1, are called **indicator (or Bernoulli) random variables.** Indicators provide the bridge between events (sets) and random variables (functions).
A realization of any event is either true or false; the event either happens or it doesn't.
An indicator random variable just translates "true" or "false" into numbers, 1 for "true" and 0 for "false".

Even though they seem simple, indicator random variables are very useful.
In the matching problem, it is not feasible to enumerate the outcomes and count when there are a large number of items and spots.
Using indicators allows you to count incrementally --- is just this item in the correct spot?
--- rather than all at once.
Representing a count as a sum of indicator random variables is a very common and useful strategy, especially in problems that involve "find the expected number of..."

We are often interested in random variables that are derived from others.
For example, if the random variable $X$ represents the radius of a randomly selected circle, then $Y = \pi X^2$ is a random variable representing the circle's area.
If the random variables $W$ and $T$ represent the weight (kg) and height (m), respectively, of a randomly selected person, then $S = W / T^2$ is a random variable representing the person's body mass index ($\text{kg}/\text{m}^2$).

**A function of a random variable is also a random variable.** That is, if $X$ is a random variable and $g$ is a function, then $Y=g(X)$ is also a random variable.
For example, if $u$ is a radius of a circle, the function $g(u) = \pi u^2$ outputs its area; if $X$ is a random variable representing the radius of a randomly selected circle then $Y = g(X)=\pi X^2$ is a random variable representing the circle's area.

**Sums and products, etc., of random variables *defined on the same sample space* are random variables.** That is, if random variables $X$ and $Y$ are defined on the same sample space then $X+Y$, $X-Y$, $XY$, and $X/Y$ are also random variables.
Similarly, it is possible to make comparisons such as $X\ge Y$ and apply other transformations for random variables defined on the same sample space.

Remember that we can visualize outcomes as rows in a spreadsheet with random variables as columns.
Random variables defined on the same sample space can be put in a single spreadsheet.
Each row corresponds to an outcome, and reading across any row there is a value in the column corresponding to each random variable.
Random variables derived from transformations of other random variables append columns to the spreadsheet.
New random variables can be defined by going row-by-row, outcome-by-outcome, and applying a transformation within each row to the values of other random variables.

```{example dd-rv-versus-number}

Donny Don't is working on a problem that starts "let $X$ be a random variable representing tomorrow's high temperature in your city".
Donny says: "There is only one tomorrow and there will only be one high temperature tomorrow in my city.
Tomorrow's high temperature will just be a single number, there's nothing *variable* about it."
Explain to Donny what it means to say "tomorrow's high temperature is a random variable".

```

```{solution dd-rv-versus-number-sol}
to Example \@ref(exm:dd-rv-versus-number)
```

```{asis, fold.chunk = TRUE}

Yes, tomorrow's high temperatue will be a single number, but we do not know what that number will be.
Tomorrow's weather conditions are uncertain, that is, *random*.
Even if the forecast calls for a high of 75 degrees F, the high temperature could be 75 degrees, or 78 or 72 or 74, etc.
A random variable represents all the different possible values that tomorrow's high temperature *might* be depending on the uncertain weather conditions.

```

There are two main types of random variables.

-   **Discrete random variables** take at most countably many possible values (e.g., $0, 1, 2, \ldots$). They are often counting variables (e.g., the number of objects placed in the correct spot).
-   **Continuous random variables** can take any real value in some interval (e.g., $[0, 1]$, $[0,\infty)$, $(-\infty, \infty)$.). That is, continuous random variables can take uncountably many different values. Continuous random variables are often measurement variables (e.g., height, weight, income).

```{example meeting-rv}

Regina and Cady will definitely arrive between noon and 1, but their exact arrival times are uncertain.
Recall the sample space from Example \@ref(exm:meeting-outcome).
Let $R$ be the random variable representing Regina's arrival time (minutes after noon), and $Y$ for Cady.

```

1.  What does the random variable $T = \min(R, Y)$ represent? What are the possible values of $T$?
2.  What does the random variable $W = |R - Y|$ represent? What are the possible values of $W$?
3.  Let $N$ be the number of people who arrive before 12:30. How can you represent $N$ in terms of $R$ and $Y$. (Hint: use indicators.)
4.  Identify each of the random variables in this problem as discrete or continuous.

```{solution meeting-rv-sol}

to Example \@ref(exm:meeting-rv)

```

```{asis, fold.chunk = TRUE}

1. $T=\min(R, Y)$ represents the time (minutes after noon) of the first arrival.
$T$ takes values in $[0, 60]$.
If either Regina and Cady arrives at time 0 (noon) then $T$ is 0; if both arrive at 1:00 then $T$ is 60.
1. $W=|R-Y|$ represents the amount of time the first person to arrive waits for the second person to arrive.
$W$ takes values in $[0, 60]$.
If both Regina and Cady arrive at the same time then $W$ is 0; if one arrives at noon and the other at 1:00 then $W$ is 60.
1. Define an indicator for the event that Regina arrives before 12:30, $I_{R < 30}$; similarly, $I_{Y < 30}$ for Cady.
Then $N= I_{R < 30} + I_{Y < 30}$.
1. $R$, $Y$, $W$, $T$ are continuous.
$N$ and the indicators are discrete.
```

We are often interested in events which involve random variables.
In the weather example, the event "tomorrow's high temperature is above 75°F" involves the random variable "tomorrow's high temperature".
Each possible outcome of tomorrow's weather conditions will correspond to a value of high temperature, but only some of these outcomes will result in values of high temperature above 75 °F.

The expressions $X=x$ or $\{X=x\}$ are shorthand for the *event* that the random variable $X$ takes the value $x$.
Remember that any event is a collection of outcomes that satisfy some criteria, a subset of the sample space.
So objects like $\{X=x\}$ are sets representing the outcomes for which the value of the random variable $X$ is equal to the number $x$.
Remember to think of the capital letter $X$ as a label standing in for a formula like "the sum of two rolls of a four-sided die" and $x$ as a dummy variable standing in for a particular value like 3.

```{example dice-rv-event}
Roll a four-sided die twice, and record the result of each roll in sequence.
Recall the sample space from Example \@ref(exm:dice-outcome).
Let $X$ be the sum of the two dice, and let $Y$ be the larger of the two rolls (or the common value if both rolls are the same).
Identify and interpret each of the following.

```

1.  $\{X = 4\}$.
2.  $\{X = 3\}$.
3.  $\{X \le 3\}$.
4.  $\{Y = 4\}$.
5.  $\{Y = 3\}$.
6.  $\{Y \le 3\}$.
7.  $\{X = 4, Y = 3\}$ (that is, $\{X = 4\}\cap \{Y = 3\}$).
8.  $\{X = 4, Y \le 3\}$.
9.  $\{X = 3, Y = 3\}$.

```{solution dice-rvevent-sol}
to Example \@ref(exm:dice-rv-event)
```

```{asis, fold.chunk = TRUE}

Notice we encountered many of these events in Example \@ref(exm:dice-event), but now we are denoting the events in terms of random variables.

1. $\{X = 4\}$, which consists of the outcomes (1, 3), (2, 2), (3, 1), is the event that the sum of the two dice is 4.
Recalling Example \@ref(exm:dice-event), $A = \{X = 4\}$.
1. $\{X = 3\}$, which consists of outcomes (1, 2) and (2, 1), is the event that the sum of the two dice is 3.
1. $\{X \le 3\}$, which consists of outcomes (1, 1), (1, 2), and (2, 1), is the event that the sum of the two dice is at most 3.
Recalling Example \@ref(exm:dice-event), $B = \{X \le 3\}$.
1. $\{Y = 4\}=\{(1, 4), (2, 4), (3, 4), (4, 4), (4, 1), (4, 2), (4,3)\}$ is the event that the larger of the two rolls is 4.
1. $\{Y = 3\}=\{(1, 3), (2, 3), (3, 3), (3, 1), (3, 2)\}$ is the event that the larger of the two rolls is 3.
Recalling Example \@ref(exm:dice-event), $C = \{Y = 3\}$.
1. $\{Y \le 3\}=\{(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)\}$ is the event that the larger of the two rolls is at most 3.
Notice that since in this example $Y$ can only take values 1, 2, 3, 4, we have $\{Y\le 3\} = \{Y=4\}^c$.
1. $\{X = 4, Y = 3\} \equiv \{X = 4\}\cap \{Y = 3\}=\{(1, 3), (3, 1)\}$ is the event that both the sum of the two dice is 4 and the larger of the two rolls is 3.
Even though this involves two random variables, it is a single event (that is, a single subset of the sample space).
There are only two outcomes for which both the sum of the two dice is 4 and the larger of the two dice is 3.
1. $\{X = 4, Y \le 3\} \equiv \{X = 4\}\cap \{Y \le 3\}=\{(1, 3), (2, 2), (3, 1)\}$ is the event that both the sum of the two dice is 4 and the larger of the two rolls is at most 3.
Notice that since in this example $\{X=4\} \subset \{Y\le 3\}$, we have $\{X = 4, Y \le 3\} = \{X=4\}$.
If the sum is 4 we know the larger roll must be at most 3.
1. $\{X = 3, Y = 3\} \equiv \{X = 3\}\cap \{Y = 3\}=\emptyset$, since there are no outcomes for which both the sum is 3 and the larger of the two dice is 3. (If the the larger of the two dice is 3, then the sum must be at least 4.)

```

```{r, dice-rv-event-sol-table, echo = FALSE}
u1 = sort(rep(1:4, 4))
u2 = rep(1:4, 4)
x = u1 + u2
y = pmax(u1, u2)

kbl(
  data.frame(paste("(", u1, ", ", u2, ")", sep=""), x, y),
  col.names = c("Outcome (First roll, second roll)", "X (sum)", "Y (max)"),
  booktabs = TRUE,
  caption = 'Table representing the sum ($X$) and larger ($Y$) of two rolls of a four-sided die. The event $(X = 4)$ is highlighted in orange.'
)   %>%
  kable_styling(fixed_thead = TRUE) %>%
  row_spec(which(x == 4), bold = T, color = "white", background = "#FFA500")


```

When dealing with probabilities, it is common to write $X=3$ instead of $\{X=3\}$, and $X = 4, Y = 3$ instead of $\{X = 4\}\cap \{Y = 3\}$; read the comma in $X = 4, Y = 3$ as "and".
But keep in mind that an expression like "$X=3$" really represents an event $\{X=3\}$, a subset of outcomes of the sample space.

```{example meeting-rv-event}

Regina and Cady plan to meet for lunch between noon and 1 but they are not sure of their arrival times.
Recall the sample space from Example \@ref(exm:meeting-outcome).
Let $R$ be the random variable representing Regina's arrival time (minutes after noon), and $Y$ for Cady.
Interpret each of the following in words and draw a picture representing it.

```

1.  $\{R > Y\}$.
2.  $\{\min(R, Y) < 0.5\}$.
3.  $\{Y<R<Y+0.25\}$.
4.  $\{R < 0.4\}$.

```{solution meeting-rv-event-sol}

to Example \@ref(exm:meeting-rv-event)

```

```{asis, fold.chunk = TRUE}

The parts of this problem are almost identical to those in Example \@ref(exm:meeting-event).
The main difference is in notation; we are now denoting events in terms of random variables.

1.  See Figure \@ref(fig:uniform-rv-plot) for pictures. $\{R>Y\}$is the event that Regina arrives after Cady (event $A$ from Example \@ref(exm:meeting-event).)
1. $\{\min(R, Y)<0.5\}$, is the event that the earlier of the two arrival times is before 12:30 (event $B$ from Example \@ref(exm:meeting-event).)  This event  can also be written as $\{R < 0.5\}\cup \{Y < 0.5\}$, the event that either Regina or Cady arrives before 12:30.
1. $\{Y<R<Y+0.25\}$ is the event that Cady arrives first and Regina arrives at most 15 minutes after Cady (event $C$ from Example \@ref(exm:meeting-event).)
1. $\{R < 0.4\} = \{(\omega_1, \omega_2): \omega_1<0.4\}$ is the event that Regina arrives before 12:24 (event $D$ from Example \@ref(exm:meeting-event).)

```

(ref:cap-uniform-rv) Illustration of the events in Exercise \@ref(exm:meeting-rv-event). The square represents the possible values of $(R, Y)$, the random vector representing the arrival times of Regina and Cady.

```{r uniform-rv-plot, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-uniform-rv)"}

dfA <- data.frame(x = c(0, 1, 1, 1),
                 y = c(0, 0, 1, 1),
                 v = c(1, 1, 1, 1))

pA <- ggplot(data = dfA, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression(R ~ "(Regina)")) +
  ylab(expression(Y ~ "(Cady)")) +
  ggtitle("Event {R > Y} is shaded orange") +
  theme(plot.title = element_text(hjust = 0.5))


dfB <- data.frame(x = c(0.5, 1, 1, 0.5),
                  y = c(0.5, 0.5, 1, 1),
                  v = c(1, 1, 1, 1))

pB <- ggplot(data = dfB, aes(x = x, y = y)) +
  geom_polygon(fill = "white", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(panel.background = element_rect(fill = "orange",
                                    colour = "orange")) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression(R ~ "(Regina)")) +
  ylab(expression(Y ~ "(Cady)")) +
  ggtitle("Event {min(R, Y) < 0.5} is shaded orange") +
  theme(plot.title = element_text(hjust = 0.5))
  

# dfC <- data.frame(x = c(1, 1, 0.5, 1),
#                   y = c(0.5, 0.5, 1, 1),
#                   v = c(1, 1, 1, 1))
# 
# pC <- ggplot(data = dfC, aes(x = x, y = y)) +
#   geom_polygon(fill = "cornflowerblue", show.legend = FALSE) +
#   scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + 
#   scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
#   theme_classic() +
#   theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
#   theme(plot.margin=unit(c(1,1,1,1),"cm")) +
#   xlab(expression(omega[1] ~ "(Regina)")) +
#   ylab(expression(omega[2] ~ "(Cady)")) +
#   ggtitle("Event C is shaded") +
#   theme(plot.title = element_text(hjust = 0.5))


dfC <- data.frame(x = c(0, 0.25, 1, 1),
                 y = c(0, 0, 0.75, 1),
                 v = c(1, 1, 1, 1))

pC <- ggplot(data = dfC, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression(R ~ "(Regina)")) +
  ylab(expression(Y ~ "(Cady)")) +
  ggtitle("Event {Y<R<Y+0.25} is shaded orange") +
  theme(plot.title = element_text(hjust = 0.5))

dfD <- data.frame(x = c(0, 0.4, 0.4, 0),
                  y = c(0, 0, 1, 1),
                  v = c(1, 1, 1, 1))

pD <- ggplot(data = dfD, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression(R ~ "(Regina)")) +
  ylab(expression(Y ~ "(Cady)")) +
  ggtitle("Event {R < 0.4} is shaded orange") +
  theme(plot.title = element_text(hjust = 0.5))


library(ggpubr)
ggarrange(pA, pB, pC, pD, ncol = 2, nrow = 2)

```

Outcomes, events, and random variables are some of the main objects of probability.
While they are related, these are distinct objects.
Mathematically, an outcome is a point, an event is a set, and a random variable is a function.
As such, different operations are valid depending on what you're dealing with.
Don't confuse operations like $\cap$ that operate on sets (events, "and") with operations like $+$ that operate on numbers and functions (random variables, "plus" meaning addition).

```{example dd-events}

At various points in his homework, Donny Don't writes the following.
Explain to Donny why each of the following symbols is nonsense,  both mathematically and intuitively using a simple example (like tomorrow's weather).
Below, $A$ and $B$ represent events, $X$ and $Y$ represent random variables.

```

1.  $A = 0.5$
2.  $A + B$
3.  $X = A$
4.  $X + A$
5.  $X \cap Y$

```{solution dd-events-sol}
to Example \@ref(exm:dd-events)
```

```{asis, fold.chunk = TRUE}

We'll respond to Donny using tomorrow's weather as an example, with $A$ representing the event that it rains tomorrow, $X$ tomorrow's high temperature (degrees F), $B=\{X>80\}$ the event that tomorrow's high temperature is above 80 degrees, and $Y$ tomorrow's rainfall (inches).


1. $A$ is a set and 0.5 is a number; it doesn't make mathematical sense to equate them.  It doesn't make sense to say  "it rains tomorrow equals 0.5".
1. $A$ and $B$ are sets; it doesn't make mathematical sense to add them.  It doesn't make sense to say "the sum of (it rains tomorrow) and (tomorrow's high temperature is above 80 degrees F)".  If we want "(it rains tomorrow) OR (tomorrow's high temperature is above 80 degrees F)", then we need $A\cup B$.  Union is an operation on sets; addition is an operation on numbers.
1. $X$ is a random variable (a function) and $A$ is an event (a set), and it doesn't make sense to equate these two different mathematical objects.  Suppose that $X$ represents tomorrow's high temperate (degrees F). It doesn't make sense to say  "tomorrow's high temperature equals the event that it rains tomorrow".
1. $X$ is a random variable (a function) and $A$ is an event (a set), and it doesn't make sense to add these two different mathematical objects.  It doesn't make sense to say  "the sum of (tomorrow's high temperature) and  (the event that it rains tomorrow)".
1. $X$ and $Y$ are random variables (functions) and intersection is an operation on sets.   $X \cap Y$ is attempting to say "tomorrow's high temperature in degrees F and the amount of rainfall in inches tomorrow". If we're talking about a random vector containing these two variables, we would write $(X, Y)$ not $X \cap Y$. If we're interested in an event involving $X$ and $Y$, we're missing qualifying information to define a valid event. We could write $X >80, Y < 2$ or $\{X > 80\} \cap \{Y < 2\}$ to represent "the event that (tomorrow's high temperature is greater than 80 degrees F) AND (the amount of rainfall tomorrow is less than 2 inches)".

```

### Summary

-   Roughly, a random variable assigns a number measuring some quantity of interest to each outcome of a random phenomenon.
-   Mathematically, a random variable is a function whose input is a sample space outcome and whose output is a real number.
-   Many events of interest involve random variables.
-   A function of a random variable is also a random variable.
-   Sums, products, and other transformations of multiple random variables *defined on the same sample space* are random variables.
-   If the sample space outcomes are represented by rows in a spreadsheet, then random variables correspond to columns.

## Probability spaces {#probspace}

In the previous sections in the chapter we defined outcomes, events, and random variables, the main mathematical objects associated with a random phenomenon.
But we haven't actually computed any probabilities yet!
So far we have only been concerned with what is *possible*.
You might have noticed that the examples often did not include any assumptions like the "die is fair", "each object is equally likely to be put in any spot", or "Regina is more likely to arrive late and Cady is more likely to arrive early".
Now we will start to incorporate assumptions of the random phenomenon to determine how *probable* various events are.

A **probability measure**, typically denoted $\IP$, assigns probabilities to *events* to quantify their relative likelihoods according to the assumptions of the model of the random phenomenon.
The probability of event $A$ is denoted $\IP(A)$.
An event is something that *can* happen; $\IP(A)$ quantifies *how likely it is that* $A$ will happen.

As we saw in Section \@ref(consistency), there are some basic logical consistency requirements that probabilities must satisfy.
A valid probability measure $\IP$ must satisfy the following three "axioms".

-   For any event $A$, $0 \le \IP(A) \le 1$.
-   If $\Omega$ represents[^simulation-2] the sample space then $\IP(\Omega) = 1$.
-   (*Countable additivity.*) If $A_1, A_2, A_3, \ldots$ are disjoint[^simulation-3] events, then $$
    \IP(A_1 \cup A_2 \cup A_2 \cup \cdots) = \IP(A_1) + \IP(A_2) +\IP(A_3) + \cdots
    $$

[^simulation-2]: $\Omega$ is the uppercase Greek letter "Omega".
    There is no one set of universally agreed on notation, but $\Omega$ is commonly used to represent a sample space.
    It is also common practice to use uppercase and lowercase letters to denote different objects.
    The lowercase $\omega$ (lowercase Greek "omega") is typically used to denote a generic outcome in the sample space.
    That is, $\Omega$ represents the collection of all the possible outcomes, and $\omega$ represents a particular or generic single outcome.

[^simulation-3]: Recall that events $A_1, A_2. A_3, \ldots$ are *disjoint* (a.k.a. mutually exclusive) if they have no outcomes in common; that is, if $A_i \cap A_j = \emptyset$ for all $i\neq j$.
    Roughly, disjoint events do not "overlap".

The above axioms require that probabilities of different events must fit together in a valid, logically coherent way.

The requirement $0\le \IP(A)\le 1$ makes sense in light of the relative frequency interpretation: an event $A$ can not occur on more than 100% of repetitions or less than 0% of repetitions of the random phenomenon.

The requirement that $\IP(\Omega)=1$ just ensures that the sample space accounts for all of the possible outcomes.
Basically, $\IP(\Omega)=1$ says that on any repetition of the random phenomenon, "something has to happen".
Roughly, $\IP(\Omega)=1$ implies that all outcomes taken together need to account for 100% of the probability.
If $\IP(\Omega)$ were less than 1, then the sample space hasn't accounted for all of the possible outcomes.

Countable addivity says that as long as events share no outcomes in common, then the probability that at least one of the events occurs is equal to the sum of the probabilities of the individual events.
In Example \@ref(exm:worldseries), the events $A$="the Astros win the 2022 World Series" and $D$="the Dodgers win the 2022 World Series" are disjoint, $A\cap D = \emptyset$; in a single World Series, both teams cannot win.
If $\IP(A) = 0.2$ and $\IP(B) = 0.09$, then the probability of $A\cup D$, the event that either the Astros or the Dodgers win, must be $\IP(A\cup D)=0.29$.

Countable additivity can be understood through a diagram with areas representing probabilities, as in the figure below which represents two events (yellow / and blue \\).
On the left, there is no "overlap" between areas so the total area is the sum of the two pieces; this depicts countable additivity for two disjoint events.
On the right, there is overlap between the two areas, so simply adding the two areas "double counts" the intersection (green $\times$) and does not result in the correct total area.
Countable additivity applies to any *countable* number[^simulation-4] of events, as long as there is no "overlap".

[^simulation-4]: It's the *number of events* that must be countable.
    The events themselves can be uncountable sets like intervals.

(ref:cap-venn-disjoint) Illustration of countable additivity for two events. The events in the picture on the left are disjoint, but not on the right.

```{r venn-disjoint, echo=FALSE, fig.cap="(ref:cap-venn-disjoint)", fig.height=14, fig.align='center'}

knitr::include_graphics("_graphics/venn-disjoint.png")

```

Many other properties follow from the axioms, some of which we have already used intuitively when constructing two-way tables.

-   *Complement rule*[^simulation-5]. For any event $A$, $\IP(A^c) = 1 - \IP(A)$.
-   *Subset rule*[^simulation-6]. If $A \subseteq B$ then $\IP(A) \le \IP(B)$.
-   *Addition rule for two events*[^simulation-7]. If $A$ and $B$ are any two events \begin{align*}
    \IP(A\cup B) = \IP(A) + \IP(B) - \IP(A \cap B)
    \end{align*}
-   *Law of total probability*. If $C_1, C_2, C_3\ldots$ are disjoint events with $C_1\cup C_2 \cup C_3\cup \cdots =\Omega$, then \begin{align*}
    \IP(A) & = \IP(A \cap C_1) + \IP(A \cap C_2) + \IP(A \cap C_3) + \cdots
    \end{align*}

[^simulation-5]: Proof: Since $\Omega = A \cup A^c$ and $A$ and $A^c$ are disjoint the axioms imply that $1=\IP(\Omega) = \IP(A \cup A^c) = \IP(A) + \IP(A^c)$.

[^simulation-6]: Proof.
    If $A \subseteq B$ then $B = A \cup (B \cap A^c)$.
    Since $A$ and $(B \cap A^c)$ are disjoint, $\IP(B) = \IP(A) + \IP(B \cap A^c) \ge \IP(A)$.

[^simulation-7]: The proof is easiest to see by considering a picture like the one in Figure \@ref(fig:venn-disjoint) .

The main "meat" of the axioms is countable additivity.
Thus, the key to many proofs of probability properties is to write relevant events in terms of disjoint events.

```{example dd-union}
Donny Don't says: "Wait a minute. You said unions are inclusive; $\IP(A\cup B)$ means the probability of $A$ or $B$ OR BOTH.  So $\IP(A\cup B)$ should just be $\IP(A)+\IP(B)$."  Explain to Donny his mistake, using the picture on the right in Figure \@ref(fig:venn-disjoint) as an example.
```

```{solution dd-union-sol}
to Example \@ref(exm:dd-union).
```

```{asis, fold.chunk = TRUE}


$A\cup B$ is inclusive so we *do* want to count the possibility of both, $A\cap B$.  The problem with simply adding $\IP(A)$ and $\IP(B)$ is that their sum *double counts* $A \cap B$.  We do want to count the outcomes that satisfy both $A$ and $B$, but *we only want to count them once*.  Subtracting $\IP(A \cap B)$ in the general addition rule for two events corrects for the double counting.

For example, consider the picture on the right in Figure \@ref(fig:venn-disjoint).  Suppose each rectangular cell represents a distinct outcome; there are 16 outcomes in total.  Assume the outcomes are equally likely, each with probability $1/16$.  Let $A$ represent the yellow / event which has probability $4/16$ and let $B$ represent the  blue \\ event which has probability 4/16.
(Remember, green represents outcomes that satisfy both blue and yellow.)
Then $\IP(A\cup B) = 6/16$, since there are 6 outcomes which satisfy either event $A$ or $B$ (or both).  However, simply adding $\IP(A)+\IP(B)$ yields $8/16$ because the two outcomes that satisfy the green event $A\cap B$ are counted both in $\IP(A)$ and $\IP(B)$.  So to correct for this double counting, we subtract out $\IP(A\cap B)$:
\[
\IP(A)+\IP(B)-\IP(A\cap B) = 4/16 + 4/16 -2/16 = 6/16 = \IP(A\cup B)
\]

```

Warning: The general addition rule for more than two events is more complicated[^simulation-8]; see [the inclusion-exclusion principle](https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle#In_probability).

[^simulation-8]: For three events, \begin{align*}
    \IP(A\cup B\cup C) & = \IP(A) + \IP(B) + \IP(C)\\
    & \qquad - \IP(A\cap B) - \IP(A \cap C) - \IP(B \cap C)\\
    & \qquad + \IP(A \cap B \cap C).
    \end{align*}

The complement rule follows from the fact that an event either happens or it doesn't.
We'll see that it is sometimes more convenient to compute directly the probability that an event does not happen and then use the complement rule.
Subtracting a computed probability from 1 seems like a small computational step, but it's an important one.
A basketball player who has a 90% chance of successfully making a free throw is much different from a player who only has a 10% chance.
Unfortunately, the complement rule step is often overlooked when doing probability calculations.
It's a good idea to ask yourself if the probability you are computing should be greater than or less than 50%.
If your computed value seems to be on the wrong side of 50%, check your calculations to see if you have forgotten (or misapplied) the complement rule.

The complement rule is often useful in probability problems that involve finding "the probability of at least one...," which on the surface involves unions (OR).
However, the general addition rule for multiple events is complicated and not very useful.
Therefore, it usually more convenient to use the complement rule and compute "the probability of at least one..." as one minus "the probability of none..."; the latter probability involves intersections (AND).
We will see more about probabilities of intersections later.

In the law of total probability the events $C_1, C_2, C_3, \ldots$, which represent "cases", form a *partition* of the sample space; each outcome in the sample space satisfies exactly one of the cases $C_i$.
The law of total probability says that we can compute the "overall" probability $\IP(A)$ by summing the probabilities of $A$ in each "case" $\IP(A\cap C_i)$.
The law of total probability basically says that we can sum across rows and columns in two-way tables.
(Later we will see a different and more useful expression of the law of total probability, involving conditional probabilities.)

The following example is one we have basically covered before, but now we introduce some more mathematical notation.
The example involves randomly selecting a U.S. household.
Note that while "randomly select" is commonly used terminology, it is not the best wording.
Remember that "random" simply means uncertain, so technically "randomly select" just means selecting in a way that the outcome is uncertain.
Suppose I want to "randomly select" one of two households, A or B.
I could put 10 tickets in a hat, with 9 labeled A and 1 labeled B, and then draw a ticket; this is random selection because the outcome of the draw is uncertain.
However, what is often meant by "randomly select" is selecting in a way that each outcome is equally likely.
To give households A and B the same chance of being selected, I would put a single ticket for each in the hat.
Randomly selecting in a way that each outcome is equally likely could be described more precisely as "selecting uniformly at random".
(We will discuss equally likely outcomes in more detail later.)

```{example largest-smallest-prob}
The probability that a randomly selected U.S. household has a pet dog is 0.47. The probability that a randomly selected U.S. household has a pet cat is 0.25. (These values are based on the 2018 [General Social Survey (GSS)](https://gss.norc.org/).)
```

1.  Represent the information provided using proper symbols.
2.  Donny Don't says: "the probability that a randomly selected U.S. household has a pet dog OR a pet cat is $0.47 + 0.25=0.72$." Do you agree? What must be true for Donny to be correct? Explain.
3.  What is the *largest possible* value of the probability that a randomly selected U.S. household has a pet dog AND a pet cat? Describe the (unrealistic) situation in which this extreme case would occur. (Hint: for the remaining parts it helps to consider two-way tables.)
4.  What is the *smallest possible* value of the probability that a randomly selected U.S. household has a pet dog AND a pet cat? Describe the (unrealistic) situation in which this extreme case would occur.
5.  Donny Don't says: "I remember hearing once that in probability OR means add and AND means multiply. So the probability that a randomly selected U.S. household has a pet dog AND a pet cat is $0.47 \times 0.25=0.1175$." Do you agree? Explain.
6.  According to the GSS, the probability that a randomly selected U.S. household has a pet dog AND a pet cat is $0.15$. Compute the probability that a randomly selected U.S. household has a pet dog OR a pet cat.
7.  Compute and interpret $\IP(C \cap D^c)$.

```{solution largest-smallest-prob-sol}
to Example \@ref(exm:largest-smallest-prob).
```

```{asis, fold.chunk = TRUE}

This is basically just Example \@ref(exm:cats-dogs) again.
Here we introduce more mathematical notation, but the ideas are the same as we discussed in Example \@ref(exm:cats-dogs).



1. The sample space consists of U.S. households.
Let $C$ be the event that the household has a pet cat, and let $D$ be the event that the household has a pet dog.
Let $\IP$ be the probability measure corresponding to randomly selecting a U.S. household. (The probability measure corresponds to however the random selection is done; though not specified, it's assumed to be uniformly at random.)
Then $\IP(C) = 0.25$ and $\IP(D) = 0.47$.
1. Donny would be correct if the events $C$ and $D$ were disjoint, which would only be true if the probability that a randomly selected U.S. household has a pet dog AND a pet cat were 0.  This is unrealistic, since I'm sure you know households (maybe even your own!) that have both pet cats and dogs.
1. $\IP(C \cup D) = \IP(C) + \IP(D) - \IP(C \cap D) = 0.25 + 0.42 - \IP(C\cap D)$.
So $\IP(C\cup D)$ is the largest it can be when $\IP(C \cap D)$ is the smallest it can be.
The smallest $\IP(C \cap D)$ can be is 0, and hence the largest $\IP(C\cup D)$ can be is 0.72, which would only be true if no households had both a pet cat and a pet dog.
The following two-way table of percents represents this unrealistic scenario.   

    |           |  D | not D | Total |
    |-----------|---:|------:|------:|
    | **C**     |  0 |    25 |    25 |
    | **not C** | 47 |    28 |    75 |
    | **Total** | 47 |    53 |   100 |

1. $\IP(C \cup D) = \IP(C) + \IP(D) - \IP(C \cap D) = 0.25 + 0.42 - \IP(C\cap D)$. $\IP(C\cup D)$ is the smallest it can be when $\IP(C \cap D)$ is the largest it can be.  The probability that the household has both a pet cat and a pet dog can not be larger than either of the two component probabilities; that is $\IP(C\cap D)\le \IP(C) = 0.25$ and $\IP(C\cap D)\le \IP(D) = 0.42$. The largest $\IP(C \cap D)$ can be is 0.25, and hence the smallest $\IP(C\cup D)$ can be is 0.42, which would only be true if every household that has a pet cat also has a pet dog. The following two-way table of percents represents this unrealistic scenario.   

    |           |  D | not D | Total |
    |-----------|---:|------:|------:|
    | **C**     | 25 |     0 |    25 |
    | **not C** | 22 |    53 |    75 |
    | **Total** | 47 |    53 |   100 |

1. Tell Donny to check the axioms of probability.  There is no requirement that the probability of an intersection must be the product of the probabilities. The two previous parts show that $0\le \IP(C \cap D) \le 0.25$, but without further information we can't determine the value of $\IP(C\cap D)$. It helps to think it in percentage terms. The extreme of 0 occurs when 0\% of households with a pet cat also have a pet dog; the extreme of 0.25 occurs when 100\% of households with a pet cat also have a pet dog.  We might expect that that the true value of $\IP(C \cap D)$ depends on the actual percentage of households with a pet cat that also have a pet dog.  Without knowing that percentage (or equivalent information), we cannot determine $\IP(C \cap D)$.  (We will explore this topic in more depth later.)
1. $\IP(C \cup D) = \IP(C) + \IP(D) - \IP(C \cap D) = 0.25 + 0.42 - 0.15 = 0.52$. Notice that this is between the hypothetical extremes of 0.42 and 0.72. Also notice that the actual $\IP(C \cap D)$ is between the hypothetical extremes of 0 and 0.25, but it is not equal to the product of 0.25 and 0.42. The moral is that we are not able to compute probabilities involving both events ($\IP(C\cup D)$, $\IP(C \cap D$)) based on the probability of each event alone. The following two-way table of percents represents the actual scenario.   

    |           |  D | not D | Total |
    |-----------|---:|------:|------:|
    | **C**     | 15 |    10 |    25 |
    | **not C** | 32 |    43 |    75 |
    | **Total** | 47 |    53 |   100 |

1. The law of total probability implies that $\IP(C) = \IP(C \cap D) + \IP(C \cap D^c)$ so $\IP(C \cap D^c) = 0.11$.
An adult either has a dog or not (the two cases); if 14% of adults have both a cat and a dog and 11% of adults have a cat but no dog, then 25% of adults have a cat.

```

Probabilities involving multiple events, such as $\IP(A \cap B)$ or $\IP(X>80, Y<2)$, are often called **joint probabilities**.
Note that the axioms do not specify any direct requirements on probabilities of intersections.
In particular, is not necessarily true that $\IP(A\cap B)$ equals $\IP(A)\IP(B)$.
It is true that probabilities of intersections can be obtained by multiplying, but the product generally involves at least one *conditional probability* that reflects any association between the events involved.
In general, joint probabilities ($\IP(A \cap B)$) can not be computed based on the individual probabilities ($\IP(A)$, $\IP(B)$) alone.
We will explore this topic in more depth later.

```{example linda}
Consider a Cal Poly student who frequently has blurry, bloodshot eyes, generally exhibits slow reaction time, always seems to have the munchies, and disappears at 4:20 each day. Which of the following events, $A$ or $B$, has a higher probability? (Assume the two probabilities are not equal.)

- $A$: The student has a GPA above 3.0.
- $B$: The student has a GPA above 3.0 and smokes marijuana regularly.

```

```{solution linda-sol}
to Example \@ref(exm:linda).
```

```{asis, fold.chunk = TRUE}

$A$ has the *higher* probability.
Many people say $B$, associating the description of the student with the "smokes marijuana regularly" part of event $B$.
But every student who satisfies event $B$ also satisfies event $A$, so $\IP(A)$ can't be any smaller than $\IP(B)$.
That is, $B\subseteq A$ so $\IP(B) \le \IP(A)$. 

```

**Warning!** Your psychological judgment of probabilities is often inconsistent with the mathematical logic of probabilities.

Probabilities are always defined for events (sets) but remember than many events are defined in terms of random variables.
For example, if $X$ is tomorrow's high temperature (degrees F) we might be interested in $\IP(\{X>80\})$, the probability of the event that tomorrow's high temperature is above 80 degrees F.
If $Y$ is the amount of rainfall tomorrow (inches) we might be interested in $\IP(\{X > 80\}\cap \{Y < 2\})$, the probability of the event that tomorrow's high temperature is above 80 degrees F and the amount of rainfall is less than 2 inches.
To simplify notation, it is common to write $\IP(X>80)$ instead of $\IP(\{X>80\})$, or $\IP(X > 80, Y < 2)$ instead of $\IP(\{X > 80\}\cap \{Y < 2\})$.
Read the comma in $\IP(X > 80, Y < 2)$ as "and".
But keep in mind that an expression like "$X>80$" really represents an event $\{X>80\}$.

A **probability model** (or **probability space**) puts all the objects we have seen so far in this chapter together in a model for the random phenomenon.
Think of a probability space as the collection of all outcomes, events, and random variables associated with a random phenomenon along with the probabilities of all events of interest under the assumptions of the model.

Before we work with some numerical probabilities, let's pause briefly to think about some of the concepts we have seen so far.
It's easy to get confused between things like events, random variables, and probabilities, and the symbols that represent them.
But a strong understanding of these fundamental concepts will help you solve probability problems.
Examples like the following do more than encourage proper use of notation.
Explaining to Donny why he is wrong will help you better understand the objects that symbols represent, how they are different from one another, and how they connect to real-world contexts.

```{example dd-notation, name='(ref:ddwddd)'}

At various points in his homework, Donny Don't writes the following.  Explain to Donny why each of the following symbols is nonsense,  both mathematically and intuitively using a simple example (like tomorrow's weather).  Below, $A$ and $B$ represent events, $X$ and $Y$ represent random variables.

```

1.  $\IP(A = 0.5)$
2.  $\IP(A + B)$
3.  $\IP(A) \cup \IP(B)$
4.  $\IP(X)$
5.  $\IP(X = A)$
6.  $\IP(X \cap Y)$

```{solution dd-notation-sol}
to Example \@ref(exm:dd-notation)
```

```{asis, fold.chunk = TRUE}

We'll respond to Donny using tomorrow's weather as an example, with $A$ representing the event that it rains tomorrow, $X$ tomorrow's high temperature (degrees F), $B=\{X>80\}$ the event that tomorrow's high temperature is above 80 degrees, and $Y$ tomorrow's rainfall (inches).

1. $A$ is a set and 0.5 is a number; it doesn't make mathematical sense to equate them.  It doesn't make sense to say  "it rains tomorrow equals 0.5".  Donny probably means "the probability that it rains tomorrow equals 0.5" which he should write as $\IP(A) = 0.5$.
1. $A$ and $B$ are sets; it doesn't make mathematical sense to add them. The symbol $A + B$ would represent “it rains tomorrow *plus* tomorrow’s high temperature is above 80 degrees F,” where "plus" literally means the mathematical sum.  Donny might mean “the probability that (it rains tomorrow) *or* (tomorrow’s high temperature is above 80 degrees),” which he should write as $\IP(A \cup B)$.  Donny might have meant to write $\IP(A) + \IP(B)$, which is valid expression since $\IP(A)$ and $\IP(B)$ are numbers. However, he should keep in mind that $\IP(A) + \IP(B)$ is not necessarily a probability of anything; this sum could even be greater than one.  In particular, since there are some rainy days with high temperatures above 80 degrees --- that is, $A$ and $B$ are not disjoint --- $\IP(A) + \IP(B)$ is greater than $\IP(A\cup B)$.  (See the general addition rule and related discussion in Section \@ref(propprob).)
Donny might also mean “the probability that (it rains tomorrow) *and* (tomorrow’s high temperature is above 80 degrees),” which he should write as $\IP(A \cap B)$.
1. $\IP(A)$ and $\IP(B)$ are numbers; union is an operation on sets, and it doesn't make mathematical sense to take a union of numbers.  See the previous part for related discussion. 
1. $X$ is a random variable, and probabilities are assigned to events.  $P(X)$ reads "the probability that tomorrow's high temperature in degrees F", a subject in need of a predicate; the phrase is missing any qualifying information that could define an event.  We assign probabilities to things that might happen (events) like "tomorrow's high temperature is above 80 degrees," which has probability $\IP(X > 80)$.
1. $X$ is a random variable (a function) and $A$ is an event (a set), and it doesn't make sense to equate these two different mathematical objects.  It doesn't make sense to say "tomorrow's high temperature in degrees F equals the event that it rains tomorrow".  We're not sure what Donny was thinking here.
1. $X$ and $Y$ are RVs (functions) and intersection is an operation on sets.  $X \cap Y$ is attempting to say "tomorrow's high temperature in degrees F and the amount of rainfall in inches tomorrow", but this is still missing qualifying information to define a valid event for which a probability can be assigned.  We could say $\IP(X > 80, Y < 2)$ to represent "the probability that (tomorrow's high temperature is greater than 80 degrees F) AND (the amount of rainfall tomorrow is less than 2 inches)". (Remember,"$X > 80, Y < 2$" is short for the event $\{X > 80\} \cap \{Y < 2\}$.)
If we want to say something like "we measure tomorrow's high temperature in degrees F and the amount of rainfall in inches tomorrow" we would write $(X, Y)$.

```

### Some probability measures for a four-sided die

The three axioms (and related properties) of a probability measure are simply minimal logical consistency requirements that must be satisfied by any probability model to ensure that probabilities fit together in a coherent way.
There are also many physical aspects of the random phenomenon or assumptions (e.g. "fairness", independence, conditional relationships) that must be considered when determining a reasonable probability measure for a particular situation.
Sometimes $\IP(A)$ is defined explicitly for an event $A$ via a formula.
But it is much more common for a probability measure to be defined only implicitly through modeling assumptions; probabilities of events then follow from the axioms and related properties.

Consider a *single* roll of a four-sided die.
Careful: other examples in this chapter have involved a pair of rolls, but here we are just considering one roll.
The sample space consists of four possible outcomes $\Omega = \{1, 2, 3, 4\}$.
Events concern what might happen on a single roll.
For example, if $A$ is the event that we roll an odd number then $A = \{1, 3\}$, consisting of the outcomes 1 and 3.

Let's first assume that the die is fair, so all four outcomes are equally likely, each with probability[^simulation-9] 1/4.
Given that the probability of each outcome[^simulation-10] is 1/4, countable additivity implie
s

[^simulation-9]: That the probability of each outcome must be 1/4 when there are four *equally likely* outcomes follows from the axioms, by writing $\{1, 2, 3, 4\} = \{1\}\cup\{2\}\cup \{3\}\cup \{4\}$, a union of disjoint sets, and applying countable additivity and $\IP(\Omega)=1$.

[^simulation-10]: Probabilities are always defined for events (sets).
    When we say loosely "the probability of an outcome $\omega$'' we really mean the probability of the event consisting of the single outcome $\{\omega\}$. In this example $\IP(\{1\})=\IP(\{2\})=\IP(\{3\})=\IP(\{4\})=1/4$.

$$
\IP(A) = \frac{\text{number of elements in $A$}}{4}, \qquad{\text{$\IP$ assumes a fair four-sided die}}
$$

For example, if $A$ is the event we roll an odd number, which occurs if we roll a 1 or a 3, then $\IP(A) = 2/4$.

Table \@ref(tab:die-events-fair) lists all the possible events, and their probabilities according to the probability measure $\IP$.

| Event            | Description                                | Probability of event assuming equally likely outcomes |
|------------------------|------------------------|------------------------|
| $\emptyset$      | Roll nothing (not possible)                | 0                                                     |
| $\{1\}$          | Roll a 1                                   | 1/4                                                   |
| $\{2\}$          | Roll a 2                                   | 1/4                                                   |
| $\{3\}$          | Roll a 3                                   | 1/4                                                   |
| $\{4\}$          | Roll a 4                                   | 1/4                                                   |
| $\{1, 2\}$       | Roll a 1 or a 2                            | 2/4                                                   |
| $\{1, 3\}$       | Roll a 1 or a 3                            | 2/4                                                   |
| $\{1, 4\}$       | Roll a 1 or a 4                            | 2/4                                                   |
| $\{2, 3\}$       | Roll a 2 or a 3                            | 2/4                                                   |
| $\{2, 4\}$       | Roll a 2 or a 4                            | 2/4                                                   |
| $\{3, 4\}$       | Roll a 3 or a 4                            | 2/4                                                   |
| $\{1, 2, 3\}$    | Roll a 1, 2, or 3 (a.k.a. do not roll a 4) | 3/4                                                   |
| $\{1, 2, 4\}$    | Roll a 1, 2, or 4 (a.k.a. do not roll a 3) | 3/4                                                   |
| $\{1, 3, 4\}$    | Roll a 1, 3, or 4 (a.k.a. do not roll a 2) | 3/4                                                   |
| $\{2, 3, 4\}$    | Roll a 2, 3, or 4 (a.k.a. do not roll a 1) | 3/4                                                   |
| $\{1, 2, 3, 4\}$ | Roll something                             | 1                                                     |

: (#tab:die-events-fair) All possible events associated with a single roll of a four-sided die, and their probabilities assuming the die is fair.

The above assignment satisfies all the axioms and so it represents a valid probability measure.
But assuming that the outcomes are equally likely is a much stricter assumption than the basic logical consistency requirements of the axioms.
There are many other possible probability measures, like in the following.

```{example die-weighted}
Now consider a single roll of a four-sided die, but suppose the die is weighted so that the outcomes are no longer equally likely. Suppose that the probability of event $\{2, 3\}$ is 0.5, of event $\{3, 4\}$ is 0.7, and of event $\{1, 2, 3\}$ is 0.6.  Complete a table, like Table \@ref(tab:die-events-fair), listing the probability of each event for this particular weighted die.  In what particular way is the die weighted?  That is, what is the probability of each the four possible outcomes?

```

```{solution die-weighted-sol}
to Example \@ref(exm:die-weighted)

```

```{asis, fold.chunk = TRUE}

Since the probability of not rolling a 4 is 0.6, the probability of rolling a 4 must be 0.4.  Since $\{3, 4\} = \{3\} \cup \{4\}$, a union of disjoint sets, the probability of rolling a 3 must be 0.3.  Similarly, the probability of rolling a 2 must be 0.2, and  the probability of rolling a 1 must be 0.1.  From there we can find the probabilities of all possible events for this particular weighted die, displayed in Table \@ref(tab:die-events-weighted).
```

| Event            | Description                                | Probability of event assuming a particular weighted die |
|------------------------|------------------------|------------------------|
| $\emptyset$      | Roll nothing (not possible)                | 0                                                       |
| $\{1\}$          | Roll a 1                                   | 0.1                                                     |
| $\{2\}$          | Roll a 2                                   | 0.2                                                     |
| $\{3\}$          | Roll a 3                                   | 0.3                                                     |
| $\{4\}$          | Roll a 4                                   | 0.4                                                     |
| $\{1, 2\}$       | Roll a 1 or a 2                            | 0.3                                                     |
| $\{1, 3\}$       | Roll a 1 or a 3                            | 0.4                                                     |
| $\{1, 4\}$       | Roll a 1 or a 4                            | 0.5                                                     |
| $\{2, 3\}$       | Roll a 2 or a 3                            | 0.5                                                     |
| $\{2, 4\}$       | Roll a 2 or a 4                            | 0.6                                                     |
| $\{3, 4\}$       | Roll a 3 or a 4                            | 0.7                                                     |
| $\{1, 2, 3\}$    | Roll a 1, 2, or 3 (a.k.a. do not roll a 4) | 0.6                                                     |
| $\{1, 2, 4\}$    | Roll a 1, 2, or 4 (a.k.a. do not roll a 3) | 0.7                                                     |
| $\{1, 3, 4\}$    | Roll a 1, 3, or 4 (a.k.a. do not roll a 2) | 0.8                                                     |
| $\{2, 3, 4\}$    | Roll a 2, 3, or 4 (a.k.a. do not roll a 1) | 0.9                                                     |
| $\{1, 2, 3, 4\}$ | Roll something                             | 1                                                       |

: (#tab:die-events-weighted) All possible events associated with a single roll of a four-sided die, and their probabilities assuming the die is weighted: roll a 1 with probability 0.1, 2 with probability 0.2, 3 with probability 0.3, 4 with probability 0.4.

The symbol $\IP$ is more than just shorthand for the word "probability".
$\IP$ denotes the underlying probability measure, which represents all the assumptions about the random phenomenon.
Changing assumptions results in a change of the probability measure and a different probability model.
We often consider several probability measures for the same sample space and collection of events; these several measures represent different sets of assumptions and different probability models.

In the four-sided die example above, suppose $\IP$ represents the probability measure corresponding to the assumption of a fair die (equally likely outcomes).
With this measure $\IP(A) = 2/4=0.5$ for $A = \{1, 3\}$.
Now let $\IQ$ represent the probability measure corresponding to the weighted die in Example \@ref(exm:die-weighted) ; then $\IQ(A) = 0.4$.
The outcomes and events are the same in both scenarios, because both scenarios involve a four sided-die.
What is different is the probability measure that assigns probabilities to the events.
One scenario assumes the die is fair while the other assumes the die has a particular weighting, resulting in two different probability measures.

Both probability measures in the dice example could be written as explicit set functions: for an event $A$

```{=tex}
\begin{align*}
\IP(A) & = \frac{\text{number of elements in $A$}}{4}, & & {\text{$\IP$ assumes a fair four-sided die}}
\\
\IQ(A) & = \frac{\text{sum of elements in $A$}}{10}, & & {\text{$\IQ$ assumes a particular weighted four-sided die}}
\end{align*}
```
We provide the above descriptions to illustrate that a probability measure operates on sets.
However, in many situations there does not exist a simple closed form expression for the set function defining the probability measure which maps events to probabilities.

```{example dice-normalize}
Consider again a single roll of a weighted four-sided die. Suppose that

- Rolling a 1 is twice as likely as rolling a 4
- Rolling a 2 is three times as likely as rolling a 4
- Rolling a 3 is 1.5 times as likely as rolling a 4
  
Let $\tilde{\textrm{Q}}$ be the probability measure corresponding to this die.  Compute $\tilde{\textrm{Q}}(A)$ for each event in  Table \@ref(tab:die-events-fair).  In what particular way is the die weighted?  That is, what is the probability of each the four possible outcomes?

```

```{solution dice-normalize-sol}
to Example \@ref(exm:dice-normalize).
```

```{asis, fold.chunk = TRUE}

Let $q = \tilde{\textrm{Q}}(\{4\})$ denote the probability of rolling a 4.  Then $\tilde{\textrm{Q}}(\{1\}) = 2q$, $\tilde{\textrm{Q}}(\{2\}) = 3q$, and $\tilde{\textrm{Q}}(\{3\}) = 1.5q$.  Since these probabilities must sum to 1, we have $2q + 3q + 1.5q + q = 1$ so $q = 2/15$. From there we can find the probabilities of all possible events for this particular weighted die, displayed in Table \@ref(tab:die-events-weighted2). Note this probability measure does not have a simple closed formula for $\tilde{\textrm{Q}}(A)$.

```

| Event            | Description                                | Probability of event assuming a particular weighted die |
|------------------------|------------------------|------------------------|
| $\emptyset$      | Roll nothing (not possible)                | 0                                                       |
| $\{1\}$          | Roll a 1                                   | 4/15                                                    |
| $\{2\}$          | Roll a 2                                   | 6/15                                                    |
| $\{3\}$          | Roll a 3                                   | 3/15                                                    |
| $\{4\}$          | Roll a 4                                   | 2/15                                                    |
| $\{1, 2\}$       | Roll a 1 or a 2                            | 10/15                                                   |
| $\{1, 3\}$       | Roll a 1 or a 3                            | 7/15                                                    |
| $\{1, 4\}$       | Roll a 1 or a 4                            | 6/15                                                    |
| $\{2, 3\}$       | Roll a 2 or a 3                            | 9/15                                                    |
| $\{2, 4\}$       | Roll a 2 or a 4                            | 8/15                                                    |
| $\{3, 4\}$       | Roll a 3 or a 4                            | 5/15                                                    |
| $\{1, 2, 3\}$    | Roll a 1, 2, or 3 (a.k.a. do not roll a 4) | 13/15                                                   |
| $\{1, 2, 4\}$    | Roll a 1, 2, or 4 (a.k.a. do not roll a 3) | 12/15                                                   |
| $\{1, 3, 4\}$    | Roll a 1, 3, or 4 (a.k.a. do not roll a 2) | 9/15                                                    |
| $\{2, 3, 4\}$    | Roll a 2, 3, or 4 (a.k.a. do not roll a 1) | 11/15                                                   |
| $\{1, 2, 3, 4\}$ | Roll something                             | 1                                                       |

: (#tab:die-events-weighted2) All possible events associated with a single roll of a four-sided die, and their probabilities assuming the die is weighted: roll a 1 with probability 4/15, 2 with probability 6/15, 3 with probability 3/15, 4 with probability 2/15.

The die rolling example is not the most exciting or practical scenario.
But the example does illustrate the idea of several probability measures, each corresponding to a different set of assumptions about the random phenomenon.
If it's difficult to imagine how to physically weight a die in these particular ways, consider the spinners (like from a kids game) in Figure \@ref(fig:die-three-spinners).

(ref:cap-die-three-spinners) Three possible spinners corresponding to the roll of a four-sided die. Left: a fair die. Middle: the weighted die of Example \@ref(exm:die-weighted). Right: the weighted die of Example \@ref(exm:dice-normalize).

```{r die-three-spinners, echo=FALSE, fig.cap="(ref:cap-die-three-spinners)", out.width='33%', fig.show='hold'}

make_discrete_spinner <- function(x, p){
  xp <- data.frame(x, p)
  cdf = c(0, cumsum(xp$p))
  plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  ggplot(xp, aes(x="", y=p, fill=x))+
    geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
    coord_polar("y", start=0) +
    blank_theme +
    # plot the possible values on the outside
    scale_y_continuous(breaks = plotp, labels=xp$x) +
    theme(axis.text.x=element_text(size=16, face="bold")) +
    # plot the probabilities as percents inside
    geom_text(aes(y = plotp,
                  label = format(percent(round(p, 3)),0)), size=6)
}

x = 1:4
p1 = rep(0.25, 4)
p2 = x / sum(x)
p3 = c(4, 6, 3, 2) / 15

make_discrete_spinner(x, p1)

make_discrete_spinner(x, p2)

make_discrete_spinner(x, p3)

```






```{example dice-probspace}

Roll a four-sided die twice; recall the sample space in Example \@ref(exm:dice-rv) and Table \@ref(tab:dice-rv-sol-table).
One choice of probability measure corresponds to assuming that the die is fair and that the 16 possible outcomes are equally likely.
Let $X$ be the sum of the two dice, and let $Y$ be the larger of the two rolls (or the common value if both rolls are the same).

```


1. Compute $\IP(E_1)$, where $E_1$ is the event that the first roll lands on 1.  
1. Compute $\IP(X = 6)$.
1. Interpret $\IP(X = 6)$ as a long run relative frequency.
1. Interpret $\IP(X = 6)$ as a relative degree of likelihood.
(Hint: compare to $\IP(X \neq 6)$.)
1. Construct a table displaying $\IP(X = x)$ for each possible value $x$ of $X$.  
1. Construct a table displaying $\IP(Y = y)$ for each possible value $y$ of $Y$.
1. Construct a table displaying $\IP(X = x, Y= y)$ for each possible value $(x, y)$ pair.


```{solution dice-probspace-sol}

to Example \@ref(exm:dice-probspace)

```


```{asis fold.chunk = TRUE}

1. Since $\IP$ corresponds to equally likely outcomes, we simply need to count the number of outcomes that satisfy the event and divide by the total number of outcomes.
There are 16 equally likely outcomes, of which 4 satisfy event $E_1$.
(Remember, the sample space corresponds to pairs of rolls, and there are 4 pairs for which the first roll is 1.)
So $\IP(E_1) = 4 / 16 = 1/4$, which makes sense if we're assuming the die is fair.
1. There are 16 equally likely outcomes, 3 of which satisfy the event that the sum is 6.
So $\IP(X = 6) = 3 / 16 = 0.1875$.
1. Over many pairs of rolls of a fair four-sided die, around 18.75% of pairs will yield a sum of 6.
1. $\IP(X\neq 6) = 13/16 = 0.8125$. The ratio of $\IP(X \neq 6)$ to $\IP(X = 6)$ is $13/3 approx 4.3$.
If you roll a fair four-sided die twice, it is 4.3 times more likely for the sum to be something other than 6 than for it to be 6.
1. The possible values of $X$ are $2, 3, 4, 5, 6, 7, 8$. Find the probability of each value by counting the corresponding outcomes using Table \@ref(tab:dice-rv-sol-table). For example, $\IP(X = 3) = \IP(\{(1, 2), (2, 1)\}) = 2/16$.
See Table \@ref(tab:dice-sum-dist-table).
1. The possible values of $Y$ are $1, 2, 3, 4$.
For example, $\IP(Y = 3) = \IP(\{(1, 3), (2, 3), (3, 1), (3, 2), (3, 3)\}) = 5/16$. See table \@ref(tab:dice-max-dist-table).
1. Similar to the previous parts, we can first construct a table with each row corresponding to a possible $(X, Y)$ pair.  $\IP((X, Y) = (4, 3))=\IP(X = 4, Y=3) = \IP(\{(1, 3), (3, 1)\}) = 2/16$. See Table \@ref(tab:dice-joint-dist-flat).


``` 

(ref:cap-dice-sum-dist-table) The distribution of $X$, the sum of two rolls of a fair four-sided die.

```{r, dice-sum-dist-table, echo = FALSE}
y = 2:8
p = c(1, 2, 3, 4, 3, 2, 1) / 16

knitr::kable(
  data.frame(y, p),
  col.names = c("x", "P(X=x)"),
  booktabs = TRUE,
  caption = "(ref:cap-dice-sum-dist-table)",
  digits = 4
)

```  



(ref:cap-dice-max-dist-table) The distribution of $Y$, the larger (or common value if a tie) of two rolls of a fair four-sided die.

```{r, dice-max-dist-table, echo = FALSE}
y = 1:4
p = c(1, 3, 5, 7) / 16

knitr::kable(
  data.frame(y, p),
  col.names = c("y", "P(Y=y)"),
  booktabs = TRUE,
  caption = "(ref:cap-dice-max-dist-table)",
  digits = 4
)

```  


```{r, dice-joint-dist-flat, echo = FALSE}
x = c(2, 3, 4, 4, 5, 5, 6, 6, 7, 8)
y = c(1, 2, 2, 3, 3, 4, 3, 4, 4, 4)
p = c(1, 2, 1, 2, 2, 2, 1, 2, 2, 1) / 16

knitr::kable(
  data.frame(paste("(", x, ", ", y, ")", sep=""), p),
  col.names = c("(x, y)", "P(X = x, Y = y)"),
  booktabs = TRUE,
  caption = 'Table representing the joint distribution of sum ($X$) and larger ($Y$) of two rolls of a four-sided die',
  digits = 4
)

```

Table \@ref(tab:dice-joint-dist-twoway) reorganizes Table \@ref(tab:dice-joint-dist-flat) into a two-way table with rows corresponding to possible values of $X$ and columns corresponding to possible values of $Y$.

Table: (\#tab:dice-joint-dist-twoway) Two-way table representation of the joint distribution of $X$ and $Y$,  the sum and the larger (or common value if a tie) of two rolls of a fair four-sided die.  Possible values of $X$ are in the leftmost column; possible values of $Y$ are in the top row.

|             |       |       |       |       |
|------------	|-----:	|-----:	|-----:	|-----:	|
| $x$ \\ $y$ 	|    1 	|    2 	|    3 	|    4 	|
| 2          	| 1/16 	|    0 	|    0 	|    0 	|
| 3          	|    0 	| 2/16 	|    0 	|    0 	|
| 4          	|    0 	| 1/16 	| 2/16 	|    0 	|
| 5          	|    0 	|    0 	| 2/16 	| 2/16 	|
| 6          	|    0 	|    0 	| 1/16 	| 2/16 	|
| 7          	|    0 	|    0 	|    0 	| 2/16 	|
| 8          	|    0 	|    0 	|    0 	| 1/16 	|




The above tables represent the *joint and marginal distributions* of the random variables $X$ and $Y$ according to the probability measure $\IP$, which reflects the assumption that the die is fair and the rolls are independent.
The distribution of a random variable describes its possible values and their relative likelihoods.
We will study distributions in much more detail as we go.

The distribution of a random variable depends on the underlying probability measure.
Changing the probability measure can change the distribution of the random variable.
For example, if we used one of the weighted dice from earlier in the sections, the probabilities in the distribution tables for $X$ and $Y$ would change.

Most four-sided dice can reasonably be assumed to be fair, and so the probability models corresponding to the weighted dice in this section might not be very practical.
However, remember that most real world situations are not as simple as rolling dice.
Just because a situation has  16 possible outcomes doesn't mean the outcomes have to be equally likely.
For example, 16 basketball teams reach the NBA playoffs every year, but that doesn't mean that all of the 16 playoff teams are equally likely to win the championship.


### Some probability measures in the meeting problem {#sec-meeting-probspace}

Consider a version of the meeting problem (Example \@ref(exm:meeting-intro)) where Regina and Cady will definitely arrive between noon and 1, but their exact arrival times are uncertain.
Rather than dealing with clock time, it is helpful to represent noon as time 0 and measure time as minutes after noon, so that arrival times take values in the continuous interval [0, 60].

We will use pictures to represent a few probability measures corresponding to different assumptions about the arrival times.
In the pictures below, lighter colors represent regions of outcomes that are more likely; darker colors, less likely.

Figure \@ref(fig:meeting-probmeasure1) corresponds to a "uniform" probability measure under which all outcomes are "equally likely".
This probability measure would be appropriate if we assume that Regina and Cady each arrive at a time uniformly at random between noon and 1, independently of each other.

```{r meeting-probmeasure1, echo = FALSE, warning = FALSE, message = FALSE, fig.cap="A uniform probability measure in the meeting problem"}
# | echo: false
# | label: fig-meeting-independent-uniform
# | fig-cap: "add"
# | fig-alt: "add"

ggplot(unit_box_prob,
       aes(x = x,
           y = y)) +
  geom_raster(aes(fill = independent_uniform), interpolate = TRUE) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_fill_viridis(limits = c(0, unit_box_max_prob)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)") +
  coord_fixed() +
  theme_classic() +
  theme(legend.position = "none")

```


For a finite sample space with equally likely outcomes, computing the probability of an event reduces to counting the number of outcomes that satisfy the event.  The continuous analog of equally likely outcomes is a *uniform probability measure*.  When the sample space is uncountable, size is measured continuously (length, area, volume) rather that discretely (counting).

\[
\IP(A) = \frac{|A|}{|\Omega|} = \frac{\text{size of } A}{\text{size of } \Omega} \qquad \text{if $\IP$ is a uniform probability measure}
\]

```{example, meeting-probspace2}

In the meeting problem, assume the uniform probability measure $\IP$ represented by Figure \@ref(fig:meeting-probmeasure1).


```


1. Find the probability that Regina arrives after Cady.
1. Find the probability that either Regina or Cady arrives before 12:30.
1. Find the probability that Cady arrives first and Regina arrives at most 15 minutes after Cady.
1. Find the probability that Regina arrives before 12:24.
1. Find the probability that Cady arrive first and Regina arrives at most 1 minute after Cady.
1. Find the probability that Cady arrives first and Regina arrives at most 1 second after.
1. Find the probability that Regina and Cady arrive at exactly the same time, with infinite precision.




```{solution meeting-probspace2-sol}

to Example \@ref(exm:meeting-probspace2)

```

```{asis, fold.chunk = TRUE}

1. See Figure \@ref(fig:meeting-probspace2-plot) for pictures. Since the sample space is $[0, 60]\times[0, 60]$, a continuous two-dimensional region, "size" is measured by area. Let $\IP$ be the uniform probability measure on $[0, 60]\times [0, 60]$.  The sample space has area 3600. The triangular region corresponding to the event that Regina arrives after Cady has area 3600/2 = 1800. So the probability that Regina arrives after Cady is $1800/3600=0.5$.
1. The L-shaped region corresponding to the event that Regina or Cady arrives before 12:30 has area $(0.75)(3600)$, so the probability is 0.75.
1. The trapezoidal region corresponding to the event that that Regina arrives at most 15 minutes after Cady (and Cady arrives first) has area $(7/32)(3600) = (0.21875)(3600)$. (It's easiest to find the area of the two unshaded triangles and subtract from the total area of 3600: $3600 - 0.5(3600) - (3600)(1-0.25)^2/2=7/32(3600)$.)
So the probability that Regina arrives at most 15 minutes after Cady (and Cady arrives first) is 0.21875.
1. The rectangular region corresponding to the event that that Regina arrives before 12:24 has area (0.4)(3600), so the probability is 0.4.
1. Similar to part 3, the probability that Regina arrives at most 1 minute after Cady (and Cady arrives first) is $1 - 0.5 - (1-1/60)^2/2=0.0165$.
1. Similar to part 3, the probability that Regina arrives at most 1 second after Cady (and Cady arrives first) is $1 - 0.5 - (1-1/3600)^2/2=0.000278$.
1. The event that Regina and Cady arrive at exactly the same time, with infinite precision, corresponds to the "Regina = Cady" line segment.  The area of this line segment is 0, so the probability that Regina and Cady arrive at exactly the same time, with infinite precision, is 0.

```

(ref:cap-meeting-probspace2) Illustration of the events in Exercise \@ref(exm:meeting-probspace2). The square represents the sample space $\Omega=[0,60]\times[0,60]$. With a uniform probability measure, the areas of the shaded regions relative to the whole represent their probabilities.


```{r meeting-probspace2-plot, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-meeting-probspace2)"}

dfA <- data.frame(x = c(0, 1, 1, 1) * 60,
                 y = c(0, 0, 1, 1) * 60,
                 v = c(1, 1, 1, 1))

pA <- ggplot(data = dfA, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression("Regina")) +
  ylab(expression("Cady")) +
  ggtitle("Regina arrives after Cady") +
  theme(plot.title = element_text(hjust = 0.5))


dfB <- data.frame(x = c(0.5, 1, 1, 0.5) * 60,
                  y = c(0.5, 0.5, 1, 1) * 60,
                  v = c(1, 1, 1, 1))

pB <- ggplot(data = dfB, aes(x = x, y = y)) +
  geom_polygon(fill = "white", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(panel.background = element_rect(fill = "orange",
                                    colour = "orange")) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression("Regina")) +
  ylab(expression("Cady")) +
  ggtitle("Regina or Cady arrives before 12:30") +
  theme(plot.title = element_text(hjust = 0.5))
  

# dfC <- data.frame(x = c(1, 1, 0.5, 1),
#                   y = c(0.5, 0.5, 1, 1),
#                   v = c(1, 1, 1, 1))
# 
# pC <- ggplot(data = dfC, aes(x = x, y = y)) +
#   geom_polygon(fill = "cornflowerblue", show.legend = FALSE) +
#   scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + 
#   scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
#   theme_classic() +
#   theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
#   theme(plot.margin=unit(c(1,1,1,1),"cm")) +
#   xlab(expression(omega[1] ~ "(Regina)")) +
#   ylab(expression(omega[2] ~ "(Cady)")) +
#   ggtitle("Event C is shaded") +
#   theme(plot.title = element_text(hjust = 0.5))


dfC <- data.frame(x = c(0, 0.25, 1, 1) * 60,
                 y = c(0, 0, 0.75, 1) * 60,
                 v = c(1, 1, 1, 1))

pC <- ggplot(data = dfC, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression("Regina")) +
  ylab(expression("Cady")) +
  ggtitle("Regina arrives at most 15 minutes after Cady") +
  theme(plot.title = element_text(hjust = 0.5))

dfD <- data.frame(x = c(0, 0.4, 0.4, 0) * 60,
                  y = c(0, 0, 1, 1) * 60,
                  v = c(1, 1, 1, 1))

pD <- ggplot(data = dfD, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression("Regina")) +
  ylab(expression("Cady")) +
  ggtitle("Regina arrives before 12:24") +
  theme(plot.title = element_text(hjust = 0.5))


library(ggpubr)
ggarrange(pA, pB, pC, pD, ncol = 2, nrow = 2)

```

The latter parts of Example \@ref(exm:meeting-probspace2) illustrate that continuous probability spaces introduce some complications that we didn't encounter when dealing with discrete probability spaces.
Regardless of the precise time in the continuous interval $[0, 60]$ at which Regina arrives, the probability that Cady arrives at that exact time, with infinite precision, is 0.
We will investigate related ideas in much more detail as we go.


```{example meeting-waiting-uniform}

Continuing Example \@ref(exm:meeting-probspace2), let $R$ be the random variable representing Regina's arrival time in $[0, 60]$, and $Y$ for Cady. Recall that the random variable $W = |R - Y|$ represents the amount of time the first person to arrive waits for the second person to arrive.
The possible values of $W$ lie in the interval $[0, 60]$.

```

1. Find and interpret $\IP(W < 15)$. (Hint: draw a picture representing the event in terms of the pairs of arrival times.)
1. Find and interpret $\IP(W > 45)$.
1. Do the values of $W$ have a "uniform distribution" throughout the interval $[0, 60]$? Explain.


```{solution meeting-waiting-uniform-sol}

to Example \@ref(exm:meeting-waiting-uniform)

```

```{asis, fold.chunk = TRUE}

1. The event corresponding to $W<15$ is depicted on the left in Figure \@ref(fig:meeting-waiting-uniform-plot). If Cady arrives first ($R>Y$, below the diagonal in the plot) then $W=R-Y$ so $W<15$ if $R -15 < Y$. If Regina arrives first ($R<Y$, above the diagonal in the plot) then $W=Y-R$ so $W<15$ if $Y < R + 15$. Putting the two cases together $W<15$ if $R - 15 < Y < R + 15$; the corresponding region of $(R, Y)$ pairs is shaded in the plot.  According to the uniform probability measure on $[0, 60]\times[0,60]$, $\IP(W <15)$ is the area of the shaded region (divided by the area of the sample space which is 1). The area of the shaded region is 0.4375(3600). (It is easiest to find the areas of the unshaded triangles and subtract from the total area, $3600(1 - 0.75^2/2 - 0.75^2/2)$.)  So $\IP(W < 15)=0.4375$ is the probability that Regina and Cady arrive within 15 minutes of each other.
1. The event corresponding to $W>45$ is depicted on the right in Figure \@ref(fig:meeting-waiting-uniform-plot).  The probability is the area of the shaded region.  So $\IP(W > 45)=0.25^2/2 + 0.25^2/2=0.0625$ is the probability that Regina and Cady arrive more than 45 minutes apart.
1. The values of $W$ are not uniformly distributed over $[0, 60]$.  For uniform probability measures, regions of the same size have the same probability.  But the probability that $W$ lies in the interval $[0, 15]$ is seven times greater than the probability that $W$ lies in the interval $[45, 60]$, even though these intervals have the same length.

```

(ref:cap-meeting-waiting-uniform) Illustration of the events in Example \@ref(exm:meeting-waiting-uniform). The square represents the sample space $[0,60]\times[0,60]$.


```{r meeting-waiting-uniform-plot, echo=FALSE, warning=FALSE, message=FALSE, fig.show="hold", out.width="50%", fig.cap="(ref:cap-meeting-waiting-uniform)"}


dfC <- data.frame(x = c(0, 0.25, 1, 1, 0.75, 0) * 60,
                 y = c(0, 0, 0.75, 1, 1, 0.25) * 60,
                 v = c(1, 1, 1, 1, 1, 1))

pC <- ggplot(data = dfC, aes(x = x, y = y)) +
  geom_polygon(fill="orange", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression("Regina")) +
  ylab(expression("Cady")) +
  ggtitle("Regina and Cady arrive within 15 minutes of each other") +
  theme(plot.title = element_text(hjust = 0.5))

plot(pC)

dfD <- data.frame(x = c(0, 0.75, 1, 1, 0.25, 0) * 60,
                 y = c(0, 0, 0.25, 1, 1, 0.75) * 60,
                 v = c(1, 1, 1, 1, 1, 1))

pD <- ggplot(data = dfD, aes(x = x, y = y)) +
  geom_polygon(fill="white", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1) * 60, expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA),
          panel.background = element_rect(fill = "orange", colour = "orange",
                                size = 2, linetype = "solid")) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression("Regina")) +
  ylab(expression("Cady")) +
  ggtitle("Regina and Cady arrive at least 45 minutes apart") +
  theme(plot.title = element_text(hjust = 0.5))

plot(pD)

# library(ggpubr)
# ggarrange(pA, pB, pC, pD, ncol = 2, nrow = 2)

```

Most random phenomenon do not involve equally likely outcomes or uniform probability measures.
Even when the underlying outcomes are equally likely, the values of related random variables are usually not.
Therefore, most interesting probability problems involve "non-uniform" probability measures.





Figure \@ref(fig:meeting-probmeasure2) corresponds to one non-uniform probability measure for the meeting problem; certain outcomes are more likely than others.
(Lighter colors represent regions of outcomes that are more likely; darker colors, less likely.)
Such a probability measure would be appropriate if we assume that Regina and Cady each are more likely to arrive around 12:30 than noon or 1:00, independently of each other.

```{r meeting-probmeasure2, echo = FALSE, warning = FALSE, message = FALSE, fig.cap="A non-uniform probability measure in the meeting problem"}
# | echo: false
# | label: fig-meeting-independent-uniform
# | fig-cap: "add"
# | fig-alt: "add"

ggplot(unit_box_prob,
       aes(x = x,
           y = y)) +
  geom_raster(aes(fill = independent_normal), interpolate = TRUE) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_fill_viridis(limits = c(0, unit_box_max_prob)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)") +
  coord_fixed() +
  theme_classic() +
  theme(legend.position = "none")

```


Switching from the uniform probability measure represented by Figure \@ref(fig:meeting-probmeasure1) to the non-uniform one represented by Figure \@ref(fig:meeting-probmeasure2) would change the probability of the events in Examples \@ref(exm:meeting-probspace2) and \@ref(exm:meeting-waiting-uniform).
(We'll see how to compute probabilities like this later.)

Figure \@ref(fig:meeting-probmeasure3) corresponds to another "non-uniform" probability measure.
Such a probability measure would be appropriate if we assume that Regina and Cady each are more likely to arrive around 12:30 than noon or 1:00, but they coordinate their arrivals so they are more likely to arrive around the same time.

```{r meeting-probmeasure3, echo = FALSE, warning = FALSE, message = FALSE, fig.cap="Another probability measure in the meeting problem"}
# | echo: false
# | label: fig-meeting-independent-uniform
# | fig-cap: "add"
# | fig-alt: "add"

ggplot(unit_box_prob,
       aes(x = x,
           y = y)) +
  geom_raster(aes(fill = bivariate_normal), interpolate = TRUE) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_fill_viridis(limits = c(0, unit_box_max_prob)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)") +
  coord_fixed() +
  theme_classic() +
  theme(legend.position = "none")

```

Figures \@ref(fig:meeting-probmeasure1), \@ref(fig:meeting-probmeasure2), and \@ref(fig:meeting-probmeasure3) represent three different sets of assumptions, and three different probability measures, for the meeting problem.
There are many other models.
Each probability measure assigns a probability to events like "Cady arrives first", "both arrive before 12:20", and "the first person to arrive has to wait less than 15 minutes for the second to arrive", and these probabilities can differ between models.

Of course, we are leaving out many details behind 
Figures \@ref(fig:meeting-probmeasure1), \@ref(fig:meeting-probmeasure2), and \@ref(fig:meeting-probmeasure3).
We will see later where probability measures like those represented by the pictures above come from, and how to use them.
But the pictures illustrate that there can be many assumed probability models for a single situation.
There will be many probability measures that satisfy the logical consistency requirements of the probability axioms.
Which one is most appropriate depends on the assumptions you make about the random phenomenon.

Perhaps the concept of multiple potential probability measures is easier to understand in a subjective probability situation.
For example, each model that is used to forecast the 2022-2023 NFL season corresponds to a probability measure which assigns probabilities to events like "the Eagles win the 2023 Superbowl".
Different sets of assumptions and models can assign different probabilities for the same events.
As another example, the weather forecaster on one local news station might report that the probability of rain tomorrow is 0.6, while an online source might report it as 0.5.
Each weather forecasting model corresponds to a different probability measure which encodes a set of assumptions about the random phenomenon.

Before moving on, we want to reiterate: Most random phenomenon do *not* involve equally likely outcomes or uniform probability measures.
Even when the underlying outcomes are equally likely, the values of related random variables are usually not.
Equally like outcomes or uniform probability measures are the simplest probability measures, and therefore are the ones we typically encounter first.
But don't let that fool you; most interesting probability problems involve non-equally likely outcomes or non-uniform probability measures.

### Summary

-   A **probability measure** $\IP$ assigns probabilities to *events* to quantify their relative likelihoods according to the assumptions of the model of the random phenomenon.
-   A valid probability measure $\IP$ must satisfy the following three "axioms".
    -   For any event $A$, $0 \le \IP(A) \le 1$.
    -   If $\Omega$ represents the sample space then $\IP(\Omega) = 1$.
    -   (*Countable additivity.*) If $A_1, A_2, A_3, \ldots$ are disjoint then $$
          \IP(A_1 \cup A_2 \cup A_2 \cup \cdots) = \IP(A_1) + \IP(A_2) +\IP(A_3) + \cdots
          $$
-   Additional properties of a probability measure follow from the axioms
    -   *Complement rule*. For any event $A$, $\IP(A^c) = 1 - \IP(A)$.
    -   *Subset rule*. If $A \subseteq B$ then $\IP(A) \le \IP(B)$.
    -   *Addition rule for two events*. If $A$ and $B$ are any two events \begin{align*}
          \IP(A\cup B) = \IP(A) + \IP(B) - \IP(A \cap B)
          \end{align*}
    -   *Law of total probability*. If $C_1, C_2, C_3\ldots$ are disjoint events with $C_1\cup C_2 \cup C_3\cup \cdots =\Omega$, then \begin{align*}
          \IP(A) & = \IP(A \cap C_1) + \IP(A \cap C_2) + \IP(A \cap C_3) + \cdots
          \end{align*}
-   A **probability model** (or **probability space**) is the collection of all outcomes, events, and random variables associated with a random phenomenon along with the probabilities of all events of interest under the assumptions of the model.
-   The axioms of a probability measure are minimal logical consistent requirements that ensure that probabilities of different events fit together in a valid, coherent way.
-   A single probability measure corresponds to a particular set of assumptions about the random phenomenon.
-   There can be many probability measures defined on a single sample space, each one corresponding to a different probability model for the random phenomenon.
-   Probabilities of events can change if the probability measure changes.

## Introduction to simulation

A probability model of a random phenomenon consists of a sample space of possible outcomes, associated events and random variables, and a probability measure which specifies probabilities of events and determines distributions of random variables.
**Simulation** involves using a probability model to artificially recreate a random phenomenon, many times, usually using a computer.
Given a probability model, we can simulate outcomes, occurrences of events, and values of random variables, according to the specifications of the probability measure which reflects the model assumptions.

Recall that probabilities can be interpreted as long run relative frequencies.
Therefore the probability of an event can be approximated by simulating, according to the assumptions of the probability model, the random phenomenon a large number of times and computing the relative frequency of repetitions on which the event occurs.
Simulation can be used to approximate probabilities of events, distributions of random variables, long run averages, and other characteristics.

In general, a simulation involves the following steps.

1.  **Set up.** Define a probability space, and related random variables and events. The probability measure encodes all the assumptions of the model, but the probability measure is often only specified indirectly. The set up might be as simple as "flip a fair coin ten times and count the number of heads". However, even when the set up can be described simply, translating the setup into computer code can be challenging[^simulation-11].
2.  **Simulate.** Simulate --- according to the assumptions reflected in the probability measure --- outcomes, occurrences of events, and values of random variables.
3.  **Summarize.** Summarize simulation output in plots and summary statistics (relative frequencies, averages, standard deviations, correlations, etc) to describe and approximate probabilities, distributions, and related characteristics.
4.  **Sensitivity analysis.** Investigate how results respond to changes in the assumptions or parameters of the simulation.

[^simulation-11]: In most situations we'll encounter in this book, the "simulate" and "summarize" steps are usually straightforward.
    In many other cases, these steps can be challenging.
    Complex set ups often require sophisticated methods, such MCMC (Markov Chain Monte Carlo) algorithms, to efficiently simulate realizations.
    Effectively summarizing high dimensional simulation output often requires the use of multivariate statistics and visualizations.

You might ask: if we have access to the probability measure, then why do we need simulation to approximate probabilities?
Can't we just compute them?
Remember that the probability measure is often only specified indirectly.
The probability measure represents the underlying assumptions under which probabilities of events of interest are determined.
But in most situations the probability measure does not provide an explicit formula for computing the probability of any particular event.
And in many cases, it is impossible to enumerate all possible outcomes.
For example, a probabilistic model of a particular Atlantic Hurricane does not provide a mathematical formula for computing the probability that the hurricane makes landfall in the U.S.
Nor does the model provide a comprehensive list of the uncountably many possible paths of the hurricane.
Rather, the model reflects a set of assumptions under which possible paths can be simulated to approximate probabilities of events of interest.

We will see techniques for computing probabilities, but in many situations explicit computation is difficult.
Simulation is a powerful tool for investigating probability models and solving complex problems.

(ref:cap-hurricane-cone) Picture of a "hurricane cone of uncertainty" from the [August 28, 2019 *Washington Post* article "How the hurricane cone of uncertainty can be a cone of confusion, and what to do about it"](https://www.washingtonpost.com/weather/2019/08/28/how-hurricane-cone-uncertainty-can-be-cone-confusion-what-do-about-it/).

### Tactile simulation: Boxes and spinners {#tactile}

While we generally use technology to conduct large scale simulations, it is helpful to first consider how we might conduct a simulation by hand using physical objects like coins, dice, cards, or spinners.

Many random phenomena can be represented in terms of a **"box model**[^simulation-12]"

[^simulation-12]: **Our use of "box models" is inspired by [@FPP].**

-   Imagine a box containing "tickets" with labels. Examples include:
    -   Fair coin flip. 2 tickets: 1 labeled H and 1 labeled T
    -   Free throw attempt of a 90% free throw shooter. 10 tickets: 9 labeled "make" and 1 labeled "miss"
    -   Card shuffling. 52 cards: each card with a pair of labels (face value, suit).
-   The tickets are shuffled in the box, some number are drawn out --- either *with replacement or without replacement* of the tickets before the next draw[^simulation-13].
-   In some cases, the order in which the tickets are drawn matters; in other cases the order is irrelevant. For example,
    -   Dealing a 5 card poker hand: Select 5 cards without replacement, order does not matter
    -   Random digit dialing: Select 4 cards with replacement from a box with tickets labeled 0 through 9 to represent the last 4 digits of a randomly selected phone number with a particular area code and exchange; order matters, e.g., 805-555-1212 is a different outcome than 805-555-2121.
-   Then something is done with the tickets, typically to measure random variables of interest. For example, you might flip a coin 10 times (by drawing from the H/T box 10 times with replacement) and count the number of H.

[^simulation-13]: "With replacement" always implies replacement at a uniformly random point in the box.
    Think of "with replacement" as "with replacement and reshuffling" before the next draw.

If the draws are made with replacement from a single box, we can think of a single circular "spinner" instead of a box, spun multiple times.
For example:

-   Fair coin flip. Spinner with half of the area corresponding to H and half T
-   Free throw attempt of a 90% free throw shooter. Spinner with 90% of the area corresponding to "make" and 10% "miss". <!-- - Random digit dialing.  Spinner marked with digits 0-9, possibly with some digits more likely than others.  spun multiple times.  Depending on what regions you are trying to sample, you might have three spinners: one to generate area code, one to generate the next three digits, and one to generate the last four digits. -->

```{example dice-sim}

Let $X$ be the sum of two rolls of a fair four-sided die, and let $Y$ be the larger of the two rolls (or the common value if a tie).
Set up a box model and explain how you would use it to simulate a single realization of $(X, Y)$.
Could you use a spinner instead?
  
```

```{solution dice-sim-sol}

to Example \@ref(exm:dice-sim)

```

```{asis, fold.chunk = TRUE}

Use a box with four tickets, labeled 1, 2, 3, 4.
Draw two tickets with replacement.
Let $X$ be the sum of the two numbers drawn and $Y$ the larger of the two numbers drawn.

It's also possible to use a spinner with 4 sectors, corresponding to 1, 2, 3, 4, each with 25% of the total area; see Figure \@ref(fig:spinner-die).
Spin the spinner twice.
Let $X$ be the sum of the two numbers spun and $Y$ the larger of the two numbers spun.

```

(ref:cap-spinner-die) Spinner corresponding to a single roll of a fair four-sided die.

```{r spinner-die, echo=FALSE, fig.cap="(ref:cap-spinner-die)", fig.width=12, fig.align='center'}

knitr::include_graphics("_graphics/spinner-die.png")

```

The spinner in Figure \@ref(fig:spinner-die) simulates the individual die rolls.
We will see later spinners for generating values of $X$, values of $Y$, and values of $(X, Y)$ pairs directly.

Note that we are able to simulate outcomes of the rolls and values of $X$ and $Y$ without defining the probability space in detail.
That is, we do not need to list all the possible outcomes and events and their probabilities.
Instead, the probability space is defined implicitly via the specification to "roll a fair four-sided die twice" or "draw two tickets with replacement from a box with four tickets labeled 1, 2, 3, 4" or "spin the spinner in Figure \@ref(fig:spinner-die) twice".
The random variables are defined by what is being measured for each outcome, the sum ($X$) and the max ($Y$) of the two draws or spins.

In Example \@ref(exm:dice-sim) we described how to simulate a single realization of $(X, Y)$; this is one "repetition" in the simulation.
A simulation usually involves many repetitions.
When conducting simulations it is important to distinguish between what entails (1) one repetition of the simulation and its output, and (2) the simulation itself and output from many repetitions.

```{example matching-box-sim}
Consider the matching problem (Example \@ref(exm:dice-outcome)) with $n=4$.
Label the objects 1, 2, 3, 4, and the spots 1, 2, 3, 4, with spot 1 the correct spot for object 1, etc. Let $X$ be the number of rocks that are placed in the correct spot,
and let $C$ be the event that at least one rock is placed in the correct spot.
Describe how you would use a box model to simulate a single realization of $Y$ and of $C$.
Could you use a spinner instead?

```

```{solution matching-box-sim-sol}

to Example \@ref(exm:matching-box-sim)

```

```{asis, fold.chunk = TRUE}

Use a box with 4 tickets, labeled 1, 2, 3, 4.
Shuffle the tickets and draw all 4 *without* replacement and record the tickets drawn *in order*.
Let $Y$ be the number of tickets that match their spot in order.
(For example, if the tickets are drawn in the order 2431 then the realized value of $X$ is 1 since only ticket 3 matches its spot in the order.)

Since $C=\{X \ge 1\}$, event $C$ occurs if $X\ge 1$ and does not occur if $X=0$.
We could record the realization of event $C$ as "True" or "False".
We could also record the realization of $I_C$, the indicator random variable for event $C$, as 1 if $C$ occurs and 0 if $C$ does not occur.

Since the tickets are drawn *without* replacement, we could not simply spin a single spinner, like the one in Figure \@ref(fig:spinner-die), four times.
If we really wanted to use the spinner in Figure \@ref(fig:spinner-die), we couldn't just spin it four times; we would sometimes have to discard spins and try again.
For example, suppose the first spin results in 2; then if the second spin results in 2 would we need to discard it and try again.
So we would usually need more than four spins to obtain a valid outcome.
If we wanted to guarantee a valid outcome in only four spins, we would need a collection of spinners, and which one we use would depend on the results of previous spins.
For example, if the first spin results in 2, then we would need to spin a spinner that only lands on 1, 3, 4; if the second spin results in 3, then we would need to spin and spinner that lands only on 1 and 4.

```

```{example meeting-box-sim}
Consider a version of the meeting problem (Example \@ref(exm:meeting-intro)) where Regina and Cady will definitely arrive between noon and 1, but their exact arrival times are uncertain.
Rather than dealing with clock time, it is helpful to represent noon as time 0 and measure time as minutes after noon, so that arrival times take values in the continuous interval [0, 60].

Explain how you would construct a spinner and use it to simulate an outcome.
Why could we not simulate this situation with a box model?

```

```{solution meeting-box-sim-sol}

to Example \@ref(exm:meeting-box-sim)

```

```{asis, fold.chunk = TRUE}
We could construct a spinner like the one in Figure \@ref(fig:uniform-spinner), but ranging from 0 to 60 instead of 0 to 1; see Figure \@ref(fig:meeting-uniform-spinner).
(Or we could just use the spinner in Figure \@ref(fig:uniform-spinner), but multiply the result of each spin by 60.)
Only selected rounded values are displayed on the circular axis, but in the idealized model the spinner is infinitely precise so that any real number between 0 and 60 is a possible outcome.
Spin it twice; the first spin represents Regina's arrival, the second Cady's.

An outcome consists of a pair of values from the continuous interal [0, 60].
Since this interval is uncountable, it's not possible to write every real number in the interval [0, 60] on a ticket to place in a box.
With a box model, we would have to round arrival times to some desired degree of precision --- nearest minute, second, millisecond, etc --- and put the rounded values on the tickets.
Which is probably fine for practical purposes!
But if we really want to create a tactile representation of continuous outcomes, a box model won't work; we tend to use spinners instead.


```

(ref:cap-meeting-uniform-spinner) A continuous [0, 60] spinner. The same spinner is displayed on both sides, with different features highlighted on the left and right. Only selected rounded values are displayed, but in the idealized model the spinner is infinitely precise so that any real number between 0 and 60 is a possible outcome.

```{r meeting-uniform-spinner, echo=FALSE, fig.cap="(ref:cap-meeting-uniform-spinner)", fig.align='center', fig.show="hold", out.width="50%"}

n = 16

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner1 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("60|0", 60 * xp$x[-1])) +
  theme(axis.text.x=element_text(size=14, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
  ggtitle(paste("Uniform(0, 60) model", sep=""))

spinner1

n = 6

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner2 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("60|0", 60 * xp$x[-1])) +
  theme(axis.text.x=element_text(size=14, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
      geom_text(aes(y = plotp,
                label = percent(p, accuracy = 0.1)), size=4, color=c("black",rep("black",4), rep("black",1))) +
  ggtitle(paste("Uniform(0, 60) model", sep=""))

spinner2

```

Notice that the values on the circular axis in Figure \@ref(fig:meeting-uniform-spinner) are evenly spaced.
For example, the intervals [0, 15] and [15, 30], both of length 15, each represent 25% of the spinner area.
If we spin the idealized spinner represented by Figure \@ref(fig:meeting-uniform-spinner) 10 times, our results might look like the following.

```{r, echo = FALSE}
n = 10
x = 60 * runif(n)

kbl(data.frame(1:n, x),
    col.names = c("Spin", "Result"),
    caption = 'Results of 10 spins of the Uniform(0, 60) spinner.'
) %>%
  kable_styling(fixed_thead = TRUE)

```

Notice the number of decimal places.
For the Uniform(0, 60) model, any value in the continuous interval between 0 and 60 is a distinct possible value: 10.25000000000... is different from 10.25000000001... is distinct from 10.2500000000000000000001... and so on.

If we plot the 10 simulated values along a number line, they are roughly evenly spaced, though there is some natural variability. (Though it's hard to discern any patterns with only 10 values.)

```{r, echo = FALSE}
ggplot(data.frame(x),
       aes(x = x,
           y = 0)) +
  geom_point(shape = "|", size = 10) +
  scale_y_continuous(limits = c(0, 5), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 60), expand = c(0, 0)) +
  theme_classic() +
  coord_fixed() +
  labs(x = "Simulated arrival time (minutes after noon)") +
  theme(axis.text.y=element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y = element_blank()
  )

```

To simulate a (Regina, Cady) pair of arrival times, we would spin the Uniform(0, 60) spinner twice.
The following displays the results of 10 repetitions, each repetition resulting in a (Regina, Cady) pair.

```{r, echo = FALSE}
n = 10
x = 60 * runif(n)
y = 60 * runif(n)


kbl(data.frame(1:n, x, y),
    col.names = c("Repetition", "Regina's time", "Cady's time"),
    caption = 'Results of 10 pairs of spins of the Uniform(0, 60) spinner.'
) %>%
  kable_styling(fixed_thead = TRUE)

```

Here is a plot of the 10 simulated pairs of arrival times.

```{r, echo = FALSE}
ggplot(data.frame(x, y),
       aes(x = x, y = y)) +
  geom_point(alpha = 0.3) +
  theme_classic() +
  scale_x_continuous(limits = c(0, 60), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 60), expand = c(0, 0)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)")  
```

Now suppose we keep repeating the process, resulting in many simulated (Regina, Cady) pairs of arrival times.
The following displays 1000 simulated pairs of (Regina, Cady) arrival times, resulting from 1000 pairs of spins of the Uniform(0, 60) spinner.

```{r, echo = FALSE}
ggplot(data.frame(x = 60 * runif(1000), y = 60 * runif(1000)),
       aes(x = x, y = y)) +
  geom_point(alpha = 0.3) +
  theme_classic() +
  scale_x_continuous(limits = c(0, 60), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 60), expand = c(0, 0)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)")
```

We see that the pairs are fairly evenly distributed throughout the square with sides [0, 60] representing the sample space.
If we simulated more values and summarized them in an appropriate plot, we would expect to see something like Figure \@ref(fig:meeting-probmeasure1).

Now suppose Regina is more likely to arrive around 12:30 than around 12:00 or 1:00.
We could construct a spinner that is more likely to land near 30 and less likely to land near 0 or 60.
We can do this by "stretching and shrinking" the labels on the circular axis to match our assumptions.
Consider the spinner in Figure \@ref(fig:meeting-normal-spinner).
Again, only selected rounded values are displayed in the picture, but the spinner represents an idealized model where the needle is "equally likely" to point to any value in the interval [0, 60].
*But pay close attention to the circular axis; the values are not equally spaced.* For example, the bottom half of the spinner corresponds to the interval [23.26, 36.74], with length 13.48 minutes, while the upper half of the spinner corresponds to the intervals [0, 23.26] and [36.74, 60], with total length 46.52 minutes.
Compared with the Uniform(0, 60) spinner, in the Normal(30, 10) spinner intervals near 30 are "stretched out" to reflect a higher probability of arriving near 12:30, while intervals near 0 and 60 are "shrunk" to reflect a lower probability of arrival near 12:00 or 1:00.
For example, the interval [20, 40] represent about 68% of the spinner area, so if we spin this spinner many times, about 68% of the arrival times will be in the interval [20, 40].
(The spinner is divided into 16 wedges of equal area, so each wedge represents 6.25% of the probability. Not all values on the axis are labeled, but you can use the wedges to eyeball probabilities.)
That is, according to this model Regina has a 68% chance of arriving within 10 minutes of 12:30, compared to a 33% chance in the Uniform(0, 60) model.


(ref:cap-meeting-normal-spinner) A continuous Normal(30, 10) spinner. The same spinner is displayed on both sides, with different features highlighted on the left and right. Only selected rounded values are displayed, but in the idealized model the spinner is infinitely precise so that any real number is a possible outcome. Notice that the values on the axis are *not* evenly spaced.

```{r meeting-normal-spinner, echo=FALSE, fig.cap="(ref:cap-meeting-normal-spinner)", fig.align='center', fig.show="hold", out.width="50%"}

n = 16

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner1 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("60|0", round(30 + 10 * qnorm(xp$x[-1]), 2))) +
  theme(axis.text.x=element_text(size=12, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
  ggtitle(paste("Normal(30, 10) model", sep=""))

spinner1


x = c("","<-2",-1, 0, 1, ">2")
p = c(pnorm(-2), pnorm(-1:2) - pnorm(-2:1), 1-pnorm(2))

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))



plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner2 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white", linetype=1) + 
  # coord_polar("y", start=0) +
  coord_curvedpolar("y", start = 0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = cdf[-c(1, length(cdf))], labels=c(10, 20, 30, 40, 50)) +
  # theme(axis.text.x=element_text(angle=c(90-180/50*(0:49), -90-180/50*(50:99)), size=8)) +
  theme(axis.text.x=element_text(angle = 0, size=12)) +
  annotate(geom="segment", y=(0:19)/20+0.000, yend = (0:19)/20+0.000,
           x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
    geom_text(aes(y = plotp,
                label = percent(p, accuracy = 0.1)), size=4, color=c("white",rep("black",4), rep("white",1))) +
  ggtitle(paste("Normal(30, 10) model", sep=""))

spinner2
```

The particular pattern represented by the spinner in Figure \@ref(fig:meeting-normal-spinner) is a *Normal(30, 10) distribution*; that is, a *Normal distribution with mean 30 and standard deviation 10*.
(Note: technically this allows for some arrival times outside the [0, 60] interval.) We will see Normal distributions in much more detail in the future; in this section we simply introduce them as one way to move beyond Uniform models.
For now, just know that a Normal(30, 10) reflects one particular pattern of variability. 
The following table compares probabilities of selected intervals under the Uniform(0, 60) and Normal(30, 10) models^[Careful: the parameters in the Uniform model are different from those in the Normal model. In the Uniform(0, 60) model, 0 is the minimum possible value; in the Normal(30, 10) model, 30 is the average value.
In the Uniform(0, 60) model, 60 is the maximum possible value; in the Normal(30, 10) model, 10 is the standard deviation.].


| Interval | Uniform(0, 60) probability | Normal(30, 10) probability |
|----------|---------------------------:|---------------------------:|
| [0. 10]  |                     16.67% |                       2.5% |
| [10, 20] |                     16.67% |                      13.6% |
| [20, 30] |                     16.67% |                      34.1% |
| [30. 40] |                     16.67% |                      34.1% |
| [40, 50] |                     16.67% |                      13.6% |
| [50, 60] |                     16.67% |                       2.5% |
|          |                            |                            |

: Comparison of probabilities for the Uniform(0, 60) and Normal(30, 10) spinners

If we spin the idealized spinner represented by Figure \@ref(fig:meeting-normal-spinner) 10 times, our results might look like the following.

```{r, echo = FALSE}
n = 10
x = 30 + 10 * rnorm(n)
x = pmin(60, pmax(0, x))
  
kbl(data.frame(1:n, x),
    col.names = c("Spin", "Result"),
    caption = 'Results of 10 spins of the Normal(30, 10) spinner.'
) %>%
  kable_styling(fixed_thead = TRUE)

```


If we plot the 10 simulated values along a number line, we tend to see more values near 30 than near 0 or 60. (Again, it's hard to discern any patterns with only 10 values.)

```{r, echo = FALSE}
ggplot(data.frame(x),
       aes(x = x,
           y = 0)) +
  geom_point(shape = "|", size = 10) +
  scale_y_continuous(limits = c(0, 5), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 60), expan = c(0, 0)) +
  theme_classic() +
  coord_fixed() +
  labs(x = "Simulated arrival time (minutes after noon)") +
  theme(axis.text.y=element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y = element_blank()
  )

```

Regina's and Cady's arrival times are each reasonably modeled by a Normal(30, 10) model, independently of each other.
To simulate a (Regina, Cady) pair of arrival times, we would spin the Normal(30, 10) spinner twice.
The following displays the results of 10 repetitions, each repetition resulting in a (Regina, Cady) pair.


```{r, echo = FALSE}
n = 10
x = 30 + 10 * rnorm(n)
y = 30 + 10 * rnorm(n)
x = pmin(60, pmax(0, x))
y = pmin(60, pmax(0, y))

kbl(data.frame(1:n, x, y),
    col.names = c("Repetition", "Regina's time", "Cady's time"),
    caption = 'Results of 10 pairs of spins of the Normal(30, 10) spinner.'
) %>%
  kable_styling(fixed_thead = TRUE)

```

Here is a plot of the 10 simulated pairs of arrival times.

```{r, echo = FALSE}
ggplot(data.frame(x, y),
       aes(x = x, y = y)) +
  geom_point(alpha = 0.3) +
  theme_classic() +
  scale_x_continuous(limits = c(0, 60), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 60), expand = c(0, 0)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)")
```

The following plots 1000 pairs of (Regina, Cady) arrival times, resulting from 1000 pairs of spins of the Normal(30, 10) spinner.

```{r, echo = FALSE}
ggplot(data.frame(x = 30 + 10 * rnorm(1000), y = 30 + 10 * rnorm(1000)),
       aes(x = x, y = y)) +
  geom_point(alpha = 0.3) +
  theme_classic() +
  scale_x_continuous(limits = c(0, 60), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 60), expand = c(0, 0)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)")
```


Compared with the simulated pairs from the Uniform(0, 60) spinner, we see many more simulated pairs in the center of the plot (when both arrive near 12:30) than in the corners of the plot (where either arrives near 12:00 or 1:00).
If we simulated more values and summarized them in an appropriate plot, we would expect to see something like Figure \@ref(fig:meeting-probmeasure2).

Recall that Figure \@ref(fig:meeting-probmeasure3) reflected a model where Regina and Cady each are more likely to arrive around 12:30 than noon or 1:00, but they coordinate their arrivals so they are more likely to arrive around the same time.
In such a situation, we could use spinners to simulate pairs of arrival times, but it's more involved than just spinning a single spinner twice.
We revisit using spinners to simulate dependent pairs later.


### Computer simulation: Dice rolling {#technology-intro}

**Note: some of the plots and tables in this and the following sections do not appear exactly as they would in Jupyter or Colab notebooks.**

We will perform computer simulations using the Python package Symbulate.
The syntax of Symbulate mirrors the language of probability in that the primary objects in Symbulate are the same as the primary components of a probability model: probability spaces, random variables, events.
Once these components are specified, Symbulate allows users to simulate many times from the probability model and summarize the results.

This section contains a brief introduction to Symbulate; more examples can be found throughout the text or in the [Symbulate documentation](https://dlsun.github.io/symbulate/index.html).
Remember to first import Symbulate during a Python session using the command

```{python, eval = FALSE}
from symbulate import *
```

We'll start with a dice rolling example.
Unless indicated otherwise, in this section $X$ represents the sum of two rolls of a fair four-sided die, and $Y$ represents the larger of the two rolls (or the common value if a tie).
We have already discussed a tactile simulation; now we have to implement that process on a computer.

#### Simulating outcomes

The following Symbulate code defines a probability space[^simulation-14] `P` for simulating the 16 equally likely ordered pairs of rolls via a box model.

[^simulation-14]: We primarily view a Symbulate probability space as a description of the probability model rather than an explicit specification of the sample space $\Omega$.
    For example, we define a `BoxModel` instead of creating a set with all possible outcomes.
    We tend to represent a probability space with `P`, even though this is a slight abuse of notation (since $\IP$ typically refers to a probability measure).

```{python dice-sym-boxmodel}

P = BoxModel([1, 2, 3, 4], size = 2, replace = True)

```

The above code tells Symbulate to draw 2 tickets (`size = 2`), with replacement[^simulation-15], from a box with tickets labeled 1, 2, 3, and 4 (entered as the Python list `[1, 2, 3, 4]`).
Each simulated outcome consists of an ordered[^simulation-16] pair of rolls
.

[^simulation-15]: The default argument for `replace` is `True`, so we could have just written `BoxModel([1, 2, 3, 4], size = 2)`.

[^simulation-16]: There is an additional argument `order_matters` which defaults to `True`, but we could set it to `False` for unordered pairs.

The `sim(r)` command simulates \`r\`\` realizations of probability space outcomes (or events or random variables).
Here is the result of one repetition.

```{python}

P.sim(1)

```

And here are the results of 10 repetitions.
(We will typically run thousands of repetitions, or more, but in this section we just run a few repetitions for illustration.)

```{python}

P.sim(10)

```

One important note is that every time `.sim()` is called, a new simulation is run.
**When running a simulation, the "simulate" step should be performed with a single call to `sim`** so that all analysis of results corresponds to the same simulated values.

#### Simulating random variables

A Symbulate `RV` is specified by the probability space on which it is defined and the mapping function which defines it.
Recall that $X$ is the sum of the two dice rolls and $Y$ is the larger (max).

```{python dice-sym-rv}
X = RV(P, sum)
Y = RV(P, max)

```

<!-- Since a random variable $X$ is a function, any `RV` can be called as a function^[The warning you get when you call a `RV` as a function just means that Symbulate is not going to check for you that the inputs to the function you used to define the `RV` actually match up with the outcomes of the probability space `P`.] to return its value $X(\omega)$ for a particular outcome $\omega$ in the probability space. -->

<!-- ```{python} -->

<!-- omega = (3, 2)  # a pair of rolls -->

<!-- X(omega), Y(omega) -->

<!-- ``` -->

The above code simply defines the random variables.
Again, we can simulate values with `.sim()`.
Since every call to `sim` runs a new simulation, we typically store the simulation results as an object.
The following commands simulate 100 values of the random variable `X` and store the results as `x`.
For consistency with standard probability notation[^simulation-17], the random variable itself is denoted with an uppercase letter `Y`, while the realized values of it are denoted with a lowercase letter `y`.

[^simulation-17]: We generally use names in our code that mirror and reinforce standard probability notation, e.g., uppercase letters near the end of the alphabet for random variables, with corresponding lowercase letters for their realized values.
    Of course, these naming conventions are not necessary and you are welcome to use more descriptive names in your code.
    For example, we could have named the probability space `DiceRolls` and the random variables `DiceSum` and `DiceMax` rather than `P, X, Y`.

```{python}
x = X.sim(100)

x # this just displays x

```

#### Simulating multiple random variables

if we call `X.sim(10000)` and `Y.sim(10000)` we get two separate simulations of 10000 pairs of rolls, one which returns the sum of the rolls for each repetition, and the other the max.
If we want to study relationships between $X$ and $Y$ we need to compute both $X$ and $Y$ for each pair of rolls in the same simulation.

We can simulate $(X, Y)$ pairs using[^simulation-18] `&`. We store the simulation output as `x_and_y` to emphasize that `x_and_y` contains pairs of values.

[^simulation-18]: Technically `&` joins two `RV`s together to form a random *vector*.
    While we often interpret Symbulate `RV` as random variable, it really functions as random vector.

```{python}
x_and_y = (X & Y).sim(10)

x_and_y # this just displays x_and_y

```

#### Simulating outcomes and random variables

When calling `X.sim(10)` or `(X & Y).sim(10)` the outcomes of the rolls are generated in the background but not saved.
We can create a `RV` which returns the outcomes of the probability space[^simulation-19].
The default mapping function for `RV` is the identity function, $g(u) = u$, so simulating values of `U = RV(P)` below returns the outcomes of the BoxModel `P` representing the outcome of the two rolls.

[^simulation-19]: You might try `(P & X).sim(10)`.
    But `P` is a probability space object, and `X` is an `RV` object, and `&` can only be used to join like objects together.
    Much like in probability theory in general, in Symbulate the probability space plays a background role, and it is usually random variables we are interested in.

```{python}
U = RV(P)

U.sim(10)

```

Now we can simulate and display the outcomes along with the values of $X$ and $Y$ using `&`.

```{python}
(U & X & Y).sim(10)
```

Because the probability space `P` returns pairs of values, `U = RV(P)` above defines a random vector.
The individual components[^simulation-20] of `U` can be "unpacked" as `U1, U2` in the following.
Here `U1` is an `RV` representing the result of the first roll and `U2` the second.

[^simulation-20]: The components can also be accessed using brackets.
    `U1, U2 = RV(P)` is shorthand for\
    `U = RV(P); U1 = U[0]; U2 = U[1]`.
    Python uses zero-based indexing, so 0 refers to the first component, 1 to the second, and so on.

```{python}
U1, U2 = RV(P)
(U1 & U2 & X & Y).sim(10)
```

#### Simulating events

Events involving random variables can also be defined and simulated.
For programming reasons, events are enclosed in parentheses `()` rather than braces $\{\}$.
For example, we can define the event that the larger of the two rolls is less than 3, $A=\{Y<3\}$, as

```{python}
A = (Y < 3) # an event
```

We can use `sim` to simulate events.
A realization of an event is `True` if the event occurs for the simulated outcome, or `False` if not.

```{python}
A.sim(10)

```

For logical equality use a double equal sign `==`.
For example, `(Y == 3)` represents the event $\{Y=3\}$.

```{python}
(Y == 3).sim(10)

```

#### Simulating transformations of random variables

Transformations of random variables are random variables.
If `X` is a Symbulate `RV` and `g` is a function, then `g(X)` is also a Symbulate `RV`.

For example, we can simulate values of $X^2$.

```{python}
(X ** 2).sim(10)
```

For many common functions, the syntax `g(X)` is sufficient.
Here `exp(u)` is the exponential function $g(u) = e^u$.

```{python}
(X & exp(X)).sim(10)
```

For user defined functions, the syntax is `X.apply(g)`.

```{python}
def g(u):
  return (u - 5) ** 2
  
Z = X.apply(g)

(X & Z).sim(10)

```

(We could have actually defined `Z = (X - 5) ** 2` here.
We'll see examples where the `apply` syntax is necessary later.)

We can also apply transformations of multiple `RV`s *defined on the same probability space*.
(We will look more closely at how Symbulate treats this "same probability space" issue later.)

For example, we can simulate values of $XY$, the product of $X$ and $Y$.

```{python}
(X * Y).sim(10)
```

Recall that we defined $X$ via `X = RV(P, sum)`.
Defining random variables $U_1, U_2$ to represent the individual rolls, we can define $X=U_1 + U_2$.
Recall that we previously defined[^simulation-21] `U1, U2 = RV(P)`.

[^simulation-21]: We can also define `U = RV(P)` and then `X = U.apply(sum)`.

```{python}
X = U1 + U2

X.sim(10)

```

Unfortunately `max(U1, U2)` does not work, but we can use the `apply` syntax.
Since we want to apply `max` to $(U_1, U_2)$ pairs, we must[^simulation-22] first join them together with `&`.

[^simulation-22]: We can also define `U = RV(P)` and then `X = U.apply(max)`.

```{python}
Y  = (U1 & U2).apply(max)

Y.sim(10)

```

#### Two "worlds" in Symbulate

Suppose we want to simulate realizations of the event $A= \{Y < 3\}$.
We saw previously that we can define the event `(Y < 3)` in Symbulate and simulate True/False realizations of it.

```{python}
A = (Y < 3)

A.sim(10)
```

Since event $A$ is defined in terms of $Y$, we can also first simulate values of `Y`, store the results as `y`, and then determine whether event $A$ occurs for the simulated `y` values using[^simulation-23] `(y < 3)`. (The results won't match the above because we are making a new call to `sim`.)

[^simulation-23]: Python automatically treats `True` as 1 and `False` as 0, so the code `(y < 3)` effectively returns both True/False realizations of event itself and 1/0 realizations of the corresponding indicator random variable.

```{python}
y = Y.sim(10)

(y < 3)
```

These two methods illustrate the two "worlds" of Symbulate, which we call "random variable world" and "simulation world".
Operations like transformations can be performed in either world.
Think of random variable world as the "before" world and simulation world as the "after" world, by which we mean before/after the `sim` step.

Most of the transformations we have seen so far happened in random variable world.
For example, we have seen how to define the sum of two dice in random variable world in a few ways, e.g., via

    P = BoxModel([1, 2, 3, 4], size = 2)
    U = RV(P)

    X = U.apply(sum)

The sum transformation is applied to define a new random variable `X`, before the `sim` step.
We could then call, e.g., `(U & X).sim(10000)`.

We could also compute simulated values of the sum in simulation world as follows.

    P = BoxModel([1, 2, 3, 4], size = 2)
    U = RV(P)

    u = U.sim(10000)

    x = u.apply(sum)

The above code will simulate all the pairs of rolls first, store them as `u` (which we can think of as a table), and then apply the sum to the simulated values (which adds a column to the table).
That is, the sum transformation happens after the `sim` step.

While either world is "correct", we generally take the random variable world approach.
We do this mostly for consistency, but also to emphasize some of the probability concepts we'll encounter.
For example, the fact that a sum of random variables (defined on the same probability space) is also a random variable is a little more apparent in random variable world.
However, it is sometimes more convenient to code in simulation world; for example, if complicated transformations are required.

When working in random variable world, it only makes sense to transform or simulate random variables defined on the same probability space.
For example, the following code would return an error.

    X = RV(BoxModel([1, 2, 3, 4], size = 2), sum)
    Y = RV(BoxModel([1, 2, 3, 4], size = 2), max)

    (X & Y).sim(10) # returns an error: "random variables must be defined on the same probability space"

The problem is that there is one box model used to determine values of `X` and a separate box model for `Y`; we can't simulate `(X, Y)` pairs of values if we're using different boxes.

Likewise, in simulation world, it only makes sense to apply transformations to values generated in the same simulation, with a single `sim` step.
For example, the following code would return an error.

    P = BoxModel([1, 2, 3, 4], size = 2)
    U1, U2 = RV(P)

    u1 = U1.sim(10000)
    u2 = U2.sim(10000)

    x = u1 + u2 # returns an error: "objects must come from the same simulation"

We'll discuss reasons for errors like these in more detail as we go.
In short, the "simulation" step should be implemented in a single call to `sim`.

#### Other probability spaces

So far we have assumed a *fair* four-sided die.
Now consider the weighted die in Example \@ref(exm:die-weighted): a single roll results in 1 with probability 1/10, 2 with probability 2/10, 3 with probability 3/10, and 4 with probability 4/10.
`BoxModel` assumes equally likely tickets by default, but we can specify non-equally likely tickets using the `probs` option.
The probability space `WelghtedRoll` in the following code corresponds to a single roll of the weighted die; the default `size` option is 1.

```{python}

WeightedRoll = BoxModel([1, 2, 3, 4], probs = [0.1, 0.2, 0.3, 0.4])
WeightedRoll.sim(10)

```

We could add `size = 2` to the `BoxModel` to create a probability space corresponding to two rolls of the weighted die.
Alternatively, We can also think of `BoxModel([1, 2, 3, 4], probs = [0.1, 0.2, 0.3, 0.4])` as defining the middle spinner in Figure \@ref(fig:die-three-spinners) that we want to spin two times, which we can do with `** 2`.

```{python}
Q = BoxModel([1, 2, 3, 4], probs = [0.1, 0.2, 0.3, 0.4]) ** 2
Q.sim(10)

```

For now you can interpret `BoxModel([1, 2, 3, 4], probs = [0.1, 0.2, 0.3, 0.4])` as defining the middle spinner in Figure \@ref(fig:die-three-spinners) and `** 2` as "spin the spinner two times".
In Python, `**` represents exponentiation; e.g., `2 ** 5 = 32`.
So `BoxModel([1, 2, 3, 4]) ** 2` is equivalent to `BoxModel([1, 2, 3, 4]) * BoxModel([1, 2, 3, 4])`.
Future sections will reveal why the product `*` notation is natural for *independent* spins of spinners.

We could use the product notation to define a probability space corresponding to a pair of rolls: one from a fair die and one from a weighted-die.

```{python}
MixedRolls = BoxModel([1, 2, 3, 4]) * BoxModel([1, 2, 3, 4], probs = [0.1, 0.2, 0.3, 0.4])
MixedRolls.sim(10)
```

Now consider the weighted die from Example \@ref(exm:dice-normalize), represented by the spinner on the right in Figure \@ref(fig:die-three-spinners).
We could use the `probs` option, but we can also imagine a box model with 15 tickets --- four tickets labeled 1, six tickets labeled 2, three tickets labeled 3, and two tickets labeled 4 --- from which a single ticket is drawn.
A `BoxModel` can be specified in this way using the following `{label: number of tickets with the label}` formulation[^simulation-24].
This formulation is especially useful when multiple tickets are drawn from the box *without replacement*.

[^simulation-24]: Braces `{}` are used here because this defines a Python *dictionary*.
    But don't confuse this code with set notation.

```{python}

Q = BoxModel({1: 4, 2: 6, 3: 3, 4: 2})
Q.sim(10)

```

While many scenarios can be represented by box models, there are also many Symbulate probability spaces other than `BoxModel`.
When tickets are equally likely and sampled with replacement, a **Discrete Uniform** model can also be used.
Think of a `DiscreteUniform(a, b)` probability space corresponding to a spinner with sectors of *equal area* labeled with integer values from `a` to `b` (inclusive).
For example, the spinner in Figure \@ref(fig:spinner-die) corresponds to `DiscreteUniform(1, 4)`.
This gives us another way to represent the probability space corresponding to two rolls of a fair-four sided die.

```{python}
P = DiscreteUniform(1, 4) ** 2
P.sim(10)
```

Note that `BoxModel` is the only probability space with the `size` argument.
For other probability spaces, the product `*` or exponentiation `**` notation must be used to simulate multiple spins.

### Computer simulation: Meeting problem {#sec-meeting-sim}

Now we'll consider a continuous example.

Regina and Cady plan to meet for lunch between noon and 1 but they are not sure of their arrival times.
Recall the sample space from Example \@ref(exm:meeting-outcome).
Let $R$ be the random variable representing Regina's arrival time (minutes after noon), and $Y$ for Cady.
Also let $T=\min(R, Y)$ be the time (minutes after noon) at which the first person arrives, and $W=|R-Y|$ be the time (minutes) the first person to arrive waits for the second person arrives.
We'll simulate these random variables under different assumptions, like those in Section \@ref(sec-meeting-probspace).

Suppose that Regina and Cady arrive uniformly at random between time 0 and 60, independently of each other.
Uniform probability measures are the continuous analog of equally likely outcomes.
The standard uniform model is the Uniform(0, 1) distribution corresponding to the spinner in Figure \@ref(fig:uniform-spinner) which returns values between[^simulation-25] 0 and 1.
Recall that the values in the picture are rounded to two decimal places, but the spinner represents an idealized model where the spinner is infinitely precise so that any real number between 0 and 1 is a possible value.
We assume that the (infinitely fine) needle is "equally likely" to land on any value between 0 and 1.
A Uniform($a$, $b$) model[^simulation-26] is defined similarly for the interval $[a, b]$.

[^simulation-25]: Why is the interval $[0, 1]$ the standard instead of some other range of values?
    Because probabilities take values in $[0, 1]$.
    We will see why this is useful in more detail later.

[^simulation-26]: Careful: don't confuse `Uniform(a, b)` with `DiscreteUniform(a, b)`.
    `Uniform(a, b)` corresponds to the continuous interval $[a, b]$.

The following code defines a Uniform(0, 60) spinner, like in Figure \@ref(fig:meeting-uniform-spinner), which we spin twice to get the (Regina, Cady) pair of outcomes.

```{python}
P = Uniform(0, 60) ** 2
P.sim(10)

```

Notice the number of decimal places.
For the Uniform(0, 60) model, any value in the continuous interval between 0 and 60 is a distinct possible value: 10.25000000000... is different from 10.25000000001... is distinct from 10.2500000000000000000001... and so on.

A probability space outcome is a (Regina, Cady) pair of arrival times.
We can define the random variables $R$ and $Y$, representing the individual arrive times, by "unpacking" the outcomes.

```{python}
R, Y = RV(P)
(R & Y).sim(10)

```

We can define $W = |R-Y|$ using the `abs` function.
In order to define $T = \min(R, Y)$ we need to use the `apply` syntax with `R & Y`.

```{python}
W = abs(R - Y)

T = (R & Y).apply(min)

(R & Y & T & W).sim(10)

```

Before investigating other assumptions, let's look at plots of some simulated values.
We can simulate values of $R$ and plot them along a number line.
(This plot is called a "rug" plot; we'll see some more useful plots soon.)


```{python, eval = FALSE}
R.sim(100).plot('rug')
```

```{python, echo = FALSE}
plt.figure()
R.sim(100).plot('rug')
plt.show()
```

The simulated values are roughly evenly spaced between 0 and 60, though there is some natural variability.

We can simulate and plot many $(R, Y)$ pairs of arrival times.

```{python, eval = FALSE}
(R & Y).sim(1000).plot()
```

```{python, echo = FALSE}
plt.figure()
(R & Y).sim(1000).plot()
plt.show()
```

We see that the pairs are fairly evenly distributed throughout the square with sides [0, 60] (which represents the sample space).
If we simulated more values and summarized them in an appropriate plot, we would expect to see something like Figure \@ref(fig:meeting-probmeasure1).

Now suppose Regina is more likely to arrive around 12:30 than around 12:00 or 1:00.
One way to model this assumption is with a Normal(30, 10) model, represented by the spinner in like in Figure \@ref(fig:meeting-normal-spinner).

```{python}
Normal(mean = 30, sd = 10).sim(10)
```

Now we define a probability space corresponding to (Regina, Cady) pairs of arrival times, by assuming that their arrival times individually follow a Normal(30, 10) model, independently of the other.
That is, we spin the Normal(30, 10) spinner twice to simulate a pair of arrival times.

```{python}
P = Normal(mean = 30, sd = 10) ** 2
P.sim(10)
```

We can unpack the individual $R$, $Y$ random variables as before.

```{python}
R, Y = RV(P)
```


Again, we can simulate values and plot them.
Compared to the rug plot based on the Uniform model, the rug plot based on the Normal model shows that more simulated $R$ values tend to be close to 30 than near 0 or 60.

```{python, eval = FALSE}
R.sim(100).plot('rug')
```

```{python, echo = FALSE}
plt.figure()
R.sim(100).plot('rug')
plt.show()
```

We can simulate and plot $(R, Y)$ pairs.


```{python, eval = FALSE}
(R & Y).sim(1000).plot()
```

```{python, echo = FALSE}
R, Y = RV(P)

plt.figure()
(R & Y).sim(1000).plot()
plt.show()
```

We see that more pairs are located near (30, 30), similar to Figure \@ref(fig:meeting-probmeasure2).
(We also might see some values outside of [0, 60].)

Now suppose we want to also assume that Regina and Cady tend to arrive around the same time.
One way to model pairs of values that have correlation is with a BivariateNormal distribution, like in the following.

```{python}
P = BivariateNormal(mean1 = 30, sd1 = 10, mean2 = 30, sd2 = 10, corr = 0.7)
P.sim(10)
```

Note that the BivariateNormal probability space returns pairs directly.
We can unpack the pairs, and plot some simulated values.

```{python, eval = FALSE}
R, Y = RV(P)

(R & Y).sim(1000).plot()
```

```{python, echo = FALSE}
R, Y = RV(P)

plt.figure()
(R & Y).sim(1000).plot()
plt.show()
```

Now we see that the points tend to follow the $R = Y$ line, so that Regina and Cady tend to arrive near the same time, similar to Figure \@ref(fig:meeting-probmeasure3).

This section has provided some basic examples of how to define probability spaces and random variables in Symbulate.
We will study specific probability models like Normal and BivariateNormal in much more detail as we go.



## Relative frequencies


A simulation involves repeatedly artificially recreating the random phenomenon a large number of times and using the results to investigate properties of interest.
In particular, we can use simulation-based relative frequencies to approximate probabilities.
That is, the probability of event $A$ can be approximated by simulating, according to the assumptions corresponding to the probability measure $\IP$, the random phenomenon a large number of times and computing the relative frequency of $A$.

$$
{\small
\IP(A) \approx \frac{\text{number of repetitions on which $A$ occurs}}{\text{number of repetitions}}, \quad \text{for a large number of repetitions simulated according to $\IP$}
}
$$

In practice, many repetitions of a simulation are performed on a computer to approximate what happens in the "long run".
However, we often start by carrying out a few repetitions by hand to help make the process more concrete.

```{example dice-sim-tactile}

Use a four-sided die (or a box or a spinner) and perform by hand 10 repetitions of the simulation in Example \@ref(exm:dice-sim).
(Yes, really do it.)
For each repetition, record the results of the first and second rolls (or draws or spins) and the values of $X$ and $Y$.
Based only on the results of your simulation, how would you approximate the following?
(Don't worry if the approximations are any good yet.)

```

1.  $\IP(A)$, where $A$ is the event that the first roll is 3.
2.  $\IP(X=6)$
3.  $\IP(X \ge 6)$
4.  $\IP(Y = 3)$
5.  $\IP(Y \ge 3)$
6.  $\IP(X=6, Y=3)$
7.  $\IP(X\ge6, Y \ge 3)$

```{solution dice-sim-tactile-sol}

to Example \@ref(exm:dice-sim-tactile).  


```

```{r, echo = FALSE}

u1 = c(2, 1, 3, 4, 3, 3, 2, 2, 1, 3)
u2 = c(1, 1, 3, 3, 2, 4, 3, 4, 2, 4)
x = u1 + u2
y = pmax(u1, u2)
A = ifelse(u1 == 3, "True", "False")
IA = as.numeric(u1 == 3)

die_df = data.frame(1:10, u1, u2, x, y, A, IA)


```

```{asis, fold.chunk = TRUE}

See Table \@ref(tab:dice-sim-tactile-results) for the results of our simulation.

1. Approximate $\IP(A)$ by 4/10, the relative frequency of event $A$ in the simulation; that is, the proportion of repetitions where the first roll is 3.
1. Approximate $\IP(X=6)$ by 2/10, the proportion of repetitions where the sum is 6.
1. Approximate $\IP(X\ge 6)$ by 5/10, the proportion of repetitions where the sum is at least 6.
1. Approximate $\IP(Y=3)$ by 3/10, the proportion of repetitions where the max is 3.
1. Approximate $\IP(Y\ge 3)$ by 7/10, the proportion of repetitions where the max is at least 3.
1. Approximate $\IP(X=6, Y = 3)$ by 1/10, the proportion of repetitions where both the sum is 6 and the max is 3.
1. Approximate $\IP(X\ge 6, Y \ge 3)$ by 5/10, the proportion of repetitions where both the sum is at least 6 and the max is at least 3.
(Since $X\ge 6$ implies $Y\ge 3$, $\IP(X\ge 6, Y\ge 3) = \IP(X\ge 6)$.)

```

Table \@ref(tab:dice-sim-tactile-results) summarizes the results of 10 repetitions of the simulation in Example \@ref(exm:dice-sim-tactile).
Results vary naturally so your simulation results will be different, but the same ideas apply.

```{r, dice-sim-tactile-results, echo = FALSE}


knitr::kable(
  die_df, booktabs = TRUE,
  col.names = c("Repetition", "First roll", "Second roll", "X", "Y", "Event A occurs?", expression(I[A])),
  caption = "Results of 10 repetitions of two rolls of a fair four-sided die. X is the sum of the two rolls, Y is the maximum, and A is the event that the first roll is a 3."
)

```

Remember that it is important to distinguish between what entails (1) one repetition of the simulation and its output, and (2) the simulation itself and output from many repetitions.
When describing a simulation, refrain from making vague statements like "repeat this" or "do it again", because "this" or "it" could refer to different elements of the simulation.
In the dice example, (1) rolling a die is repeated to generate a single $(X, Y)$ pair, and (2) the process of generating $(X, Y)$ pairs is repeated to obtain the simulation[^simulation-29] results.
That is, a single repetition involves an ordered pair of die rolls, resulting in an outcome $\omega$, and the values of the sum $X(\omega)$ and max $Y(\omega)$ are computed for the outcome $\omega$.
The process described in the previous sentence is repeated many times to generate many outcomes and $(X, Y)$ pairs according to the probability model.

[^simulation-29]: Do we perform "a simulation", or "many simulations"?
    Throughout, "a simulation" refers to the collection of results corresponding to repeatedly artificially recreating the random process.
    "A repetition" refers to a single artificial recreation resulting in a single simulated outcome.
    We perform a simulation which consists of many repetitions.

Think of simulation results being organized in a table like Table \@ref(tab:dice-sim-tactile-results), where each row corresponds to a different repetition of the simulation and each column corresponds to a different random variable or event.
Remember that indicators are the bridge between events and random variables.
On each repetition of the simulation an event either occurs or not.
We could record the occurrence of an event as "True/False" for each repetition, or we could record the 1/0 value of the corresponding indicator random variable; see the last two columns in Table \@ref(tab:dice-sim-tactile-results) for an example.

```{example dice-sim-tactile-dist}
Continuing Example \@ref(exm:dice-sim-tactile).

1. Construct a table of the simulated relative frequencies of each possible value $x$ of $X$.
1. Construct a table of the simulated relative frequencies of each possible value $(x, y)$ pair of $(X, y)$.

```

```{solution dice-sim-tactile-dist-sol}

to Example \@ref(exm:dice-sim-tactile-dist).  


```

```{asis, fold.chunk = TRUE}

For discrete random variables like these we can make tables or plots summarizing the observed values of the random variables and their corresponding relative frequencies.

Summarizing our simulation results from Table \@ref(tab:dice-sim-tactile-results), the observed values of $X$ and corresponding relative frequencies are

| $x$ | Relative frequency |
|-----|-------------------:|
| 2   |               1/10 |
| 3   |               2/10 |
| 4   |                  0 |
| 5   |               2/10 |
| 6   |               2/10 |
| 7   |               3/10 |
| 8   |                  0 |
  
The above table^[We would typically only include the values observed in the simulation in the summary. However, we include 4 and 8 here because if we had performed more repetitions we would have observed these values.] represents an approximation of the distribution of $X$ (albeit a bad approximation); compare with Table \@ref(tab:dice-sum-dist-table).

We can summarize simulated $(X, Y)$ *pairs* and their relative frequencies, as in the following two-way table (compare with Table \@ref(tab:dice-joint-dist-twoway).

| $x, y$ |    1 |    2 |    3 |    4 |
|--------|-----:|-----:|-----:|-----:|
| 2      | 1/10 |    0 |    0 |    0 |
| 3      |    0 | 2/10 |    0 |    0 |
| 4      |    0 |    0 |    0 |    0 |
| 5      |    0 |    0 | 2/10 |    0 |
| 6      |    0 |    0 | 1/10 | 1/10 |
| 7      |    0 |    0 |    0 | 3/10 |
| 8      |    0 |    0 |    0 |    0 |

  
```

Simulation results are summarized in tables and plots.
Figure \@ref(fig:dice-sim-tactile-results-plot) displays two plots summarizing the results in Table \@ref(tab:dice-sim-tactile-results).
Each dot represents the results of one repetition; the plot on the left displays the simulated $(X, Y)$ pairs, and the plot on the right displays the simulated values of $X$ alone along with their frequencies.
While this simulation only consists of 10 repetitions, a larger scale simulation and the summarization of results would follow the same process.

(ref:cap-dice-sim-tactile) Plot summaries of the simulation results in Table \@ref(tab:dice-sim-tactile-results) of 10 repetitions of two rolls of a fair four-sided die, where $X$ is the sum and $Y$ is the larger (or common value if a tie) of the two rolls.

```{r dice-sim-tactile-results-plot, echo = FALSE, fig.show = 'hold', out.width = '50%', fig.cap = "(ref:cap-dice-sim-tactile)"}

plot(x + c(0, 0, 0, 0, 0, 0.1, 0.1, 0, 0.1, -0.1), y,
     xlab = "X", ylab = "Y", xaxt = 'n', yaxt = 'n',
     xlim = c(1.5, 8.5), ylim = c(0.5, 4.5), xaxs = "i", yaxs = "i")
segments(x0 = 2.5:7.5, y0 = 0, x1 = 2.5:7.5, y1 = 5, lty = 3)
segments(x0 = 1.5, y0 = 0.5:4.5, x1 = 8.5, y1 = 0.5:4.5, lty = 3)
axis(1, at = 2:8, tck = 0)
axis(2, at = 1:4, tck = 0)


stripchart(x, method = "stack",
           xaxt = 'n', xlim = c(2, 8),
           offset = .5, at = .15, pch = 1, 
           xlab = "X", ylab = "Number of repetitions")
axis(1, at = 2:8)


```

You might have noticed that many of the simulated relative frequencies in Example \@ref(exm:dice-sim-tactile) provide terrible estimates of the corresponding probabilities.
For example, the true probability that the first roll is a 3 is $\IP(A) = 0.25$ while the simulated relative frequency is 0.4.
The problem is that the simulation only consisted of 10 repetitions.
Probabilities can be approximated by *long run* relatively frequencies, but 10 repetitions certainly doesn't qualify as the long run!
The more repetitions we perform the better our estimates should be.
But how many repetitions is sufficient?
And how accurate are the estimates?
We will address these issues in Section \@ref(moe).



### A few Symbulate commands for summarizing simulation output


We'll continue with the dice rolling example.
Recall the setup.

```{python}
P = DiscreteUniform(1, 4) ** 2

X = RV(P, sum)

Y = RV(P, max)
```

First we'll simulate and store 10 values of $X$.

```{python}
x = X.sim(10)
x # displays the simulated values
```

Suppose we want to find the relative frequency of 6. We can count the number of simulated values equal to 6 with `count_eq()`.

```{python}
x.count_eq(6)
```

The count is the frequency. To find the relative frequency we simply divide by the number of simulated values.

```{python}
x.count_eq(6) / 10
```

We can find frequencies of other events using the "count" functions:

-   `count_eq(u)`: count *equal to* u
-   `count_neq(u)`: count *not equal to* u
-   `count_leq(u)`: count *less than or equal to* u
-   `count_lt(u)`: count *less than* u
-   `count_geq(u)`: count *greater than or equal to* u
-   `count_gt(u)`: count *greater than* u
-   `count`: count according to a specified True/False criteria.

Using `count()` with no inputs to defaults to "count all", which provides a way to count the total number of simulated values.
(This is especially useful when conditioning.)


```{python}
x.count_eq(6) / x.count()
```



The `tabulate` method provides a quick summary of the individual simulated values and their frequencies.

```{python}
x.tabulate()

```

By default, `tabulate` returns frequencies (counts).
Adding the argument[^simulation-27] `normalize = True` returns relative frequencies (proportions).

[^simulation-27]: "Normalize" is used in the sense of Section \@ref(consistency) and refers to rescaling the values so that they add up to 1 but the ratios are preserved.

```{python}
x.tabulate(normalize = True)

```


We often initially simulate a small number of repetitions to see what the simulation is doing and check that it is working properly.
However, in order to accurately approximate probabilities or distribution we simulate a large number of repetitions (usually thousands for our purposes).
Now let's simulate many $X$ values and summarize the results.

```{python}
x = X.sim(10000)

x.tabulate(normalize = True)
```

Compare the simulation results to the theoretical probabilities in Table \@ref(tab:dice-sum-dist-table). 

Graphical summaries play an important role in analyzing simulation output.
We have previously seen rug plots of individual simulated values.
Rug plot emphasize that realizations of a random variable  are numbers along a number line.
However, a rug plot does not adequately summarize relative frequencies.
Instead, calling `.plot()` produces^[For discrete random variables `'impulse'` is the default plot type. Like `.tabulate()`, the `.plot()` method also has a `normalize` argument; the default is `normalize=True`.] an *impulse plot* which displays the simulated values and their relative frequencies; see Figure \@ref(fig:dice-sum-marginal-sim). 
Since the simulated values are stored as `x`, the same simulated values are used to produce the table and the plot.


(ref:cap-dice-sum-marginal-sim) Simulation-based approximate distribution of $X$, the sum of two rolls of a fair four-sided die.


```{python, eval = FALSE}
x.plot()
```

```{python, echo = FALSE, dice-sum-marginal-sim, fig.cap = "(ref:cap-dice-sum-marginal-sim)"}

plt.figure()
x.plot()
plt.show()

```




We will introduce several more plot types and commands for summarizing simulation output in following sections.

Figure \@ref(fig:dice-max-marginal-sim) contains a summary of a simulated $Y$ values.  Compare with Table \@ref(tab:dice-max-dist-table).


(ref:cap-dice-max-marginal-sim) Simulation-based approximate distribution of $Y$, the larger (or common value if a tie) of two rolls of a fair four-sided die.

```{python}

y = Y.sim(10000)
y.tabulate()

```

```{python, eval = FALSE}
y.plot()
```


```{python dice-max-marginal-sim, echo = FALSE, fig.cap = "(ref:cap-dice-max-marginal-sim)"}

plt.figure()
y.plot()
plt.show()

```


Now we simulate and summarize a few  $(X, Y)$ pairs.

```{python}
x_and_y = (X & Y).sim(10)
x_and_y 

```



Pairs of values can also be tabulated.

```{python}
x_and_y.tabulate()

```


```{python}
x_and_y.tabulate(normalize = True)

```

Individual pairs can be plotted in a scatter plot, which is a two-dimensional analog of a rug plot.

```{python, eval = FALSE}
x_and_y.plot()

```

```{python, echo = FALSE}
plt.figure()
x_and_y.plot()
plt.show()

```


The values can be "jittered" slightly, as below, so that points do not coincide.


```{python, eval = FALSE}
x_and_y.plot(jitter = True)

```

```{python, echo = FALSE}
plt.figure()
x_and_y.plot(jitter = True)
plt.show()

```

The two-dimensional analog of an impulse plot is a *tile plot*. For two discrete variables, the `'tile'` plot type produces a tile plot (a.k.a. heat map) where rectangles represent the simulated pairs with their relative frequencies visualized on a color scale.

```{python, echo = FALSE}
plt.figure()
x_and_y.plot('tile')
plt.show()

```

```{python, eval = FALSE}
x_and_y.plot('tile')

```

<!-- (ref:cap-dice-tile) Tile plot visualization of the simulation-based approximate joint distribution of the sum ($X$) and larger ($Y$) of two rolls of a fair four-sided die.  Color intensity represents relative frequencies of pairs. -->


<!-- ```{r, dice-tile, echo = FALSE, fig.cap = "(ref:cap-dice-tile)"} -->

<!-- knitr::include_graphics("_graphics/dice-tile.png") -->
<!-- ``` -->

<!-- We can add the impulse plot for each of $X$ and $Y$ in the margins of the tile plot using the `'marginal'` argument.   -->

<!-- ```{python, eval = FALSE} -->
<!-- xy.plot(['tile', 'marginal']) -->
<!-- plt.show() -->

<!-- ``` -->

<!-- (ref:cap-dice-tile-marginal) Tile and impulse plot visualization of the simulation-based approximate joint and marginal distributions of the sum ($X$) and larger ($Y$) of two rolls of a fair four-sided die. -->




<!-- ```{r, dice-tile-marginal, echo = FALSE, fig.cap = "(ref:cap-dice-tile-marginal)"} -->

<!-- knitr::include_graphics("_graphics/dice-tile-marginal.png") -->
<!-- ``` -->


Custom functions can be used with `count` to compute relative frequencies of events involving multiple random variables. Suppose we want to approximate $\IP(X<6, Y \ge 2)$.  We first define a Python function which takes as an input a pair `u = (u[0], u[1])` and returns `True` if `u[0] < 6` and `u[1] >= 2`.

```{python}

def is_x_lt_6_and_y_ge_2(u):
  if u[0] < 6 and u[1] >= 2:
    return True
  else:
    return False

```

Now we can use this function along with `count` to find the simulated relative frequency of the event $\{X <6, Y \ge 2\}$.
Remember that `x_and_y` stores $(X, Y)$ pairs of values, so the first coordinate `x_and_y[0]` represents values of $X$ and the second coordinate `x_and_y[1]` represents values of $Y$.

```{python}
x_and_y.count(is_x_lt_6_and_y_ge_2) / x_and_y.count()
```

We could also count use Boolean logic; basically using indicators and the property $\ind_{\{X<6,Y\ge 2\}}=\ind_{\{X<6\}}\ind_{\{Y\ge 2\}}$.

```{python}
((x_and_y[0] < 6) * (x_and_y[1] >= 2)).count_eq(True) / x_and_y.count()
```


Now we simulate and many $(X, Y)$ pairs and summarize their frequencies.

```{python}
x_and_y = (X & Y).sim(10000)

x_and_y.tabulate()
```

Here are the relative frequencies; compare with Table \@ref(tab:dice-joint-dist-flat).

```{python}
x_and_y.tabulate(normalize = True)
```

When there are thousands of simulated pairs, a scatter plot does not adequately display relative frequencies, even with jittering.

```{python, eval = FALSE}
x_and_y.plot(jitter = True)

```

```{python, echo = FALSE}
plt.figure()
x_and_y.plot(jitter = True)
plt.show()

```




 
The tile plot provides a better summary.
Notice how the colors represent the relative frequencies in the previous table.

```{python, echo = FALSE}
plt.figure()
x_and_y.plot('tile')
plt.show()

```

```{python, eval = FALSE}
x_and_y.plot('tile')

```

Finally, we find the simulated relative frequency of the event $\{X <6, Y \ge 2\}$.

```{python}
x_and_y.count(is_x_lt_6_and_y_ge_2) / x_and_y.count()
```


```{example meeting-sim-intro}

Recall the meeting problem. Use simulation to approximate the probability that Regina and Cady arrive with 15 minutes of each other for each of the three models in Section \@ref(sec-meeting-sim). 

```

1.  Uniform model
1.  Normal model
1.  Bivariate Normal model


```{solution meeting-sim-intro-sol}

to Example \@ref(exm:meeting-sim-intro).  

```

```{asis, fold.chunk = TRUE}

There are a few ways to code this, but it is easiest to define the waiting time variable $W$ and find simulated relative frequencies of the event $\{W<15\}$.

```

First the Uniform model.

```{python}

P = Uniform(0, 60) ** 2
R, Y = RV(P)

W = abs(R - Y)

w = W.sim(10000)

w.count_lt(15) / w.count()

```


Next the Normal model.

```{python}

P = Normal(30, 10) ** 2
R, Y = RV(P)

W = abs(R - Y)

w = W.sim(10000)

w.count_lt(15) / w.count()

```

Finally the Bivariate Normal model.

```{python}

P = BivariateNormal(mean1 = 30, sd1 = 10, mean2 = 30, sd2 = 10, corr = 0.7)
R, Y = RV(P)

W = abs(R - Y)

w = W.sim(10000)

w.count_lt(15) / w.count()

```

We see that changing the assumptions changes the probability.

We'll cover summarizing simulations of continuous random variables in much more detail later.

### Approximating probabilities: Simulation margin of error {#moe}

The probability of an event can be approximated by simulating the random phenomenon a large number of times and computing the relative frequency of the event.
After enough repetitions we expect the simulated relative frequency to be *close to* the true probability, but there probably won't be an exact match.
Therefore, in addition to reporting the approximate probability, we should also provide a margin of error which indicates how close we think our simulated relative frequency is to the true probability.

Section \@ref(rel-freq) introduced the relative frequency interpretation in the context of flipping a fair coin.
After many flips of a fair coin, we expect the proportion of flips resulting in H to be close to 0.5.
But how many flips is enough?
And how "close" to 0.5?
We'll investigate these questions now.

```{r, echo = FALSE, cache = TRUE, warning=FALSE, message=FALSE}

N = 100
n = 10000
p = 0.5
ci = 0.95
z = 2


phat = data.frame(phat = rbinom(N, n, p) / n)

phat = phat %>%
  mutate(ci_lb = phat - z * sqrt(p * (1 - p) / n),
         ci_ub = phat + z * sqrt(p * (1 - p) / n),
         good_ci = abs(phat - p) <= z * sqrt(p * (1 - p) / n))

plot_coin_phat_n1 <- ggplot(phat,
                            aes(x = phat,
                                color = good_ci,
                                fill = good_ci)) +
  geom_dotplot() +
  theme_classic() +
  theme(axis.line = element_line(color = "black"),
        legend.position = "none") +
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(breaks = p - seq(-3, 3, 0.5) * sqrt(p * (1 - p) / n)) +
  geom_vline(xintercept = p + c(-1, 1) * z * sqrt(p * (1 - p) / n),
             linetype = "dotted", color = "black", size = 1.5) +
  labs(x = "Proportion of heads")

num_good_ci_n1 <- phat %>% select(good_ci) %>% sum()

num_good_phat <- phat %>% filter(phat == p) %>% count()

```

Consider Figure \@ref(fig:coin-sim1) below.
Each dot represents a set of 10,000 fair coin flips.
There are 100 dots displayed, representing 100 different sets of 10,000 coin flips each.
For each set of flips, the proportion of the 10,000 flips which landed on head is recorded.
For example, if in one set 4973 out of 10,000 flips landed on heads, the proportion of heads is 0.4973.
The plot displays 100 such proportions; similar values have been "binned" together for plotting.
We see that `r num_good_ci_n1` of these 100 proportions are between 0.49 and 0.51, represented by the blue dots.
So if "between 0.49 and 0.51" is considered "close to 0.5", then yes, in 10000 coin flips we would expect[^simulation-30] the proportion of heads to be close to 0.5.

[^simulation-30]: In 10000 flips, the probability of heads on between 49% and 51% of flips is 0.956, so `r num_good_ci_n1` out of 100 provides a rough estimate of this probability.
    We will see how to compute such a probability later.

(ref:cap-coin-sim1) Proportion of flips which are heads in 100 sets of **10,000** fair coin flips. Each dot represents a set of **10,000** fair coin flips. In `r num_good_ci_n1` of these 100 sets the proportion of heads is between 0.49 and 0.51 (the blue dots).

```{r coin-sim1, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-coin-sim1)"}

print(plot_coin_phat_n1)

```

Our discussion of Figure \@ref(fig:coin-sim1) suggests that 0.01 might be an appropriate margin of error for a simulation based on 10,000 flips.
Suppose we perform a simulation of 10000 flips with 4973 landing on heads.
We could say "we estimate that the probability that a coin land on heads is equal to 0.4973".
But such a precise estimate would almost certainly be incorrect, due to natural variability in the simulation.
In fact, only `r num_good_phat` sets[^simulation-31] resulted in a proportion of heads exactly equal to the true probability of 0.5.

[^simulation-31]: This is difficult to see in Figure \@ref(fig:coin-sim1) due to the binning.
    But we can at least tell from Figure \@ref(fig:coin-sim1) that at most a handful of the 100 sets resulted in a proportion of heads exactly equal to 0.5.

A better statement would be "we estimate that the probability that a coin land on heads is 0.4973 *with a margin of error*[^simulation-32] *of 0.01*".
This means that we estimate that the true probability of heads is within 0.01 of 0.4973.
In other words, we estimate that the true probability of heads is between 0.4873 and 0.5073, an interval whose endpoints are $0.4973 \pm 0.01$.
This interval estimate is "accurate" in the sense that the true probability of heads, 0.5, *is* between 0.4873 and 0.5073.
By providing a margin of error, we have sacrificed a little precision --- "equal to 0.4973" versus "within 0.01 of 0.4973" --- to achieve greater accuracy.

[^simulation-32]: Technically, we should say "a margin of error *for 95% confidence* of 0.01".
    We'll discuss "confidence" in a little more detail soon.

Let's explore this idea of "accuracy" further.
Recall that Figure \@ref(fig:coin-sim1) displays the proportion of flips which landed on heads for 100 sets of 10000 flips each.
Suppose that for each of these sets we form an interval estimate of the probability that the coin lands on heads by adding/subtracting 0.01 from the simulated proportion, as we did for $0.4973 \pm 0.01$ in the previous paragraph.
Figure \@ref(fig:coin-sim1-ci) displays the results.
Even though the proportion of heads was equal to 0.5 in only `r num_good_phat` sets, in `r num_good_ci_n1` of these 100 sets (the blue dots/intervals) the corresponding interval contains 0.5, the true probability of heads.
For almost all of the sets, the interval formed via "relative frequency $\pm$ margin of error" provides an accurate estimate of the true probability.
However, not all the intervals contain the true probability, which is why we often qualify that our margin of error is for "95% confidence" or "95% accuracy".
We will see more about "confidence" soon.
In any case, the discussion so far, and the results in Figure \@ref(fig:coin-sim1) and Figure \@ref(fig:coin-sim1-ci), suggest that 0.01 is a reasonable choice for margin of error when estimating the probability that a coin lands on heads based on 10000 flips.

(ref:cap-coin-sim1-ci) Interval estimates of the probability of heads based on 100 sets of **10,000** fair coin flips. Each dot represents the proportion of heads in a set of **10,000** fair coin flips. (The sets have been sorted based on their proportion of heads.) For each set an interval is obtained by adding/subtracting the margin of error of 0.01 from the proportion of heads. In `r num_good_ci_n1` of these 100 sets (the blue dots/intervals) the corresponding interval contains the true probability of heads (0.5, represented by the vertical black line).

```{r coin-sim1-ci, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-coin-sim1-ci)"}

plot_coin_phat_n1_ci <- ggplot(phat %>%
                                 arrange(phat) %>%
                                 mutate(repetition = row_number()),
                               aes(x = phat,
                                   y = repetition,
                                   color = good_ci)) +
  geom_point() +
  geom_segment(aes(x = ci_lb, xend = ci_ub,
                   y = repetition, yend = repetition,
                   color = good_ci)) +
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
    geom_vline(xintercept = p, color = "black", size = 1.0) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "Proportion of heads, with margin of error",
       y = "Set")

print(plot_coin_phat_n1_ci)
```

```{r, echo = FALSE, cache = TRUE, warning=FALSE, message=FALSE}

N = 100
n = 1000000
p = 0.5
ci = 0.95
z = 2


phat = data.frame(phat = rbinom(N, n, p) / n)

phat = phat %>%
  mutate(ci_lb = phat - z * sqrt(p * (1 - p) / n),
         ci_ub = phat + z * sqrt(p * (1 - p) / n),
         good_ci = abs(phat - p) <= z * sqrt(p * (1 - p) / n))

plot_coin_phat_n2 <- ggplot(phat,
                            aes(x = phat,
                                color = good_ci,
                                fill = good_ci)) +
  geom_dotplot() +
  theme_classic() +
  theme(axis.line = element_line(color = "black"),
        legend.position = "none") +
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(breaks = p - seq(-3, 3, 0.5) * sqrt(p * (1 - p) / n)) +
  geom_vline(xintercept = p + c(-1, 1) * z * sqrt(p * (1 - p) / n),
             linetype = "dotted", color = "black", size = 1.5) +
  labs(x = "Proportion of heads")

num_good_ci_n2 <- phat %>% select(good_ci) %>% sum()

```

What if we want to be stricter about what qualifies as "close to 0.5"?
That is, what if a margin of error of 0.01 isn't good enough?
You might suspect that with even more flips we would expect to observe heads on even closer to 50% of flips.
Indeed, this is the case.
Figure \@ref(fig:coin-sim2) displays the results of 100 sets of *1,000,000* fair coin flips.
The pattern seems similar to Figure \@ref(fig:coin-sim1) but pay close attention to the horizontal axis which covers a much shorter range of values than in the previous figure.
Now `r num_good_ci_n2` of the 100 proportions are between *0.499 and 0.501*.
So in 1,000,000 flips we would expect[^simulation-33] the proportion of heads to be between 0.499 and 0.501, pretty close to 0.5.
This suggests that 0.001 might be an appropriate margin of error for a simulation based on 1,000,000 flips.

[^simulation-33]: In 1,000,000 flips, the probability of heads on between 49.9% and 50.1% of flips is 0.955, and `r num_good_ci_n2` out of 100 sets provides a rough estimate of this probability.

(ref:cap-coin-sim2) Proportion of flips which are heads in 100 sets of **1,000,000** fair coin flips. Each dot represents a set of **1,000,000** fair coin flips. In `r num_good_ci_n2` of these 100 sets the proportion of heads is between 0.499 and 0.501 (the blue dots).

```{r coin-sim2, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-coin-sim2)"}

print(plot_coin_phat_n2)

```

```{r, echo = FALSE, cache = TRUE, warning=FALSE, message=FALSE}

N = 100
n = 100000000
p = 0.5
ci = 0.95
z = 2


phat = data.frame(phat = rbinom(N, n, p) / n)

phat = phat %>%
  mutate(ci_lb = phat - z * sqrt(p * (1 - p) / n),
         ci_ub = phat + z * sqrt(p * (1 - p) / n),
         good_ci = abs(phat - p) <= z * sqrt(p * (1 - p) / n))

plot_coin_phat_n3 <- ggplot(phat,
                            aes(x = phat,
                                color = good_ci,
                                fill = good_ci)) +
  geom_dotplot() +
  theme_classic() +
  theme(axis.line = element_line(color = "black"),
        legend.position = "none") +
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(breaks = p - seq(-3, 3, 0.5) * sqrt(p * (1 - p) / n)) +
  geom_vline(xintercept = p + c(-1, 1) * z * sqrt(p * (1 - p) / n),
             linetype = "dotted", color = "black", size = 1.5) +
  labs(x = "Proportion of heads")

num_good_ci_n3 <- phat %>% select(good_ci) %>% sum()

```

What about even more flips?
In Figure \@ref(fig:coin-sim3) each dot represents a set of *100 million* flips.
The pattern seems similar to the previous figures, but again pay close attention the horizontal access which covers a smaller range of values.
Now `r num_good_ci_n3` of the 100 proportions are between *0.4999 and 0.5001*.
So in 100 million flips we would expect[^simulation-34] the proportion of heads to be between 0.4999 and 0.5001, pretty close to 0.5.
This suggests that 0.0001 might be an appropriate margin of error for a simulation based on 100,000,000 flips.

[^simulation-34]: In 100 million flips, the probability of heads on between 49.99% and 50.01% of flips is 0.977, and `r num_good_ci_n3` out of 100 sets provides a rough estimate of this probability.

(ref:cap-coin-sim3) Proportion of flips which are heads in 100 sets of **100,000,000** fair coin flips. Each dot represents a set of **100,000,000** fair coin flips. In `r num_good_ci_n3` of these 100 sets the proportion of heads is between 0.4999 and 0.5001 (the blue dots).

```{r coin-sim3, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-coin-sim3)"}

print(plot_coin_phat_n3)

```

The previous figures illustrate that the more flips there are, the more likely it is that we observe a proportion of flips landing on heads close to 0.5.
We also see that with more flips we can refine our definition of "close to 0.5": increasing the number of flips by a factor of 100 (10,000 to 1,000,000 to 100,000,000) seems to give us an additional decimal place of precision ($0.5\pm0.01$ to $0.5\pm 0.001$ to $0.5\pm 0.0001$.)


### A closer look at margin of error

We will now carry out an analysis similar to the one above to investigate simulation margin of error and how it is influenced by the number of simulated values used to compute the relative frequency.  Continuing the dice example, suppose we want to estimate $p=\IP(X=6)$, the probability that the sum of two rolls of a fair four-sided equals six. Since there are 16 possible equally likely outcomes, 3 of which result in a sum of 3, the true probability is $p=3/16=0.1875$. 

We will perform a "meta-simulation". The process is as follows

1. Simulate two rolls of a fair four-sided die.  Compute the sum ($X$) and see if it is equal to 6.
1. Repeat step 1 to generate $N$ simulated values of the sum ($X$).  Compute the relative frequency of sixes: count the number of the $N$ simulated values equal to 6 and divide by $N$.  Denote this relative frequency $\hat{p}$ (read "p-hat").
1. Repeat step 2 a large number of times, recording the relative frequency $\hat{p}$ for each set of $N$ values.

Be sure to distinguish between steps 2 and 3.  A simulation will typically involve just steps 1 and 2, resulting in a single relative frequency based on $N$ simulated values.  Step 3 is the "meta" step; we see how this relative frequency varies from simulation to simulation to help us in determing an appropriate margin of error.  The important quantity in this analysis is $N$, the *number of independently simulated values used to compute the relative frequency* in a single simulation. We wish to see how $N$ impacts margin of error.  The number of simulations in step 3 just needs to be "large" enough to provide a clear picture of how the relative frequency varies from simulation to simulation.  The more the relative frequency varies from simulation to simulation, the larger the margin of error needs to be.

We can combine steps 1 and 2 of the meta-simulation to put it in the framework of the simulations from earlier in this chapter.  Namely, we can code the meta-simulation as a single simulation in which

- A sample space outcome represents $N$ values of the sum of two fair-four sided dice
- The main random variable of interest is the proportion of the $N$ values which are equal to 6.

Let's first consider $N=100$. The following Symbulate code defines the probability space corresponding to 100 values of the sum of two-fair four sided dice.  Notice the use of `apply` which functions much in the same way^[One difference between `RV` and `apply`: `apply` preserves the type of the input object.  That is, if `apply` is applied to a `ProbabilitySpace` then the output will be a `ProbabilitySpace`; if `apply` is applied to an `RV` then the output will be an `RV`.  In contrast, `RV` always creates an `RV`.] as `RV`.


```{python metasim-p}
N = 100
P = BoxModel([1, 2, 3, 4], size = 2).apply(sum) ** N
P.sim(5)

```

In the code above

- `BoxModel([1, 2, 3, 4], size = 2)` simulates two rolls of a fair four-sided die
- `.apply(sum)` computes the sum of the two rolls
- `** N` repeats the process `n` times to generate a set of `N` independent values, each value representing the sum of two rolls of a fair four-sided die
- `P.sim(5)` simulates 5 sets, each set consisting of `N` sums

Now we define the random variable which takes as an input a set of $N$ sums and returns the proportion of the $N$ sums which are equal to six.

```{python metasim-phat}

phat = RV(P, count_eq(6)) / N
phat.sim(5)

```

In the code above

- `phat` is an `RV` defined on the probability space `P`. Recall that an outcome of `P` is a set of `N` sums (and each sum is the sum of two rolls of a fair four-sided die).
- The function that defines the `RV` is `count.eq(6)`, which counts the number of values in the set that are equal to 6. We then^[Unfortunately, for techincal reasons, `RV(P, count_eq(6) / N)` will not work.  It is possible to divide by `N` within `RV` if we define a custom function `def rel_freq_six(x): return x.count_eq(6) / N`
and then define `RV(P, rel_freq_six)`.] divide by `N`, the total number of values in the set, to get the relative frequency.  (Remember that a transformation of a random variable is also a random variable.)
- `phat.sim(5)` generates 5 simulated values of the relative frequency `phat`.  Each simulated value of `phat` is the relative frequency of sixes in `N` sums of two rolls of a fair four-sided die.


Now we simulate and summarize a large number of values of `phat`.
We'll simulate 100 values for illustration (as we did in Figures \@ref(fig:coin-sim1), \@ref(fig:coin-sim2), and \@ref(fig:coin-sim3)).
Be sure not to confuse 100 with `N`.
Remember, the important quantity is `N`, the number of simulated values used in computing each relative frequency.

```{python, eval = TRUE, cache = TRUE}
plt.figure()
phat.sim(100).plot(type = "impulse", normalize = False)
plt.ylabel('Number of simulations');
plt.show()

```


We see that the 100 relative frequencies are roughly centered around the true probability 0.1875, but there is variability in the relative frequencies from simulation to simulation. From the range of values, we see that most relative frequencies are within about 0.08 or so from the true probability 0.1875. So a value around 0.08 seems like a reasonable value of the margin of error, but the actual value depends on what we mean by "most".  We can get a clearer picture if we run more simulations.  The following plot displays the results of 10000 simulations, each resulting in a value of $\hat{p}$.  Remember that each relative frequency is based on $N=100$ sums of two rolls.

```{python, eval = TRUE, cache = TRUE}
plt.figure()
phats = phat.sim(10000)
phats.plot(type = "impulse", normalize = False)
plt.ylabel('Number of simulations');
plt.show()
```

Let's see how many of these 10000 simulated proportions are within 0.08 of the true probability 0.1875.




```{python, eval = TRUE, cache = TRUE}
1 - (phats.count_lt(0.1875 - 0.08) + phats.count_gt(0.1875 + 0.08)) / 10000

```

In roughly 95% or so of simulations, the simulated relative frequency was within 0.08 of the true probability.  So 0.08 seems like a reasonable margin of error for "95% confidence" or "95% accuracy".  However, a margin of error of 0.08 yields pretty imprecise estimates, ranging from about 0.10 to 0.27.  Can we keep the degree of accuracy at 95% but get a smaller margin of error, and hence a more precise estimate?  Yes, if we increase the number of repetitions used to compute the relative frequency.

Now we repeat the analysis, but with $N=10000$.  In this case, each relative frequency is computed based on 10000 independent values, each value representing a sum of two rolls of a fair four-sided die. As before we start with 100 simulated relative frequencies.

```{python, eval = TRUE, cache = TRUE}

N = 10000
P = BoxModel([1, 2, 3, 4], size = 2).apply(sum) ** N
phat = RV(P, count_eq(6)) / N

phats = phat.sim(100)
phats.plot(type = "impulse", normalize = False)
plt.ylabel('Number of simulations');
plt.show()

```

Again we see that the 100 relative frequencies are roughly centered around the true probability 0.1875, but there is less variability in the relative frequencies from simulation to simulation for $N=10000$ than for $N=100$.
Pay close attention to the horizontal axis.
From the range of values, we see that most relative frequencies are now within about 0.008 of the true probability 0.1875.

```{python, eval = TRUE, cache = TRUE}
1 - (phats.count_lt(0.1875 - 0.008) + phats.count_gt(0.1875 + 0.008)) / 100

```

As with $N=100$, running more than 100 simulations would give a clearer picture of how much the relative frequency based on $N=10000$ simulated values varies from simulation to simulation. But even with just 100 simulations, we see that a margin of error of about 0.008 is required for roughly 95% accuracy when $N=10000$, as opposed to 0.08 when $N=100$. As we observed in the coin flipping example earlier in this section, it appears that increasing $N$ by a factor of 100 yields an extra decimal place of precision. That is, increasing $N$ by a factor of 100 decreases the margin of error by a factor of $\sqrt{100}$.  In general, the margin of error is inversely related to $\sqrt{N}$.


(ref:cap-moe-compare) Comparison of margins of error for 95% confidence for the meta-simulations in this section.

```{r, moe-compare, echo = FALSE}
ns = 100 ^ (1:3)

compare_df = data.frame(c("A fair coin flip lands H",
                          "Two rolls of a fair four-sided die sum to 6"),
                        c(0.5, 0.1875),
                    round(t(cbind(2 * 0.5 / sqrt(ns), 2 * sqrt(3 / 16 * (1 - 3 / 16)) / sqrt(ns))), 4))

knitr::kable(
  compare_df,
  digits = 5,
  booktabs = TRUE,
  col.names = c("Probability that", "True value",
                "95% m.o.e. (N = 100)", "95% m.o.e. (N = 10000)",
                "95% m.o.e. (N = 1000000)"),
  caption = "(ref:cap-moe-compare)"
)

```


The two examples in this section illustrate that the margin of error also depends somewhat on the true probability. The margin of error required for 95% accuracy is larger when the true probability is 0.5 than when it is 0.1875.  It can be shown that when estimating a probability $p$ with a relative frequency based on $N$ simulated repetitions, the margin of error required for 95% confidence^[We will see
    the rationale behind this formula later. The factor 2
    comes from the fact that for a Normal distribution, about 95% of values
    are within 2 standard deviations of the mean. Technically, the
    factor 2 corresponds to 95% confidence only when a single
    probability is estimated. If multiple probabilities are estimated
    simultaneously, then alternative methods should be used,
    e.g., increasing the factor 2 using a
    [Bonferroni
    correction](https://en.wikipedia.org/wiki/Bonferroni_correction). For example, a multiple of 4 rather than 2 produces very conservative error bounds for 95% confidence even when many probabilities are being estimated.] is
\[
2\frac{\sqrt{p(1-p)}}{\sqrt{N}}
\]
For a given $N$, the above quantity is maximized when $p$ is 0.5.  Since $p$ is usually unknown --- the reason for performing the simulation is to approximate it --- we plug in 0.5 for a somewhat conservative margin of error of $1/\sqrt{N}$.

For a fixed $N$, there is a tradeoff between accuracy and precision.
The factor 2 in the margin of error formula above corresponds to 95% accuracy.
Greater accuracy would require a larger factor, and a larger margin of error, resulting in a wider --- that is, less precise --- interval.
For example, 99% confidence requires a factor of roughly 2.6 instead of 2, resulting in an interval that is roughly 30 percent wider.
The confidence level does matter, but the primary influencer of margin of error is $N$, the number of repetitions on which the relative frequency is based.
Regardless of confidence level, the margin of error is on the order of magnitude of $1/\sqrt{N}$.

In summary, **the margin of error when approximating a probability based on a simulated relative frequency is roughly on the order \(1/\sqrt{N}\), where \(N\) is the number of independently simulated values used to calculate the relative frequency.** Warning: alternative methods are necessary when the actual probability being estimated is very close to 0 or to 1.


A probability is a theoretical long run relative frequency.
A probability can be approximated by a relative frequency from a large number of simulated repetitions, but there is some simulation margin of error.
Likewise, the average value of $X$  after a large number of simulated repetitions is only an approximation to the theoretical long run average value of $X$.
The margin of error is also on the order of $1/\sqrt{N}$ where $N$ is the number of simulated values used to compute the average.
We will explore margins of error for long run averages  in more detail later.

Pay attention to the wording: $N$ is the number of independently^[In all the situations in this book the values will be simulated independently.  However, there are many simulation methods where this is not true, most notably MCMC (Markov Chain Monte Carlo) methods. The margin of error needs to be adjusted to reflect any dependence between simulated values.] simulated values *used to calculate the relative frequency*.
This is not necessarily the number of simulated values.
For example, suppose we use simulation to approximate the probability that the larger of two rolls of a fair four-sided die is 4 *when the sum is equal to 6.*
We might start by simulating 10000 pairs of rolls.
But the sum would be equal to 6 in only about 1875 pairs, and it is only these pairs that would be used to compute the relative frequency that the larger roll is 4 to approximate the probability of interest.
The appropriate margin of error is roughly $1/\sqrt{1875} \approx 0.023$.
Compared to 0.01 (based on the original 10000 repetitions) the margin of error of 0.023 results in intervals that are 130 percent wider.
Carefully identifying the number of values *used to calculate the relative frequency* is especially important when determining appropriate simulation margins of error for approximating *conditional probabilities*, which we'll discuss in more detail later.



### Approximating multiple probabilities

When using simulation to estimate a single probability, the primary influencer of margin of error is $N$, the number of repetitions on which the relative frequency is based.
It doesn't matter as much whether we use, say, 95% versus 99% confidence.
That is, it doesn't matter too much whether we compute our margin of error using
\[
2\frac{\sqrt{p(1-p)}}{\sqrt{N}},
\]
with a multiple of 2 for 95% confidence, or if we replace 2 by 2.6 for 99% confidence.
(Remember, we can plug in 0.5 for the unknown $p$ for a conservative margin of error.)
A margin of error based on 95% or 99% (or another confidence level in the neighborhood) provides a reasonably accurate estimate of the probability. 
However, using simulation to *approximate multiple probabilities simultaneously* requires a little more care with the confidence level.


In the previous section we used simulation to estimate $\IP(X=6)$.
Now suppose we want to approximate $\IP(X = x)$ for each value of $x = 2,3,4,5,6,7,8$.
We could run a simulation like the one in Section \@ref(symbulate-distribution) to obtain results like those in Figure \@ref(fig:dice-sum-marginal-sim) and the table before it.
Each of the relative frequencies in the table is an approximation of the true probability, and so each of the relative frequencies should have a margin of error, say 0.01 for a simulation based on 10000 repetitions.
Thus, the simulation results yield a *collection* of seven interval estimates, an interval estimate of $\IP(X = x)$ for each value of $x = 2,3,4,5,6,7,8$.
Each interval in the collection either contains the respective true probability or not.
The question is then: In what percent of simulations will *every* interval in the collection contain the respective true probability?

Figure \@ref(fig:multiple-ci1) summarizes the results of 100 simulations.
Each simulation consists of 10000 repetitions, with results similar to those in Figure \@ref(fig:dice-sum-marginal-sim) and the table before it.
Each simulation is represented by a row in Figure \@ref(fig:multiple-ci1), consisting of seven 95% interval estimates, one for each value of $x$.
Each panel represents a different value of $x$; for each value of $x$, around 95 out of the 100 simulations yield estimates that contain the true probability $\IP(X=x)$, represented by the vertical line.


```{r echo=FALSE, warning=FALSE, message=FALSE}
x = 2:8
p_vec = c(1, 2, 3, 4, 3, 2, 1) / 16

n = 10000
N = 100

zmax = 5
z = 2

sim = data.frame(t(rmultinom(N, n, p_vec) / n))
names(sim) = paste("x=", x, sep = "")

good_ci = NULL

for (i in 1:length(x)) {
  good_ci_i = (abs(sim[, i] - p_vec[i]) < z * sqrt(p_vec[i] * (1 - p_vec[i]) / n))
  good_ci <- good_ci %>% bind_cols(good_ci_i)
}

n_good_ci = apply(good_ci, 1, sum)
n_good_sets = sum(n_good_ci == length(x))

sim <- sim %>%
  mutate(n_good_ci = n_good_ci) %>%
  arrange(n_good_ci) %>%
  mutate(repetition = row_number())

phat_lims = c(max(0, min(p_vec - zmax * sqrt(p_vec * (1 - p_vec) / n))),
              min(1, max(p_vec + zmax * sqrt(p_vec * (1 - p_vec) / n))))

plot_i = list()

for (i in 1:length(x)) {
  
  p = p_vec[i]
  
  sim_i = sim %>%
    select(i, repetition) %>%
    rename(phat = 1) %>%
    mutate(ci_lb = pmax(0, phat - z * sqrt(p * (1 - p) / n)),
           ci_ub = pmin(1, phat + z * sqrt(p * (1 - p) / n)),
           good_ci = (abs(phat - p) <= z * sqrt(p * (1 - p) / n)))
  
  p <- ggplot(sim_i, aes(x = phat, y = repetition, color = good_ci)) +
    geom_point() +
    geom_segment(aes(x = ci_lb, xend = ci_ub,
                     y = repetition, yend = repetition,
                     color = good_ci)) +
    # scale_x_continuous(limits = phat_lims) + 
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
        geom_vline(xintercept = p, color = "black", size = 1.0) +
    labs(title = paste("x =", x[i]),
         x = "",
         y = "Simulation") +
    theme_classic() +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
        # geom_rect(aes(xmin = 0,xmax=1,ymin=0,ymax=n_good_sets),fill="black",alpha=0.2)
  
  if (i > 1) {
    p <- p +
      theme(axis.title.y = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank(),
            axis.line.y = element_blank())
  }
  
  plot_i[[i]] = p
  
}
```



(ref:cap-multiple-ci1) Results of 100 simulations. Each simulation yields a collection of seven 95% confidence intervals.


```{r multiple-ci1, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-multiple-ci1)"}

do.call(grid.arrange, c(plot_i, nrow = 1))

```


However, let's zoom in on the bottom of Figure \@ref(fig:multiple-ci1).
Figure \@ref(fig:multiple-ci2) displays the results of the `r N - n_good_sets` simulations at the bottom of Figure \@ref(fig:multiple-ci1).
Look carefully row by row in Figure \@ref(fig:multiple-ci2); in each of these simulations at least one of the seven intervals in the collection does not contain the true probability.
In other words, *every* interval in the collection contains the respective true probability in only `r n_good_sets` of the 100 simulations (the other simulations in Figure \@ref(fig:multiple-ci1).)
While we have 95 percent confidence in our interval estimate of $\IP(X = x)$ for any single $x$, we only have around `r n_good_sets` percent confidence in our approximate *distribution* of $X$.
Our confidence "grade" has gone from A range (95 percent) to C range (`r n_good_sets` percent).

(ref:cap-multiple-ci2) Subset of `r N - n_good_sets` simulations from Figure \@ref(fig:multiple-ci1). In each of these simulations, at least one 95% confidence interval does not contain the respective true probability.


```{r multiple-ci2, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-multiple-ci2)"}

plot_i2 = list()

for (i in 1:length(x)) {
  p = plot_i[[i]]
  p <- p +
    ylim(0, N - n_good_sets)
  plot_i2[[i]] = p

}

do.call(grid.arrange, c(plot_i2, nrow = 1))


```


When approximating multiple probabilities based on a single simulation, such as when approximating a distribution, margins of error and interval estimates need to be adjusted to obtain *simultaneous* 95% confidence.
The easiest way to do this is to make all of the intervals in the collection wider.

There are many procedures for adjusting a collection of interval estimates to achieve simultaneous confidence; we won't get into any technical details.
As a very rough, but simple and typically conservative rule, when approximating many probabilities based on a single simulation, we recommend making the margin of error twice as large as when approximating a single probability.
That is, **use a margin of error of
$2/\sqrt{N}$ (rather than $1/\sqrt{N}$) to achieve simultaneous 95% confidence when approximating many probabilities based on a single simulation.**



### Beware a false sense of precision

Why don't we always run something like one trillion repetitions so that our margin of error is tiny?
There is a cost to simulating and storing more repetitions in terms of computational time and memory.
Also, remember that simulating one trillion repetitions doesn't guarantee that the margin of error is actually based on anywhere close to one trillion repetitions, especially when conditioning on a low probability event.

Most importantly, keep in mind that any probability model is based on a series of assumptions and these assumptions are not satisfied exactly by the random phenomenon.
A precise estimate of a probability *under the assumptions of the model* is not necessarily a comparably precise estimate of the true probability.
Reporting probability estimates out to many decimal places conveys a false sense of precision and should typically be avoided.



For example, the probability that any particular coin lands on heads is probably not 0.5 *exactly*.
But any difference between the true probability of the coin landing on heads and 0.5 is likely not large enough to be practically meaningful^[There is actually [some evidence](https://www.stat.berkeley.edu/~aldous/Real-World/coin_tosses.html) that a coin flip is slightly more likely to land the same way it started; e.g, a coin that starts H up is more likely to land H up.  But the tendency is small.].
That is, assuming the coin is fair is a reasonable model.

Suppose we assume that the probability that a coin lands on heads is exactly 0.5 and that the results of different flips are independent.
If we flip the coin 1000 times the probability that it lands on heads at most 490 times is 0.2739864.
(We will see a formula for computing this value later.)
If we were to simulate one trillion repetitions (each consisting of 1000 flips) to estimate this probability then our margin of error would be 0.000001; we could expect accuracy out to the sixth decimal place. 
However, reporting the probability with so many decimal places is somewhat disingenuous.
If the probability that the coin lands on heads were 0.50001, then the probability of at most 490 heads in 1000 flips would be 0.2737757.
If the probability that the coin lands on heads were 0.5001, then the probability of at most 490 heads in 1000 flips would be 0.2718834.
Reporting our approximate probability as something like 0.2739864 $\pm$ 0.000001 says more about the precision in our *assumption* that the coin is fair than it does about the true probability that the coin lands on heads at most 490 times in 1000 flips.
A more honest conclusion would result from running 10000 repetitions and reporting our approximate probability as something like 0.27 $\pm$ 0.01.
Such a conclusion reflects more genuinely that there's some "wiggle room" in our assumptions, and that any probability computed according to our model is at best a reasonable approximation of the "true" probability. 




For most of the situations we'll encounter in this book, estimating a probability to within 0.01 of its true value will be sufficient for practical purposes, and so basing approximations on 10000 simulated values will be appropriate.  Of course, there are real situations where probabilities need to be estimated much more precisely, e.g., the probability that a bridge will collapse.  Such situations require more intensive methods.



## Conditioning {#cond}

<!-- \newcommand{\IP}{\textrm{P}} -->
<!-- \newcommand{\IQ}{\textrm{Q}} -->
<!-- \newcommand{\E}{\textrm{E}} -->
<!-- \newcommand{\Var}{\textrm{Var}} -->
<!-- \newcommand{\SD}{\textrm{SD}} -->
<!-- \newcommand{\Cov}{\textrm{Cov}} -->
<!-- \newcommand{\Corr}{\textrm{Corr}} -->
<!-- \newcommand{\Xbar}{\bar{X}} -->
<!-- \newcommand{\Ybar}{\bar{X}} -->
<!-- \newcommand{\xbar}{\bar{x}} -->
<!-- \newcommand{\ybar}{\bar{y}} -->
<!-- \newcommand{\ind}{\textrm{I}} -->
<!-- \newcommand{\dd}{\text{DDWDDD}} -->
<!-- \newcommand{\ep}{\epsilon} -->
<!-- \newcommand{\reals}{\mathbb{R}} -->



Conditioning concerns how probabilities of events or distributions of random variables are influenced by information about the occurrence of events or the values of  random variables.
(Later, we will also see that conditioning provides a useful strategy for breaking problems down into smaller parts.)


A probability is a measure of the likelihood or degree of uncertainty of an event.
A conditional probability revises this measure to reflect any "new" information about the outcome of the underlying random phenomenon.






```{example impeach}

The probability^[These number are estimates based on [data from polls as of Oct 9, 2019](https://fivethirtyeight.com/features/two-weeks-in-impeachment-is-becoming-more-popular/). I wrote this exercise in Fall 2019. In Fall 2020, I decided not to change it, knowing that would make it outdated. But then Trump was impeached again in January 2021. And now we have the Jan 6 committee hearings.] that a randomly
selected American adult supports impeachment of President Trump is 0.49.

```

1. Suppose the randomly selected person is a Democrat.  Do you think the probability that the randomly selected *Democrat* supports impeachment is 0.49?
1. The probability^[[Estimate as of Sept 2019.](https://news.gallup.com/poll/15370/party-affiliation.aspx)] that a randomly selected American is a Democrat is 0.31. Donny Don't^[Remember, this is a Simpsons reference, not a Trump reference.] says that the probability that a randomly selected American both (1) is a Democrat, *and* (2) supports impeachment is equal to $0.49\times 0.31$. Do you agree?
1. Without further information, provide a range of "logically possible" values for the probability in the previous part.  ("Logically possible" means they satisfy the rules of probability, even though they might not be realistic in context.)
1. Suppose that the probability that a randomly selected American both is a Democrat and supports impeachment is 0.26. Construct an appropriate two-way table of probabilities.
1. Construct a corresponding two-way table of hypothetical counts.
1. Find the probability^[The resulting value is estimated based on [data from polls as of Oct 9, 2019](https://fivethirtyeight.com/features/two-weeks-in-impeachment-is-becoming-more-popular/) and [party affiliation as of Sept 2019](https://news.gallup.com/poll/15370/party-affiliation.aspx).] that a randomly selected American *who is a Democrat* supports impeachment.
1. How can the probability in the previous part be written in terms of the probabilities provided in the setup? 
1. Find the probability that a randomly selected American *who supports impeachment* is a Democrat.



```{solution, impeach-sol}
to Example \@ref(exm:impeach)
```

```{asis, fold.chunk = TRUE}

1. The probability that the randomly selected *Democrat* supports impeachment is probably a lot larger than 0.49.  Knowing that the selected person is a Democrat would change the probability of supporting impeachment.
1. No. Consider a hypothetical set of 100 Americans.  We would expect about 31 of these 100 Americans to be Democrats.  However, we would not expect just 15 --- that is, about half ($(0.49)(31)\approx 15$) --- of the 31 Democrats to support impeachment; we'd expect more, say 26 out of the 31.  If 26 of the 100 Americans are Democrats who support impeachment, this would be consistent with a value of 0.26 for the probability that a randomly selected American both (1) is a Democrat, *and* (2) supports impeachment equal.
1. We could make a table like in the following part and see what values produce valid tables.  If $A$ is the event that the selected person supports impeachment, and $D$ is the event that the person is a Democrat, and $\IP$ corresponds to randomly selecting an American, then $\IP(A) = 0.49$ and $\IP(D) = 0.31$.  By the subset rule $\IP(A\cap B)\le \min(\IP(A), \IP(D)) = 0.31$.  The largest $\IP(A\cap D)$ can be is 0.31, which corresponds to all Democrats supporting impeachment.  In this case, the smallest $\IP(A \cap D)$ can be is 0, which corresponds to no Democrats supporting impeachment.  The extremes are not realistic, but without knowing more information, we do not know where $\IP(A\cap D)$ lies in $0\le \IP(A \cap D) \le 0.31$.  
1. If the probability that a randomly selected American both is a Democrat and supports impeachment is 0.26, then the two-way table of probabilities is

    |       |  $A$ | $A^c$ | Total |
    |-------|-----:|------:|------:|
    | $D$   | 0.26 |  0.05 |  0.31 |
    | $D^c$ | 0.23 |  0.46 |  0.69 |
    | Total | 0.49 |  0.51 |  1.00 |

1. It is often much easier to work with counts rather than probabilities.  Start with a nice round total^[For the purposes of constructing a hypothetical table, it doesn't matter what value you use for the total, as long as you don't round any of the counts in the interior cells. If interior cells are decimals, either leave them as decimals, or add a few zeros to the total count and redo.] count like 10000 and then construct a table of hypothetical counts, assuming the counts follow the probabilities in the table above.

    |              | Impeach | Not Impeach | Total |
    |--------------|--------:|------------:|------:|
    | Democrat     |    2600 |         500 |  3100 |
    | Not Democrat |    2300 |        4600 |  6900 |
    | Total        |    4900 |        5100 | 10000 |

1. Working with counts, there are 3100 Democrats, of which 2600 support impeachment, so $2600/3100=0.839$ is the probability that a randomly selected American *who is a Democrat* supports impeachment.
1. $\frac{\IP(A\cap D)}{\IP(D)} = \frac{0.26}{0.31}=0.839$ 
1. There are 4900 Americans who support impeachment, of which 2600 are Democrats, so $\frac{2600}{4900} = \frac{0.26}{0.49}=\frac{\IP(A\cap D)}{\IP(A)} =0.531$ is the probability that a randomly selected American *who supports impeachment* is a Democrat.  Notice that this part and the previous part have the same numerator, $\IP(A\cap D)$, but different *denominators*.  Also notice that the probabilities are quite different in this part and the previous part.


```





The **conditional probability of event $A$ given event $B$**, denoted $\IP(A|B)$, is defined as ^[Provided $\IP(B)>0$.  We will assume throughout that all events being conditioned on have non-zero probability.  We will discuss some issues related to conditioning on the value of a continuous random variable later.] 
\[
\IP(A|B) = \frac{\IP(A\cap B)}{\IP(B)}
\]


The conditional probability $\IP(A|B)$ represents how the likelihood or degree of uncertainty of event $A$ should be updated to reflect information that event $B$ has occurred.
The *unconditional* probability $\IP(A)$ is often called the *prior probability* (a.k.a., base rate) of $A$ (prior to observing $B$). 
The *conditional* probability $\IP(A|B)$ is the *posterior probability* of $A$ after observing $B$.


In general, knowing whether or not event $B$ occurs influences the probability of event $A$.  That is, 
\[
\text{In general, } \IP(A|B) \neq \IP(A)
\]
For example, without knowing a person's political party, the probability of supporting impeachment is 0.49, but after learning the person is a Democrat, the probability of supporting impeachment changed to 0.839.

Be careful: order is essential in conditioning.  That is,
\[
\text{In general, } \IP(A|B) \neq \IP(B|A)
\]





```{example nba-conditional2}

Which of the following is larger - 1 or 2?
  
1. The probability that a randomly selected man who is greater than six feet tall plays in the NBA.
1. The probability that a randomly selected man who plays in the NBA is greater than six feet tall.

```

```{solution, nba-conditional2-sol}
to Example \@ref(exm:nba-conditional2)
```

```{asis, fold.chunk = TRUE}

The probability in (2) is much larger.  The corresponding fractions would have the same numerator --- number of men who are both greater than six feet tall and play in the NBA --- but vastly different denominators. 


1. There are over a billion men in the world who are greater than six feet tall, only a few hundred of whom play in the NBA.  The probability that a randomly selected man who is greater than six feet tall plays in the NBA is pretty close to 0.
1. There only a few hundred men who play in the NBA, almost all of whom are greater than six feet tall.  The probability that a randomly selected man who plays in the NBA is greater than six feet tall is pretty close to 1.

```

When dealing with probabilities, especially conditional probabilities, be sure to ask "probability *of what*?" That is, what is the appropriate *sample space*? Thinking in fraction terms, be sure to identify the total/baseline group which corresponds to the *denominator*.  Be very careful when translating between numbers and words.


To emphasize, $\IP(A|B)$ is not the same as $\IP(B|A)$ and they can be vastly different. In particular, the conditional probabilities can be highly influenced by the original unconditional probabilities of the events, $\IP(A)$ and $\IP(B)$, sometimes called the **base rates**.  Don't neglect the base rates when evaluating probabilities.

For example, the probability that a randomly selected man plays in the NBA is pretty close to 0 (the base rate).  Learning that the man is greater than six feet tall is not going to change much our probability that he plays in the NBA.


### Simulating conditional probabilities

```{example impeach-sim}

Consider simulating a randomly selected American and determining whether or not the person supports impeachment and whether or not the person is a Democrat, as in the scenario in  Example \@ref(exm:impeach).  Remember we are given $\IP(A) = 0.49$, $\IP(D) = 0.31$, and $\IP(A\cap D) = 0.26$ where $A$ is the event that the selected person supports impeachment and $D$ is the event that the selected person is a Democrat.

```

1. Donny Don't says we need two spinners: One spinner with areas of 0.49 and 0.51 to represent Support/Not support, and another spinner with areas of 0.31 and 0.69 to represent Democrat/Not Democrat.  Then spin each spinner once to simulate one repetition.  Do you agree?
1. How could you perform one repetition of the simulation using just a single spinner? (Hint: it needs 4 sectors.)
1. How could you perform a simulation, using the spinner in the previous part, to estimate $\IP(A | D)$?
1. What determines the order of magnitude of the the margin of error for your estimate in the previous part?
1. What is another method for performing the simulation and estimating $\IP(A |D)$ that has a smaller margin of error?  What is the disadvantage of this method?



```{solution, impeach-sim-sol}
to Example \@ref(exm:impeach-sim)
```


```{asis, fold.chunk = TRUE}

1. No, this assumes there is no relationship between party and support.  But we know that Democrats will be much more likely to support impeachment than non-Democrats.  In general, you can not simulate pairs of events simply from the marginal probabilities of each.
1. You need to construct a spinner for the possible occurrences of the *pairs* of events --- both occur, $A$ occurs and $D$ does not, $D$ occurs and $A$ does not, neither occurs --- and their *joint* probabilities.  From the three given probabilities we can determine:
   - Democrat and support: $\IP(A\cap D)= 0.26$
   - Democrat and not support: $\IP(A\cap D^c)= 0.23$
   - not Democrat, and support$\IP(A^c\cap D)= 0.05$
   - not Democrat, and not support $\IP(A^c\cap D^c)= 0.46$
   (see the interior cells in the two-way table from Example \@ref(exm:impeach)). See Figure \@ref(fig:impeach-sim-spinner).
1. The following method fixes the number of total spins, say 10000.
    - Spin the joint spinner from the previous part once to simulate a (party, support) pair.
    - Repeat a fixed number of times, say 10000.
    - Discard the repetitions on which the person was not a Democrat, that is, the repetitions on which $B$ did not occur.  You would expect to have around 3100 repetitions left.
    - Among the remaining repetitions (on which $D$ occurred), count the number of repetitions on which $A$ also occurred.  So for the roughly 3100 repetitions for which the person was a Democrat, count the repetitions on which the person also supported impeachment; you would expect a count of around 2600.
    - Estimate $\IP(A|D)$ by dividing the two previous counts to obtain a conditional relative frequency.
    \[
    \IP(A | D)\approx \frac{\text{Number of repetitions on which both $A$ and $D$ occurred}}{\text{Number of repetitions on which $D$ occurred}}
    \]
1. Only those repetitions in which $D$ occurred are used to estimate $\IP(A|D)$.  So the order of magnitude of the margin of error is determined by the number of repetitions on which $D$ occurs.  Roughly this would be around 3100, rather than 10000.
1. The previous method simulated a fixed number of repetitions first, and then discarded the ones that did not meet the condition.  We could instead discard repetitions that do not meet the condition as we go, and keep performing repetitions until we get a fixed number, say 10000, that do satisfy the condition.  In this way, the estimate $\IP(A |D)$ will be based on the fixed number of repetitions, say 10000, that satisfy event $D$.  The disadvantage is increased computational burden; we will need to simulate and discard many repetitions in order to achieve that the desired number that satisfy the condition.

```


(ref:impeach-spinner) Spinner corresponding to Example \@ref(exm:impeach-sim).

```{r impeach-sim-spinner, echo=FALSE, fig.cap="(ref:impeach-spinner)", fig.width=12, fig.align='center'}

knitr::include_graphics(c("_graphics/spinner-impeach.png"))  

```  

There are two basic ways to use simulation to approximate a conditional probability $\IP(A|B)$.

- Simulate the random phenomenon for a set number of repetitions (say 10000), *discard those repetitions on which $B$ does not occur*, and compute the relative frequency of $A$ among the remaining repetitions (on which $B$ does occur).  
  - Disadvantage: the margin of error is based on only the number of repetitions used to compute the relative frequency.  So if you perform 10000 repetitions but $B$ occurs only on 2000, then the margin of error for estimate $\IP(A|B)$ is roughly on the order of $1/\sqrt{2000} = 0.022$ (rather than $1/\sqrt{10000} = 0.01$.  Especially if $\IP(B)$ is small, the margin of error could be large resulting in an imprecise estimate of $\IP(A|B)$. 
  - Advantage: not computationally intensive.
- Simulate the random phenomenon *until obtaining a certain number of repetitions (say 10000) on which $B$ occurs*, discarding those repetitions on which $B$ does not occur as you go, and compute the relative frequency of $A$ among the remaining repetitions (on which $B$ does occur).  
  - Advantage: the margin of error will be based on the set number of repetitions on which $B$ occurs.
  - Disadvantage: requires more time/computer power. Especially if $\IP(B)$ is small, it will require a large number of repetitions of the simulation to achieve the desired number of repetitions on which $B$ occurs.
    
In Symulate, `filter` can be used to extract repetitions that satisfy a condition.  First we'll simulate impeachment support status and party affiliation for 10000 hypothetical Americans.
Each ticket in the `BoxModel` has a (Support, Party) pair of labels, like the spinner in Figure \@ref(fig:impeach-sim-spinner).


```{python}
P = BoxModel([('Support', 'Democrat'), ('Support', 'Not Democrat'), ('Not Support', 'Democrat'), ('Not Support', 'Not Democrat')],
             probs = [0.26, 0.23, 0.05, 0.46])

sim_all = P.sim(10000)

sim_all
```

```{python}
sim_all.tabulate()
```

Now we'll apply `filter` to retain only the Democrats.
The function `is_Democrat` takes as an input^[`Support_Party` is a pair so `Support_Party[0]` is the first component (Support) and `Support_Party[1]` is the second component (Party).] a (support status, party affiliation pair) and returns `True` if Democrat (and `False` otherwise).
Applying `filter(is_Democrat)` will only return results for which `is_Democrat` returns `True`.

```{python}
def is_Democrat(Support_Party):
    return Support_Party[1] == 'Democrat'
  
sim_Dem = sim_all.filter(is_Democrat)

sim_Dem
```


```{python}
sim_Dem.tabulate()

```

Conditional relative frequencies are computed based only on repetitions which satisfy the event being conditioned on.

```{python}
sim_Dem.tabulate(normalize = True)

```


In Symbulate, the given symbol `|` applies the second method to simulate a fixed number of repetitions that satisfy the event being conditioned on.  Be careful when using `|` when conditioning on an event with small probability.  In particular, be careful when conditioning on the value of a continuous random variable.

Below we use `RV` syntax to carry out the simulation and conditioning.
Technically, a random variable always returns a number, but `RV` in Symbulate does allow for non-numerical outputs.
In most situations, we will usually deal with true random variables, and the code syntax below will be more natural.

The following simulates (Support, Party) pairs until 10000 Democrats are obtained.

```{python}

Support, Party = RV(P)

sim_Dem = ( (Support & Party) | (Party == 'Democrat') ).sim(10000)

sim_Dem
```


```{python}
sim_Dem.tabulate()

```

Since all 10000 simulated pairs satisfy the event being conditioned on (Democrat), they are all included in the computation of the conditional relative frequencies.

```{python}
sim_Dem.tabulate(normalize = True)
```



    
### Joint, conditional, and marginal probabilities

Within the context of two events, we have joint, conditional, and marginal probabilities.

- Joint: unconditional probability involving both events, $\IP(A \cap B)$.
- Conditional: conditional probability of one event given the other, $\IP(A | B)$, $\IP(B | A)$.
- Marginal: unconditional probability of a single event $\IP(A)$, $\IP(B)$.

The relationship $\IP(A|B) = \IP(A\cap B)/\IP(B)$ can  be stated generically as
\[
\text{conditional} = \frac{\text{joint}}{\text{marginal}}
\]
We will see later that similar relationships are true for distributions of random variables.


In the previous impeachment problem, we were provided the joint and marginal probabilities and we computed conditional probabilities.  But in many problems conditional probabilities are provided or can be determined directly.

```{example impeach2}

Recent polls^[[As of Oct 9, 2019](https://fivethirtyeight.com/features/two-weeks-in-impeachment-is-becoming-more-popular/)] suggest that

- 83% of Democrats support impeachment of President Trump
- 44% of Independents support impeachment of President Trump
- 14% of Republicans support impeachment of President Trump

```

1. The average of these three percentages is $(83+44+14)/3 = 47$.  Is it necessarily true that 47% of all Americans support impeachment?
1. Based on recent polls^[[Party affiliation as of Sept 2019.](https://news.gallup.com/poll/15370/party-affiliation.aspx)]

    - 31% of Americans are Democrats
    - 40% of Americans are Independent
    - 29% of Americans are Republicans
    
    Define the event $A$ to represent "supports impeachment" and $D, I, R$ to correspond to affiliation in each of the parties.  If the probability measure $\IP$ corresponds to randomly selecting an American, write all the percentages above as probabilities using proper notation.  
    
1. Find the probability that a randomly selected American is a Democrat who supports impeachment.  Is this a joint, conditional, or marginal probability?
1. Construct an appropriate two-way table.
1. Find the probability that a randomly selected American supports impeachment.  How does this differ from the average of the three percentages in part 1?  Why?
1. Now suppose that the randomly selected American supports impeachment.  How does this information change the probability that the selected American belongs to a particular political party?  Answer by computing appropriate probabilities (and using proper notation).
1. How does each of the probabilities from the previous part compare to the respective prior probability?  Does this make sense?



```{solution, impeach2-sol}
to Example \@ref(exm:impeach2)
```

```{asis, fold.chunk = TRUE}

1. No, think of extreme cases as illustrations.  If almost all of Americans were Democrats, then the overall probability of supporting impeachment would be close to 0.83, while if almost all of Americans were Republicans, then the overall probability of supporting impeachment would be close to 0.14.  So the overall probability of supporting impeachment depends on the party affiliation breakdown.
1. If the probability measure $\IP$ corresponds to randomly selecting an American then
    - $\IP(A|D) = 0.83$
    - $\IP(A|I) = 0.44$
    - $\IP(A|R) = 0.14$
    - $\IP(D) = 0.31$
    - $\IP(I) = 0.40$
    - $\IP(R) = 0.29$
1. The probability that a randomly selected American is a Democrat who supports impeachment is $\IP(A \cap D) = \IP(A|D)\IP(D) = (0.83)(0.31) = 0.2573$, a joint probability.  In 10000 hypothetical Americans, we would expect 3100 to be Democrats, and of those 3100 Democrats we would expect 2573 (or 83%) to support impeachment.  So out of the 10000 Americans, 2573 are Democrats who support impeachment.
1. Continue in the manner of the previous part to complete a two-way table of counts for 10000 hypothetical Americans.

    |             	| Impeach 	| Not Impeach 	| Total 	|  
    |-------------	|--------:	|------------:	|------:	|  
    | Democrat    	|    2573 	|         527 	|  3100 	|  
    | Independent 	|    1760 	|        2240 	|  4000 	|  
    | Republican  	|     406 	|        2494 	|  2900 	|  
    | Total       	|    4739 	|        5261 	| 10000 	|  
  
1. Out of the 10000 Americans, 4739 support impeachment, so the probability that a randomly selected American supports impeachment^[This number differs from the one in the previous impeachment problem because of rounding errors in the probabilities reported in the setups.] is $\IP(A)=0.4739$.  This is actually pretty close to the average of the 3 impeachment percentages, but that's just a coincidence.  The overall probability is actually a *weighted average*; in terms of the probabilities given in the setup, the table calculations show
    \begin{align*}
    \IP(A) & = \IP(A \cap D) + \IP(A \cap I) + \IP(A \cap R)\\
    & = \IP(A|D)\IP(D) + \IP(A|I)\IP(I) + \IP(A|R)\IP(R)\\
    & = (0.83)(0.31) + (0.44)(0.40) + (0.14)(0.29)
    \end{align*}
    This is an illustration of the "law of total probability", which we will discuss in more detail soon.
1. We want $\IP(D|A)$, etc.
    - $\IP(D|A) = \frac{2573}{4739} = \frac{(0.83)(0.31)}{(0.83)(0.31) + (0.44)(0.40) + (0.14)(0.29)} = 0.543$.
    - $\IP(I|A) = \frac{1760}{4739} = \frac{(0.44)(0.40)}{(0.83)(0.31) + (0.44)(0.40) + (0.14)(0.29)} = 0.371$.
    - $\IP(R|A) = \frac{406}{4739} = \frac{(0.14)(0.29)}{(0.83)(0.31) + (0.44)(0.40) + (0.14)(0.29)} = 0.086$.
    This is an illustration of "Bayes rule", which we will discuss in more detail soon.
1. How does each of the probabilities from the previous part compare to the respective prior probability?  Does this make sense?
    - $\IP(D|A) = 0.543$, which is greater than the prior probability of Democrat $\IP(D) = 0.31$.  Knowing the person supports impeachment increases the probability that the person is a Democrat.
    - $\IP(I|A) = 0.371$, which is slightly less than the prior probability of Independent $\IP(I) = 0.40$.  Knowing the person supports impeachment slightly decreases the probability that the person is an Independent.
    - $\IP(R|A) = 0.086$, which is less than the prior probability of Republican $\IP(R) = 0.29$.  Knowing the person supports impeachment decreases the probability that the person is a Republican.

```

A **mosaic plot** provides a nice visual of joint, marginal, and one-way conditional probabilities, and can be used to illustrate the law of total probability. The mosaic plot^[Unfortunately, mosaic plots are not available in Symbulate yet.] on the left in Figure \@ref(fig:impeach-mosaic) represents conditioning on political party.  The vertical bars represent the conditional probabilities of supporting/not supporting impeachment for each political party.  The widths of the vertical bars are scaled in proportion to the marginal distribution of party; the bar for Independent is a little wider than the others.  The area of each sub-rectangle represents a joint probability.  The single bar to the right of the plot displays the marginal probability of supporting/not supporting impeachment.

The plot on the right in Figure \@ref(fig:impeach-mosaic) represents conditioning on support of impeachment.  Now the widths of the vertical bars represent the distribution of supporting/not supporting impeachment, the heights within the bars represent conditional probabilities for party affiliation given support status, and the single bar to the right represents the marginal distribution of party affiliation.

(ref:cap-impeach-mosaic) Mosaic plots for Example \@ref(exm:impeach2).  The plot on the left represents conditioning on party affiliation, while the plot on the right represents conditioning on support for impeachment.

```{r impeach-mosaic, echo=FALSE, fig.cap="(ref:cap-impeach-mosaic)", out.width='50%', fig.show='hold'}

knitr::include_graphics(c("_graphics/impeach-mosaic.png", "_graphics/impeach-mosaic2.png"))

```



```{example impeach2-sim}

Consider simulating a randomly selected American and determining whether or not the person supports impeachment and whether or not the person is a Democrat, as in the scenario in  Example \@ref(exm:impeach2).  Remember we are given $\IP(A|D) = 0.83$, $\IP(A|I) = 0.44$, $\IP(A|R) = 0.14$, $\IP(D) = 0.31$, $\IP(I) = 0.40$, and $\IP(R)=0.29$.

```

How could you perform one repetition of the simulation using spinners based solely on the probabilities provided in the problem, without first constructing a two-way table or finding $\IP(A\cap B)$, etc?  (Hint: you'll need a few spinners, but you might not spin them all in a single repetition.)

```{solution, impeach2-sim-sol}
to Example \@ref(exm:impeach2-sim)
```


```{asis, fold.chunk = TRUE}

There will be 4 spinners, but only 2 will be spun in any single repetition.

- "Party" spinner: Areas of 0.31, 0.40, and 0.29 correspond to, respectively, Democrat, Independent, Republican.  Spin this to determine party affiliation.
- "Impeachment" spinners --- only one of the following will be spun in a single repetition:
    - Impeachment given Democrat: areas of 0.83 and 0.17 corresponding to, respectively, support, not support.  If the result of the "party" spinner is Democrat, spin this spinner to determine support for impeachment.
    -  Impeachment given Independent: areas of 0.44 and 0.56 corresponding to, respectively, support, not support.  If the result of the "party" spinner is Independent, spin this spinner to determine support for impeachment.
    -  Impeachment given Republican: areas of 0.14 and 0.86 corresponding to, respectively, support, not support.  If the result of the "party" spinner is Republican, spin this spinner to determine support for impeachment.

```

We can code the above in Symbulate by defining a custom probability space.  An outcome is a (party, impeachment) pair.  Each of the 4 spinners corresponds to a `BoxModel`.  We define a function that defines how to simulate one repetition, using the `draw` method.  Then we use that function to define a custom `ProbabilitySpace`.

```{python}

def party_impeachment_sim():
    party = BoxModel(['D', 'I', 'R'], probs = [0.31, 0.40, 0.29]).draw()
    if party == 'D':
        support = BoxModel(['Imp', 'NotImp'], probs = [0.83, 0.17]).draw()
    if party == 'I':
        support = BoxModel(['Imp', 'NotImp'], probs = [0.44, 0.56]).draw()
    if party == 'R':
        support = BoxModel(['Imp', 'NotImp'], probs = [0.14, 0.86]).draw()
    return party, support
    
P = ProbabilitySpace(party_impeachment_sim)
P.sim(10000).tabulate()

```



### Multiplication rule


Rearranging the definition of conditional probability we get the
**Multiplication rule:** the probability that two events both occur is

$$
\begin{aligned}
\IP(A \cap B) & = \IP(A|B)\IP(B)\\
& = \IP(B|A)\IP(A)
\end{aligned}
$$

The multiplication rule says that you should think "multiply" when you see "and".  However, be careful about *what* you are multiplying: to find a joint probability you need an unconditional and an appropriate conditional probability.  You can condition either on $A$ or on $B$, provided you have the appropriate marginal probability; often, conditioning one way is easier than the other. Be careful: the multiplication rule does *not* say that $\IP(A\cap B)$ is the same as $\IP(A)\IP(B)$.  



For example:

- 31% *of Americans* are Democrats
- 83.9% *of Democrats* support impeachment
- So 26% *of Americans* are Democrats who support impeachment, $0.26 = 0.31\times 0.839$.

\[
\frac{\text{Democrats who support impeachment}}{\text{Americans}} = \left(\frac{\text{Democrats}}{\text{Americans}}\right)\left(\frac{\text{Democrats who support impeachment}}{\text{Democrats}}\right)
\]

Generically, the multiplication rule says
\[
\text{joint} = \text{conditional}\times\text{marginal}
\]
We will see later that similar relationships are true for distributions of random variables.


### Conditioning is "slicing and renormalizing"


The process of conditioning can be thought of as **"slicing and renormalizing".**

- Extract the "slice" corresponding to the event being conditioned on (and discard the rest).  For example, a slice might correspond to a particular row or column of a two-way table.  
- "Renormalize" the values in the slice so that corresponding probabilities add up to 1.

We will see that the "slicing and renormalizing" interpretation also applies when dealing with *conditional distributions* of random variables, and corresponding plots.  Slicing determines the *shape*; renormalizing determines the *scale*.  Slicing determines relative probabilities; renormalizing just makes sure they add up to 1.

```{example, impeach-slicing}

Recall Example \@ref(exm:impeach).  Remember we are given $\IP(A) = 0.49$, $\IP(D) = 0.31$, and $\IP(A\cap D) = 0.26$ where $A$ is the event that the selected person supports impeachment and $D$ is the event that the selected person is a Democrat.

``` 

1. How many times more likely is it for an *American* to be a Democrat who supports impeachment than to be a Democrat who does not support impeachment?
1. How many times more likely is it for a *Democrat* to support impeachment than to not support impeachment?
1. What do you notice about the answers to the two previous parts?


```{solution, impeach-slicing-sol}
to Example \@ref(exm:impeach-slicing)
```


```{asis, fold.chunk = TRUE}

1. Note that the probability that an American is a Democrat who does not support impeachment is $\IP(A^c \cap D) = \IP(D) - \IP(A\cap D) = 0.31 - 0.26 = 0.05$. The ratio in question is $\frac{\IP(A \cap D)}{\IP(A^c \cap D)} = \frac{0.26}{0.05} = 5.2$. An *American* is 5.2 times more likely to be a Democrat who supports impeachment than to be a Democrat who does not support impeachment.
1. Recall that $\IP(A|D) = 0.839$ and $\IP(A^c|D) = 0.161$. The ratio in question is $\frac{\IP(A |D)}{\IP(A^c | D)} = \frac{0.839}{0.161} = 5.2$. A *Democrat* is 5.2 times more likely to support impeachment than to not support impeachment.
1. The ratios are the same! Conditioning on Democrat just slices out the Americans who are Democrats.  The ratios are determined by the overall probabilities for Americans, as in part 1.  The conditional probabilities, given Democrat, in part 2 simply rescale the probabilities for Americans who are Democrats to add up to 1.

```

The following is a Venn diagram type example of slicing and renormalizing.


```{example venn-conditional}

Each of the three Venn diagrams below represents a sample space with 16 equally likely outcomes.  Let $A$ be the yellow `/`  event, $B$ the blue `\` event, and their intersection $A\cap B$ the green $\times$ event. Suppose that areas represent probabilities, so that for example $\IP(A) = 4/16$.

Find $\IP(A|B)$ for each of the scenarios.  Be sure to indicate what represents the "slice" in each scenario.

``` 



```{r venn-conditional-plot, echo = FALSE}

knitr::include_graphics(c("_graphics/venn-conditional.png"))

```



```{solution, venn-condition-sol}
to Example \@ref(exm:venn-conditional)
```


```{asis, fold.chunk = TRUE}

In each case, the slice represents the 4 blue outcomes.

1. Left: $\IP(A|B)=0$. After conditioning on $B$, there are now 4 equally likely outcomes, of which none satisfy $A$.
1. Middle: $\IP(A|B) = 2/4$. After conditioning on $B$, there are now 4 equally likely outcomes, of which 2 satisfy $A$.
1. Right: $\IP(A|B) = 1/4$. After conditioning on $B$, there are now 4 equally likely outcomes, of which 1 satisfies $A$.

```





### Independence {#independence}


In general, the conditional probability of event $A$ given some other event $B$ is usually different from the unconditional probability of $A$.  That is, in general $\IP(A | B) \neq \IP(A)$.   Knowledge of the occurrence of event $B$ typically influences the probability of event $A$, and vice versa.  If so, we say that events $A$ and $B$ are *dependent*.


However, in some situations knowledge of the occurrence of one event does not influence the probability of another.  For example, if a coin is flipped twice then knowing that the first flip landed on Heads does not change the probability that the second flips lands on Heads.  In these situations we say the events are independent.




```{example puppy}
Consider the following hypothetical data.

|                               | Democrat ($D$) | Not Democrat ($D^c$) | Total |
|-------------------------------|---------------:|---------------------:|------:|
| Loves puppies ($L$)           |            180 |                  270 |   450 |
| Does not love puppies ($L^c$) |             20 |                   30 |    50 |
| Total                         |            200 |                  300 |   500 |

Suppose a person is randomly selected from this group.  Consider the events
\begin{align*}
L & = \{\text{person loves puppies}\}\\
D & = \{\text{person is a Democrat}\}
\end{align*}

```

1. Compute and interpret $\IP(L)$.
1. Compute and interpret $\IP(L|D)$.
1. Compute and interpret $\IP(L|D^c)$.
1. What do you notice about $\IP(L)$, $\IP(L|D)$, and $\IP(L|D^c)$?
1. Compute and interpret $\IP(D)$.
1. Compute and interpret $\IP(D|L)$.
1. Compute and interpret $\IP(D|L^c)$.
1. What do you notice about $\IP(D)$, $\IP(D|L)$, and $\IP(D|L^c)$?
1. Compute and interpret $\IP(D \cap L)$.
1. What is the relationship between $\IP(D \cap L$) and $\IP(D)$ and $\IP(L)$?
1. When randomly selecting a person from this particular group, would you say that events $D$ and $L$ are independent?  Why?


```{solution, puppy-sol}
to Example \@ref(exm:puppy)
```


```{asis, fold.chunk = TRUE}


1. The probability that the randomly selected person loves puppies is $\IP(L)=450/500=0.9$.
1. The conditional probability that the randomly selected person loves puppies given that the person is a Democrat is $\IP(L|D)=180/200=0.9$.
1. The conditional probability that the randomly selected person loves puppies given that the person is not a Democrat is $\IP(L|D^c)=270/300=0.9$.
1. $\IP(L)=\IP(L|D)=\IP(L|D^c)=0.9$.  Regardless of whether or not the person is a Democrat the probability that they love puppies is 0.9, the overall probability that a person loves puppies.
1. The probability that the randomly selected person is a Democrat is $\IP(D)=200/500=0.4$.
1. The conditional probability that the randomly selected person is a Democrat given that the person loves puppies is $\IP(D|L)=180/450=0.4$.
1. The conditional probability that the randomly selected person is a Democrat given that the person does not love puppies is $\IP(D|L^c)=20/50=0.4$.
1. $\IP(D)=\IP(D|L)=\IP(D|L^c)=0.4$.  Regardless of whether or not the person loves puppies the probability that the person is a Democrat is 0.4, the overall probability that a person is a Democrat.
1. The probability that the randomly selected person is a Democrat and loves puppies is $\IP(D \cap L)=180/500=0.36$.
1. $\IP(D \cap L) = 0.36 = (0.4)(0.9)=\IP(D)\IP(L)$.  The joint probability is a product of the marginal probabilities.
1. Yes, the events $D$ and $L$ are independent.  Knowing whether or not the person is a Democrat does not change the probability that the person loves puppies, and vice versa.

```

As in the example, events $A$ and $B$ are **independent** if the knowing whether or not one occurs does not change the probability of the other.
For events $A$ and $B$ (with $0<\IP(A)<1$ and $0<\IP(B)<1$) the following are equivalent.
That is, if one is true then they all are true; if one is false, then they all are false.

\begin{align*}
\text{$A$ and $B$} & \text{ are independent}\\
\IP(A \cap B) & = \IP(A)\IP(B)\\
\IP(A^c \cap B) & = \IP(A^c)\IP(B)\\
\IP(A \cap B^c) & = \IP(A)\IP(B^c)\\
\IP(A^c \cap B^c) & = \IP(A^c)\IP(B^c)\\
\IP(A|B) & = \IP(A)\\
\IP(A|B) & = \IP(A|B^c)\\
\IP(B|A) & = \IP(B)\\
\IP(B|A) & = \IP(B|A^c)
\end{align*}



In general, the multiplication rule says
\begin{align*}
\IP(A \cap B) & = \IP(A|B)\IP(B)\\
\text{Joint} & = \text{Conditional}\times\text{Marginal}
\end{align*}

For independent events, the multiplication rule simplifies

\begin{align*}
\text{If $A$ and $B$ are independent then } && \IP(A \cap B) & = \IP(A)\IP(B)\\
\text{If independent then } && \text{Joint} & = \text{Product of Marginals}
\end{align*}

The last statement above is why independence is represented by the product `*` syntax in Symbulate.
For example, in the meeting problem, if Regina's arrival follows a Uniform(0, 60) model, Cady's follows a Normal(30, 10) model, and they arrive independently of each other, we can simulate pairs of arrivals as follows; note the use of `*`.

```{python}
P = Uniform(0, 60) * Normal(30, 10)
P.sim(5)
```

We can think of `*` as "spin each spinner independently, but what `*` really does is create a joint probability space as the product of two marginal probability spaces.



```{example, independent-mosaic}
Figure \@ref(fig:independent-mosaic-plot) displays four mosaic plots, each representing probabilities corresponding to two events $A$ and $B$.  Which of the mosaic plots represent independent events?
  
```

```{solution, independent-mosaic-sol}
to Example \@ref(exm:independent-mosaic)
```


```{asis, fold.chunk = TRUE}
The bottom two plots represent independent events.  In these situations $\IP(B|A) = \IP(B|A^c) = \IP(B)$.

```

(ref:cap-independent-mosaic) Four different mosaic plots for two events $A$ and $B$.  In which of the plots are the events $A$ and $B$ independent?

```{r independent-mosaic-plot, echo=FALSE, fig.cap="(ref:cap-independent-mosaic)"}

knitr::include_graphics(c("_graphics/independent-mosaic.png"))

```






```{example venn-independent}

Each of the three Venn diagrams below represents a sample space with 16 equally likely outcomes.  Let $A$ be the yellow `/`  event, $B$ the blue `\` event, and their intersection $A\cap B$ the green $\times$ event. Suppose that areas represent probabilities, so that for example $\IP(A) = 4/16$.

In which of the scenarios are events $A$ and $B$ independent?

``` 



```{r venn-independent-plot, echo = FALSE}

knitr::include_graphics(c("_graphics/venn-conditional.png"))

```


```{solution, venn-independent-sol}
to Example \@ref(exm:venn-independent)
```



```{asis, fold.chunk = TRUE}

In each case, $\IP(A)=4/16$.  Condition on event $B$, by zooming in on the blue slice, and see if $\IP(A|B)$ is the same as $\IP(A)$. 

1. Left: $\IP(A|B)=0\neq 4/16 = \IP(A)$.  Therefore, events $A$ and $B$ are not independent.
1. Middle: $\IP(A|B) = 2/4\neq 4/16 = \IP(A)$.  Therefore, events $A$ and $B$ are not independent.
1. Right: $\IP(A|B) = 1/4= 4/16 = \IP(A)$.  Therefore, events $A$ and $B$ are independent. The *ratio of yellow to total* is the same as the *ratio of the green part of blue to blue*.  If we zoom into the blue part of the picture (slice) and then resize it to the size of the original picture (renormalize), then the green part takes up 1/4 of the area just as the yellow part did in the original picture.

```


Do not confuse "disjoint" with "independent".  Disjoint means two events do not "overlap". Independence means two events *"overlap in just the right way"*.  You can pretty much forget "disjoint" exists; you will naturally apply the addition rule for disjoint events correctly without even thinking about it.  Independence is much more important and useful, but also requires more care.


## Marginal distributions {#dist-intro}

Even when outcomes of a random phenomenon are equally likely, values of related random variables are usually not.
The **(probability) distribution** of a collection of random variables identifies the possible values that the random variables can take and their relative likelihoods.
We will see many ways of describing a distribution, depending on how many random variables are involved and their types (discrete or continuous).


In the context of multiple random variables, the distribution of any one of the random variables is called a **marginal distribution**.

### Discrete random variables


The probability distribution of a single discrete random variable $X$ is often displayed in a table containing the probability of the event $\{X=x\}$ for each possible value $x$.


```{example dice-probspace2}
Roll a four-sided die twice; recall the sample space in Example \@ref(exm:dice-rv) and Table \@ref(tab:dice-rv-sol-table).
One choice of probability measure corresponds to assuming that the die is fair and that the 16 possible outcomes are equally likely.
Let $X$ be the sum of the two dice, and let $Y$ be the larger of the two rolls (or the common value if both rolls are the same).
```




1. Construct a table and plot displaying the marginal distribution of $X$.  
1. Construct a table and plot displaying the marginal distribution of $Y$.
1. Describe the distribution of $Y$ in terms of long run relative frequency.
1. Describe the distribution of $Y$ in terms of relative degree of likelihood.


```{solution dice-probspace2-sol}
to Example \@ref(exm:dice-probspace2)
```


```{asis fold.chunk = TRUE}
1. The possible values of $X$ are $2, 3, 4, 5, 6, 7, 8$. Find the probability of each value using Table \@ref(tab:dice-rv-sol-table). For example, $\IP(X = 3) = \IP(\{(1, 2), (2, 1)\}) = 2/16$. Table \@ref(tab:dice-sum-dist-table2) displays the distribution in a table, and Figure \@ref(fig:dice-sum-dist-plot2) displays the corresponding impulse plot.
1. The possible values of $Y$ are $1, 2, 3, 4$. For example, $\IP(Y = 3) = \IP(\{(1, 3), (2, 3), (3, 1), (3, 2), (3, 3)\}) = 5/16$. Table \@ref(tab:dice-max-dist-table2) displays the distribution in a table, and Figure \@ref(fig:dice-max-dist-plot2) displays the corresponding impulse plot.
1. Suppose we roll a fair four-sided die twice and find the larger of the two rolls.
If we repeat this process many times, resulting in many pairs of rolls, then in the long run the larger of the two rolls will be 1 in 6.25% of pairs, 2 in 18.75% pairs,   3 in 31.25% of pairs, and 4 in 43.75% of pairs.
1. When we roll a fair four-sided die twice and find the larger of the two rolls, 1 is the least likely value, 2 is 3 times more likely than 1, 3 is 1.67 times more likely than 2, and 4 is 1.4 times more likely than 3.

``` 

(ref:cap-dice-sum-dist-table2) The marginal distribution of $X$, the sum of two rolls of a fair four-sided die.

```{r, dice-sum-dist-table2, echo = FALSE}
y = 2:8
p = c(1, 2, 3, 4, 3, 2, 1) / 16
knitr::kable(
  data.frame(y, p),
  col.names = c("x", "P(X=x)"),
  booktabs = TRUE,
  caption = "(ref:cap-dice-sum-dist-table2)",
  digits = 4
)
```  

(ref:cap-dice-sum-dist-plot2) The marginal distribution of $X$, the sum of two rolls of a fair four-sided die.

```{r dice-sum-dist-plot2, echo = FALSE, fig.cap = "(ref:cap-dice-sum-dist-plot2)"}

ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 2:8) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "P(X = x)")

```



(ref:cap-dice-max-dist-table2) The marginal distribution of $Y$, the larger (or common value if a tie) of two rolls of a fair four-sided die.

```{r, dice-max-dist-table2, echo = FALSE}
y = 1:4
p = c(1, 3, 5, 7) / 16
knitr::kable(
  data.frame(y, p),
  col.names = c("y", "P(Y=y)"),
  booktabs = TRUE,
  caption = "(ref:cap-dice-max-dist-table2)",
  digits = 4
)
```  

(ref:cap-dice-max-dist-plot2) The marginal distribution of $Y$, the larger (or common value if a tie) of two rolls of a fair four-sided die.


```{r dice-max-dist-plot2, echo = FALSE, fig.cap = "(ref:cap-dice-max-dist-plot2)"}

ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "orange") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 2:8) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "y",
       y = "P(Y = y)")

```


```{example dice-change-dist}
Continuing Example \@ref(exm:dice-probspace2), suppose that instead of a fair die, the weighted die in Example \@ref(exm:die-weighted) is rolled twice.  Answer the following without doing any computations.
```

1. Are the possible values of $X$ the same as in Table \@ref(tab:dice-sum-dist-table2)?  Is the distribution of $X$ the same as in Table \@ref(tab:dice-sum-dist-table2)?
1. Are the possible values of $Y$ the same as in Table \@ref(tab:dice-max-dist-table2)?  Is the distribution of $Y$ the same as in Table \@ref(tab:dice-max-dist-table2)?


```{solution dice-change-dist-sol}
to Example \@ref(exm:dice-change-dist)
```

```{asis, fold.chunk = TRUE}
In both parts, yes the possible values are the same.  There are still 16 possible outcomes and the random variables are still measuring the same quantities as before.  But the distributions are all different.  With the weighted die some outcomes are more likely than others, and some values of the random variables are more likely than before.  For example, the probabilities of the events $\{X = 8\}$, $\{Y=4\}$, and $\{X = 8, Y=4\}$ are larger with the weighted die than with the fair die, because each roll of the weighted die is more likely to result in a 4 than the fair die.
```

The distribution of a random variable depends on the underlying probability measure.
Changing the probability measure can change the distribution of the random variable.
In particular, conditioning on an event can change the distribution of a random variable.



In some cases, a distribution has a "formulaic" shape.
For a discrete random variable $X$, $\IP(X=x)$ can often be written explicitly as a function of $x$.


Consider again $X$, the sum of two rolls of a fair four-sided die.
The probabilities of the possible values $x$ follow a clear triangular pattern as a function of $x$.


(ref:cap-dice-sum-dist-plot2-pmf) The marginal distribution of $X$, the sum of two rolls of a fair four-sided die. The black dots represent the marginal probability mass function of $X$.

```{r dice-sum-dist-plot2-pmf, echo = FALSE, fig.cap = "(ref:cap-dice-sum-dist-plot2-pmf)"}

y = 2:8
p = c(1, 2, 3, 4, 3, 2, 1) / 16

ggplot(data.frame(y, p),
       aes(x = y,
           y = p)) +
  geom_point(col = "black", shape = 1, size = 4) +
  geom_line(linetype = "dotted") +
  geom_segment(aes(x = y,
                   xend = y,
                   y = 0,
                   yend = p), 
               size = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 2:8) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.01), 
                                           add = c(0, 0.01)),
                     limits = c(0, 0.25)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "P(X = x)")

```

For each possible value $x$ of the random variable $X$, $\IP(X=x)$ can be obtained from the following formula

\[
p(x) =
\begin{cases}
\frac{4-|x-5|}{16}, & x = 2, 3, 4, 5, 6,7, 8,\\
0, & \text{otherwise.}
\end{cases}
\]

That is, $\IP(X = x) = p(x)$ for all $x$.
For example, $\IP(X = 2) = 1/16 = p(2)$; $\IP(X=5)=4/16=p(5)$; $\IP(X=7.5)=0=p(7.5)$.
To specify the distribution of $X$ we could provide Table \@ref(tab:dice-rv-sol-table), or we could just provide the function $p(x)$ above. 
Notice that part of the specification of $p(x)$ involves the possible values of $x$; $p(x)$ is only nonzero for $x=2,3, \ldots, 8$.
Think of $p(x)$ as a compact way of representing Table \@ref(tab:dice-sum-dist-table2).
The function $p(x)$ is called the **probability mass function** of the discrete random variable $X$.





### Simulating from a marginal distribution

The distribution of a random variable specifies the long run pattern of variation (or relative degree of likelihood) of values of the random variable over many repetitions of the underlying random phenomenon.
The distribution of a random variable ($X$) can be approximated by

- simulating an outcome of the underlying random phenomenon ($\omega$)
- observing the value of the random variable for that outcome ($(X(\omega)$)
- repeating this process many times
- then computing relative frequencies involving the simulated values of the random variable to approximate probabilities of events involving the random variable (e.g., $\IP(X\le x)$).

We carried out this process for the dice rolling example in Section \@ref(technology-intro), where each repetition involved simulating a pair of rolls (outcome $\omega$) and then finding the sum ($X(\omega)$) and max ($Y(\omega)$).

Any marginal distribution can be represented by a single spinner, as the following example illustrates.

```{example dice-spinners-ex}
Continuing Example \@ref(exm:dice-probspace2).
```

1. Construct a spinner to represent the marginal distribution of $X$.
1. How could you use the spinner from the previous part to simulate a value of $X$.
1. Construct a spinner to represent the marginal distribution of $Y$.
1. How could you use the spinner from the previous part to simulate a value of $Y$.
1. Donny Don't says: "Great! I can simulate an $(X, Y)$ pair just by spinning the spinner in Figure \@ref(fig:dice-spinners-sum) to generate $X$ and the one in Figure \@ref(fig:dice-spinners-max) to generate $Y$."
Is Donny correct?
If not, can you help him see why not?

```{solution dice-spinners-ex-sol}
to Example \@ref(exm:dice-spinners-ex).
```


```{asis, fold.chunk = TRUE}

1. The spinner in Figure \@ref(fig:dice-spinners-sum) represents the marginal distribution of $X$; see Table \@ref(tab:dice-sum-dist-table2).
1. Just spin it! The spinner returns the possible values of $X$ according to the proper probabilities.
If we're interested in simulating the sum of two rolls of a fair four-sided dice, we don't have to roll the dice; we can just spin the $X$ spinner once.
1. The spinner in Figure \@ref(fig:dice-spinners-max) represents the marginal distribution of $Y$; see Table \@ref(tab:dice-max-dist-table2).
1. Just spin it! The spinner returns the possible values of $Y$ according to the proper probabilities.
If we're interested in simulating the larger of two rolls of a fair four-sided dice, we don't have to roll the dice; we can just spin the $Y$ spinner once.
1. Donny is incorrect.
Yes, spinning the $X$ spinner in Figure \@ref(fig:dice-spinners-sum) will generate values of $X$ according to the proper marginal distribution, and similarly with Figure \@ref(fig:dice-spinners-max) and $Y$.
However, spinning each of the spinners will *not* produce $(X, Y)$ pairs with the correct *joint* distribution.
For example, Donny's method could produce $X=2$ and $Y=4$, which is not a possible $(X, Y)$ pair.
Donny's method treats the values of $X$ and $Y$ as if they were *independent*; the result of the $X$ spin would not change what could happen with the $Y$ spin (since the spins are physically independent).
However, the $X$ and $Y$ values are related.
For example, if $X=2$ then $Y$ must be 1; if $X=4$ then $Y$ must be 2 or 3; etc.
Later we'll see a spinner which does properly simulate $(X, Y)$ pairs.

``` 

(ref:cap-dice-spinners-sum) Spinner representing the marginal distribution of $X$, the sum of two rolls of a fair four-sided die. 

```{r dice-spinners-sum, echo=FALSE, fig.cap="(ref:cap-dice-spinners-sum)", fig.height=12, fig.align='center'}

knitr::include_graphics(c(
  "_graphics/spinner-dice-sum-marginal.png"))

```


(ref:cap-dice-spinners-max) Spinner representing the marginal distribution of $Y$, the larger of two rolls of a fair four-sided die.

```{r dice-spinners-max, echo=FALSE, fig.cap="(ref:cap-dice-spinners-max)", fig.height=12, fig.align='center'}

knitr::include_graphics(c(
  "_graphics/spinner-dice-max-marginal.png"))

```


In principle, there are always two ways of simulating a value $x$ of a random variable $X$.

1. **Simulate from the probability space.** Simulate an outcome $\omega$ from the underlying probability space and set $x = X(\omega)$.
1. **Simulate from the distribution.** Construct a spinner corresponding to the distribution of $X$ and spin it once to generate $x$.

The second method requires that the distribution of $X$ is known.
However, as we will see in many examples, *it is common to specify the distribution of a random variable directly without defining the underlying probability space*.




```{example dice-marginal-sim-from-dist}
Continuing the dice rollowing example, we saw the Symbulate code for the "simulate from the probability space" method in Section \@ref(technology-intro).
Now we consider the "simulate from the distribution method".

1. Write the Symbulate code to define $X$ by specifying its marginal distribution.
1. Write the Symbulate code to define $Y$ by specifying its marginal distribution.

```

```{solution dice-marginal-sim-from-dist-sol}

to Example \@ref(exm:dice-marginal-sim-from-dist).

```

```{asis, fold.chunk = TRUE}

We simulate a value of $X$ from its marginal distribution by spinning the spinner in Figure \@ref(fig:dice-spinners-sum).
Similarly, we simulate a value of $Y$ from its marginal distribution by spinning the spinner in Figure \@ref(fig:dice-spinners-max).
We can define a BoxModel corresponding to each of these spinners, and then define a random variable through the identify function.
Essentially, we define the random variable by specifying its distribution, rather specifying the underlying probability space.
Note that the default `size` argument in `BoxModel` is `size = 1`, so we have omitted it.

Careful: the method below will not work if we want to simulate $(X, Y)$ pairs.

```

```{python}
X = RV(BoxModel([2, 3, 4, 5, 6, 7, 8], probs = [1 / 16, 2 / 16, 3 / 16, 4 / 16, 3 / 16, 2 / 16, 1 / 16]))

x = X.sim(10000)
```

```{python}
x.tabulate(normalize = True)
```


```{python, eval = FALSE}
x.plot()
```


```{python, echo = FALSE}
plt.figure()
x.plot()
plt.show()
```


Similarly we can define $Y$ as

```{python}
Y = RV(BoxModel([1, 2, 3, 4], probs = [1 / 16, 3 / 16, 5 / 16, 7 / 16]))


```

`BoxModel` with the `probs` option can be used to define general discrete distributions (when there are finitely many possible values).
Many commonly encountered distributions have special names and are in built in to Symbulate.
For example, we will see that a random variable that takes values 0, 1, 2, 3 with respective probability 1/8, 3/8, 3/8, 1/8 follows a "Binomial(3, 0.5)" distribution; in Symbulate we can use `Binomial(3, 0.5)` in place of `BoxModel([0, 1, 2, 3], probs = [1 / 8, 3 / 8, 3 / 8, 1 / 8 ])`.



### Continuous random variables {#sec-linear-rescaling}





Uniform probability measures are the continuous analog of equally likely outcomes.  The standard uniform model is the Uniform(0, 1) distribution corresponding to the spinner in Figure \@ref(fig:uniform-spinner-10-sectors) which returns values between^[Why is the interval $[0, 1]$ the standard instead of some other range of values?  Because probabilities take values in $[0, 1]$.  We will see why this is useful in more detail later.] 0 and 1.  Only select rounded values are displayed, but the spinner represents an idealized model where the spinner is infinitely precise so that any real number between 0 and 1 is a possible value. We assume that the (infinitely fine) needle is "equally likely" to land on any value between 0 and 1.


(ref:cap-uniform-spinner-10-sectors) A continuous Uniform(0, 1) spinner. Only selected rounded values are displayed, but in the idealized model the spinner is infinitely precise so that any real number between 0 and 1 is a possible value.

```{r uniform-spinner-10-sectors, echo=FALSE, fig.cap="(ref:cap-uniform-spinner-10-sectors)", fig.align='center'}

n = 10

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("1|0", 1 * xp$x[-1])) +
  theme(axis.text.x=element_text(size=14, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
  ggtitle(paste("Uniform(0, 1) model", sep=""))

spinner
```

Let $U$ be the random variable representing the result of a single spin of the Uniform(0, 1) spinner.
The following Symbulate code defines an `RV` that follows the Uniform(0, 1) distribution.
(Recall that the default function used to define a Symbulate `RV` is the identity.)


```{python}

U = RV(Uniform(0, 1))

u = U.sim(100)

u
```

Notice the number of decimal places.  Remember that for a continuous random variable, any value in some uncountable interval is possible.  For the Uniform(0, 1) distribution, any value in the continuous interval between 0 and 1 is a distinct possible value: 0.20000000000... is different from 0.20000000001... is different from 0.2000000000000000000001... and so on.

The rug plot displays 100 simulated values of $U$.  Note that the values seem to be "evenly spread" between 0 and 1, though there is some natural simulation variability.



```{python, eval = FALSE}

u.plot('rug')

```

```{python, echo = FALSE}
plt.figure()
u.plot('rug')
plt.show()
```


For continuous random variables, finding frequencies of individual values and impulse plots don't make any sense since each simulated value will only occur once in the simulation. (Again, note the number of decimal places; if the first simulated value is 0.234152908738084237900631086, we're not going to see that exact value again.)
Instead, we summarize simulated values of continuous random variables in a histogram (the Symbulate default plot for summarizing values on a continuous scale).
A **histogram** groups the observed values into "bins" and plots densities or frequencies for each bin^[Symbulate chooses the number of bins automatically, but you can set the number of bins using the `bins` option, e.g., `.plot(bins = 10)`].


```{python, eval = FALSE}
u.plot()
```


```{python, echo = FALSE}
plt.figure()
u.plot()
plt.show()
```


Below we add a rug plot to the histogram to see the individual values that into each bin.
We also add the argument `normalize = False` to display bin frequencies, that is, counts of the simulated values falling in each bin, on the vertical axis.



```{python, eval = FALSE}

u.plot(['rug', 'hist'], bins = 10, normalize = False)

```

```{python, echo = FALSE}

plt.figure()
u.plot(['rug', 'hist'], bins = 10, normalize = False)
plt.show()

```


Typically, in a histogram *areas* of bars represent relative frequencies; in which case the axis which represents the height of the bars is called "density".
It is recommended that the bins all have the same width^[
Symbulate will always produce a histogram with equal bin widths.] so that the ratio of the heights of two different bars is equal to the ratio of their areas.
That is, with equal bin widths, bars with the same height represent the same area/relative frequency; though the area of the bar rather than the height is the actual relative frequency. 

The distribution of a random variable represents in long run pattern of variation, so we won't get a very clear picture with just 100 simulated values.
Now we simulate many values of $U$.  We see that the bars all have roughly the same height, represented by the horizontal line, and hence roughly the same area/relative frequency, though there are some natural fluctuations due to the randomness in the simulation.


```{python, eval = FALSE}

u = U.sim(10000)

u.plot()
Uniform(0, 1).plot() # plots the horizontal line of constant height

```

```{python, echo = FALSE}

u = U.sim(10000)

plt.figure()
u.plot()
Uniform(0, 1).plot() # plots the horizontal line of constant height
plt.show()

```

If we simulate even more values and use even more bins, we can see that the density height is roughly the same for all possible value (aside from some natural simulation variability).
This "curve" with constant height over [0, 1] is called the "Uniform(0, 1) density".


```{python, eval = FALSE}

u = U.sim(100000)
u.plot(bins = 100)
Uniform(0, 1).plot() # plots the horizontal line of constant height

```

```{python, echo = FALSE}

plt.figure()
u = U.sim(100000)
u.plot(bins = 100)
Uniform(0, 1).plot() # plots the horizontal line of constant height
plt.show()

```


*Area* represents relative frequency/probability for histograms and densities.
We can approximate the probability that $U$ is less than 0.2 by the corresponding relative frequency, which is represented by the *area* of the histogram over [0, 0.2].

```{python}
u.count_lt(0.2) / u.count()
```

```{python, eval = FALSE}
u.plot()
Uniform(0, 1).plot() # plots the horizontal line of constant height
plt.fill_between(np.arange(0.0, 0.21, 0.01), 0, 1, alpha = 0.7, color = 'gray'); # shades the plot

```


```{python, echo = FALSE}
plt.figure()

u.plot()
Uniform(0, 1).plot() # plots the horizontal line of constant height
plt.fill_between(np.arange(0.0, 0.21, 0.01), 0, 1, alpha = 0.7, color = 'gray'); # shades the plot

plt.show()

```

```{example uniform-zero-prob-sim}
Let $U$ be a random variable with a Uniform(0, 1) distribution.
Suppose we want to approximate $\IP(U = 0.2)$; that is,  $\IP(U = 0.2000000000000000000000\ldots)$, the probability that $U$ is equal to 0.2 with infinite precision.
```




1. Use simulation to approximate $\IP(U = 0.2)$.
1. What do you notice? Why does this make sense?
1. Use simulation to approximate $\IP(|U - 0.2|<0.005) = \IP(0.195 < U < 0.205)$, the probability that $U$ *rounded to two decimal places* is equal to 0.2.
1. Use simulation to approximate $\IP(|U - 0.2|<0.0005) = \IP(0.1995 < U < 0.2005)$, the probability that $U$ *rounded to three decimal places* is equal to 0.2.
1. Explain why $\IP(U = 0.2) = 0.$ Also explain, why this is not a problem in practical applications.


```{solution uniform-zero-prob-sim-sol}
to Example \@ref(exm:uniform-zero-prob-sim)
```


```{asis fold.chunk = TRUE}

1. No matter how many values we simulate, the simulated frequency of 0.2000000000000000000000 is going to be 0.
1. Any value in the uncountable, continuous interval [0, 1] is possible. The chance of getting 0.2000000000000000000, with infinite precision, is 0.
1. While no simulated value is equal to 0.2, about 1% of the simulated values are within 0.005 of 0.2.
1. While no simulated value is equal to 0.2, about 0.1% of the simulated values are within 0.0005 of 0.2.
1. See below for discussion.
```

```{python}

u.count_eq(0.2000000000000000000000)

```

```{python}

u.filter_lt(0.205).filter_gt(0.195)

```

```{python}

abs(u - 0.2).count_lt(0.01 / 2) / u.count()

```


```{python}

u.filter_lt(0.2005).filter_gt(0.1995)

```

```{python}

abs(u - 0.2).count_lt(0.001 / 2) / u.count()

```



**The probability that a continuous random variable $X$ equals any particular value is 0.** That is, if $X$ is continuous then $\IP(X=x)=0$ for all $x$.
A continuous random variable can take uncountably many distinct values. Simulating values of a continuous random variable corresponds to an idealized spinner with an infinitely precise needle which can land on any value in a continuous scale.
Of course, infinite precision is not practical, but continuous distributions are often reasonable and tractable mathematical models.

In the Uniform(0, 1) case, $0.200000000\ldots$ is different than $0.200000000010\ldots$ is different than $0.2000000000000001\ldots$, etc.  Consider the spinner in Figure \@ref(fig:uniform-spinner-10-sectors). The spinner in the picture only has tick marks in increments of 0.01.
When we spin, the probability that the needle lands closest to the 0.2 tick mark is 0.01.  But if the spinner were labeled in 1000 increments of 0.001, the probability of landing closest to the 0.2 tick mark is 0.001.  And with four decimal places of precision, the probability is 0.0001. And so on.  The more precise we mark the axis, the smaller the probability the spinner lands closest to the 0.2 tick mark.  The Uniform(0, 1) density represents what happens in the limit as the spinner becomes infinitely precise.  The probability of landing closest to the 0.2 tick mark gets smaller and smaller, eventually becoming 0 in the limit.

Even though any specific value of a continuous random variable has probability 0, *intervals* still can have positive probability.
In particular, the probability that a continuous random variable  is "close to" a specific value can be positive.
In practical applications involving continuous random variables we always have some reasonable degree of precision in mind.
For example, if we're interested in the probability that the high temperature tomorrow is 70 degrees F, we're talking to the nearest degree, or maybe 0.1 degrees, but not "exactly equal to 70.0000000000000".
In practical applications involving continuous random variables, "equal to" really means "close to", and "close to" probabilities correspond to intervals which can have positive probability.

In the meeting problem, assume that Regina's arrival time $R$ follows a Uniform(0, 60) distribution.
Here is a histogram of simulated values, along with the Uniform(0, 60) density.

```{python, eval = FALSE}

R = RV(Uniform(0, 60))

R.sim(10000).plot()
Uniform(0, 60).plot()

```

```{python, echo = FALSE}

R = RV(Uniform(0, 60))

plt.figure()
R.sim(10000).plot()
Uniform(0, 60).plot()
plt.show()
```





Remember that in a histogram, *area* represents relative frequency.  The Uniform(0, 60) distribution covers a wider range of possible values than the Uniform(0, 1) distribution.  Notice how the values on the vertical density axis change to compensate for the longer range on the horizontal variable axis.  The histogram bars now all have a height of roughly $\frac{1}{60} = 0.0167$ (aside from natural simulation variability).  The total area of the rectangle with a height of $\frac{1}{60}$ and a base of $60$ is 1.

### Normal distributions {#sim-normal}

Now suppose that in the meeting problem we assume that Regina's arrival time $R$ follows a Normal(30, 10) distribution.
Recall that we represented a Normal(30, 10) distribution with the following spinner. (It's the same spinner on both side, just with different features highlighted.)

```{r, echo = FALSE, ref.label = c('meeting-normal-spinner'), fig.align='center', fig.show="hold", out.width="50%"}

```


Notice that the values on the spinner axis are *not* equally spaced.  Even though only some values are displayed on the spinner axis, imagine this spinner represents an infinitely fine model where any value between 0 and 60 is possible^[Technically, for a Normal distribution, *any* real value is possible.  But values that are more than 3 or 4 standard deviations occur with small probability.].


Since the axis values are not evenly spaced, different intervals of the same length will have different probabilities.  For example, the probability that this spinner lands on a value in the interval [20, 30] is about 0.341, but it is about 0.136 for the interval [10, 20]. 

Consider what the distribution of values simulated using this spinner would look like.

- About half of values would be below 30 and half above
- Because axis values near 30 are stretched out, values near 30 would occur with higher frequency than those near 0 or 60.
- The shape of the distribution would be symmetric about 30 since the axis spacing of values below 30 mirrors that for values above 500.
For example, about 34% of values would be between 20 and 30, and also 34% between 30 and 40.
- About 68% of values would be between 20 and 40.
- About 95% of values would be between 10 and 50.

And so on.  We could compute percentages for other intervals by measuring the areas of corresponding sectors on the spinner to complete the pattern of variability that values resulting from this spinner would follow.  This particular pattern is called a "Normal(30, 10)" distribution.

As in the previous section we can define a random variable in Symbulate by specifying its distribution.


```{python}

R = RV(Normal(30, 10))

r = R.sim(100)
r
```

Plotting the values, we see that values near 30 occur more frequently than those near 0 or 60.
The histogram is not "flat" like in the Uniform case.

```{python, eval = FALSE}

r.plot(['rug', 'hist'])

```

```{python, , echo = FALSE}

plt.figure()
r.plot(['rug', 'hist'])
plt.show()

```

We'll get a much clearer picture of the distribution if we  simulate many values.
We see that the histogram appears like it can be approximated by a smooth, "bell-shaped" curve, called the *Normal(30, 10) density*.



(ref:cap-normal-sat-density) Histogram representing the approximate distribution of values simulated using the spinner in Figure \@ref(fig:meeting-normal-spinner).  The smooth solid curve models the theoretical shape of the distribution, called the "Normal(30, 10)" distribution. 


```{python, eval = FALSE}
r = R.sim(10000)

r.plot() # plot the simulated values
Normal(30, 10).plot() # plot the smooth density curve


```


```{python normal-sat-density, echo = FALSE, fig.cap="(ref:cap-normal-sat-density)"}
r = R.sim(10000)

plt.figure()
r.plot() # plot the simulated values
Normal(30, 10).plot() # plot the smooth density curve
plt.show()

```

The bell-shaped curve depicting the Normal(30, 10) density is one **probability density function**.
Probability density functions for continuous random variables are analogous to probability mass functions for discrete random variables.  However, while they play analogous roles, they are different in one fundamental way; namely, a probability mass function outputs probabilities directly, while a probability density function does not.  A probability density function only provides the density height; the *area under the density curve determines probabilities*. We will investigate densities in more detail later.

For the Normal(30, 10) distribution the parameter 30 represents the mean, and the parameter 10 represents the standard deviation. We will discuss these ideas in more detail later.



### Percentiles

A distribution is characterized by its percentiles.


```{example meeting-normal-percentile}
Recall that the spinner in Figure \@ref(fig:meeting-normal-spinner) represents the Normal(30, 10) distribution.
According to this distribution:
```

1. What percent of values are less than 23.26?
1. What is the 25th percentile?
1. What is the 75th percentile?
1. A value of 40 corresponds to what percentile?

```{solution meeting-normal-percentile-sol}
to Example \@ref(exm:meeting-normal-percentile)
```


```{asis fold.chunk = TRUE}

1. From the picture of the spinner, 25% of values are less than 23.26.
1. The previous part implies that 23.26 is the 25th percentile.
1. 75% of values are less than 36.74, so 36.74 is the 75th percentile.
1. 40 is (roughly) the 84th percentile. About 84% of values are less than 40.

```



Roughly, the value $x$ is the $p$th **percentile** (a.k.a. quantile) of a distribution of a random variable $X$ if $p$ percent of values of the variable are less than or equal to $x$: $\IP(X\le x) = p$.
A spinner basically describes a distribution by specifying all the percentiles.

- The 25th percentile goes 25% of the way around the axis (at "3 o'clock")
- The 50th percentile goes 50% of the way around the axis (at "6 o'clock")
- The 75th percentile goes 75% of the way around the axis (at "9 o'clock")

And so on. Filling in the circular axis of the spinner with the appropriate percentiles determines the probability that the random variable lies in any interval.

In Symbulate, percentiles of simulated values can be computed using `quantile`.
The 25th percentile of the simulated values of $R$ is



```{python}

r.quantile(0.25)
```

Remember, we're dealing with simulated values so they won't follow the distribution exactly. Compare with

```{python}
r.count_lt(23.26) / r.count()
```


The 75th percentile of simulated values is


```{python}

r.quantile(0.75)
```


We can also compute quantiles of the theoretical distribution; compare with the spinner.

```{python}
Normal(30, 10).quantile(0.25)
```

```{python}
Normal(30, 10).quantile(0.75)
```

```{python}
Normal(30, 10).quantile(0.84)
```

```{python}
Normal(30, 10).quantile(0.975)
```

Continuing in this way we can fill in the rest of the axis labels on the Normal(30, 10) spinner. For example, `Normal(30, 10).quantile(0.854)` tells us the value that should go 85.4% of the way around the spinner axis (at "10:15").
The percentiles of the Normal(30, 10) distribution are what determine that particular bell-shape.



### Transformations


Many random variables are derived as transformations of other random variables.
A function of a random variable is a random variable: if $X$ is a random variable and $g$ is a function then $Y=g(X)$ is a random variable.  In general, the distribution of $g(X)$ will have a different shape than the distribution of $X$.  The exception is when $g$ is a linear rescaling.


A **linear rescaling** is a transformation of the form $g(u) = a +bu$, where $a$ (intercept) and $b$ (slope^[You might be familiar with "$mx+b$" where $b$ denotes the intercept. In Statistics, $b$ is often used to denote slope. For example, in R `abline(a = 32, b = 1.8)` draws a line with intercept 32 and slope 1.8.]) are constants.
For example, converting temperature from Celsius to Fahrenheit using $g(u) = 32 + 1.8u$ is a linear rescaling.


If $U$ has a Uniform(0, 1) distribution, its possible values lie in [0, 1].
The random  variable $60U$, a linear rescaling of $U$, takes values in [0, 60], and its distribution has a uniform shape.
The linear rescaling changes the range of possible values, but not the general shape of the distribution.

```{python, eval = FALSE}

U = RV(Uniform(0, 1))

R = 60 * U


R.sim(10000).plot()
Uniform(0, 60).plot() # plot the Uniform(0, 60) density

```



```{python, echo = FALSE}

U = RV(Uniform(0, 1))


plt.figure()
(60 * U).sim(10000).plot()
Uniform(0, 60).plot() # plot the Uniform(0, 60) density
plt.show()

```

This suggests two ways of simulating a value from a Uniform(0, 60) distribution:

- Construct a Uniform(0, 60) spinner and spin it.
- Spin a Uniform(0, 1) spinner and multiply the result by 60.


What about a nonlinear transformation, like a logarithmic or square root transformation?  In contrast to a linear rescaling, a nonlinear rescaling does change the shape of a distribution.

Let's consider $U^22$, where $U$ has a Uniform(0, 1) distribution. Now $U^2$ also takes values in [0, 1], but its distribution does not have a uniform shape.



```{python, eval = FALSE}
Z = U ** 2

z = Z.sim(10000)

z.plot()

```


```{python, echo = FALSE}
Z = U ** 2

z = Z.sim(10000)

plt.figure()
z.plot()
plt.show()

```



We see that $U^2$ is more likely to be near 0 than near 1.
Squaring numbers between 0 and 1 makes "pushes" them towards $0.5^2 = 0.25$, $0.1^2 = 0.01$, etc.
The following spinner corresponds to the distribution approximated by the histogram above.



```{r uniform-square, echo = FALSE}

n = 16

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("1|0", round(1 * xp$x[-1] ^ 2, 2))) +
  theme(axis.text.x=element_text(size=12, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
  ggtitle(paste("Distribution of U^2, where U~Uniform(0, 1)", sep=""))

spinner
```



See how the values on the spinner correspond to the percentiles of the simulated values.


```{python}

z.quantile(0.25)
```


```{python}

z.quantile(0.5)
```

```{python}

z.quantile(0.75)
```





## Averages {#LRA}

On any single repetition of the simulation a particular event either occurs or not.  Summarizing simulation results for events involves simply counting the number of repetitions on which the event occurs and finding related proportions.

On the other hand, random variables typically take many possible values over the course of many repetitions.  We are still interested in relative frequencies of events, like $\{X=6\}$, $\{Y \ge 3\}$, and $\{X > 5, Y \ge 3\}$.  But for random variables we are also interested in their distributions which describe the possible values that the random variables can take and their relative likelihoods.  While the marginal distribution contain all the information about a single random variables, it is also useful to summarize some key features of a distribution. For example, probabilities of particular events concerning a random variable can be interpreted as long run relative frequencies.

One summary characteristic of a distribution is the **long run average value** of the random variable.  We can approximate the long run average value by simulating many values of the random variable and computing the average (mean) in the usual way.

```{example dice-sim-tactile-ev}
Let $X$ be the sum of two rolls of a fair four-sided die, and let $Y$ be the larger of the two rolls (or the common value if a tie).
Recall your tactile simulation from Example \@ref(exm:dice-sim-tactile). Based only on the results of your simulation, approximate the long run average value of each of the following.
(Don't worry if the approximations are any good yet.)

```

1. $X$
1. $Y$
1. $X^2$
1. $XY$


```{solution dice-sim-tactile-ev-sol}

to Example \@ref(exm:dice-sim-tactile-ev).  


```

```{r, echo = FALSE}

u1 = c(2, 1, 3, 4, 3, 3, 2, 2, 1, 3)
u2 = c(1, 1, 3, 3, 2, 4, 3, 4, 2, 4)
x = u1 + u2
y = pmax(u1, u2)

die_df = data.frame(1:10, u1, u2, x, y, x ^ 2, x * y)


```

<!-- NEED TO FIGURE OUT HOW TO SHOW/HIDE THE STUFF BELOW THAT HAS BOTH R AND TEXT -->

Our simulation results are in Table \@ref(tab:dice-sim-tactile-results-ev) below.

1. Approximate the long run average value of $X$ by summing the 10 simulated values of $X$ and dividing by 10.
\[
\frac{`r paste(x, collapse=" + ")`}{10} = `r round(mean(x), 3)`
\]
1. Approximate the long run average value of $Y$ by summing the 10 simulated values of $Y$ and dividing by 10.
\[
\frac{`r paste(y, collapse=" + ")`}{10} = `r round(mean(y), 3)`
\]
1. First, for each repetition square the value of $X$ to obtain the $X^2$ column. Then approximate the long run average value of $X^2$ by summing the 10 simulated values of $X^2$ and dividing by 10.
\[
\frac{`r paste(x ^ 2, collapse=" + ")`}{10} = `r round(mean(x ^ 2), 3)`
\]
<!-- paste(paste("(", x, ")^2", sep=""), collapse = "+") -->
<!-- expression(paste(x ^"2", sep="")) -->
1. First, for each repetition compute the product $XY$ to obtain the $XY$ column. Then approximate the long run average value of $XY$ by summing the 10 simulated values of $XY$ and dividing by 10.
\[
\frac{`r paste(x * y, collapse=" + ")`}{10} = `r round(mean(x * y), 3)`
\]
<!-- paste(paste("(",x, ")(", y, ")", sep=""), collapse = " + ") -->




We reproduce the results of our simulation in 
Table \@ref(tab:dice-sim-tactile-results-ev) with additional columns for $X^2$ and $XY$. Results vary naturally so your simulation results will be different, but the same ideas apply.

```{r, dice-sim-tactile-results-ev, echo = FALSE}


knitr::kable(
  die_df, booktabs = TRUE,
  col.names = c("Repetition", "First roll", "Second roll", "X", "Y", expression(X^2), "XY"),
  caption = "Results of 10 repetitions of two rolls of a fair four-sided die"
)

```


Of course, 10 repetitions is not enough to reliably approximate the *long run* average value.
But whether the average is based on 10 values or 10 million, an average is computed in the usual way: sum the values and divide by the number of values.
We'll consider what happens in the long run soon, but first a caution about averages of transformations.

```{example dd-lra}
Donny Don't says: "Why bother creating columns for $X^2$ and $XY$? If I want to find the average value of $X^2$ I can just square the average value of $X$. For the average value of $XY$ I can just multiply the average value of $X$ and the average value of $Y$." Do you agree?  (Check to see if this works for your simulation results.) If not, explain why not.
```

```{solution dd-lra-sol}
to Example \@ref(exm:dd-lra)
```




It is easy to check that Donny is wrong just by inspecting the simulation results: `r round(mean(x), 3)`^2^ $\neq$ `r round(mean(x ^ 2), 3)`,
`r round(mean(x), 3)` $\times$ `r round(mean(y), 3)`  $\neq$ `r round(mean(x * y), 3)`.
<!-- $`r paste(round(mean(x), 3), sep = "")`^2 \neq `r paste(round(mean(x ^ 2), 3), sep = "")`$, $`r paste(round(mean(x), 3), sep = "")`\times `r paste(round(mean(y), 3), sep = "")`  \neq `r paste(round(mean(x * y), 3), sep = "")`$. -->
To see why, suppose we had just performed two repetitions, resulting in the first two rows of Table \@ref(tab:dice-sim-tactile-results-ev).
\[
\text{Average of $X^2$} = \frac{3^2 + 2^2}{2} =6.5 \neq 6.25= \left(\frac{3 + 2}{2}\right)^2=(\text{Average of $X$})^2
\]
Squaring first and then averaging (which yields 6.5) is not the same as averaging first and then squaring (which yields 6.25), essentially because $(3+2)^2\neq 3^2 + 2^2$.

Similarly,
\[
{\small
\text{Average of $XY$} = \frac{(3)(2) + (2)(1)}{2} =4 \neq 3.75= \left(\frac{3 + 2}{2}\right)\left(\frac{2 + 1}{2}\right)=(\text{Average of $X$})\times (\text{Average of $Y$})
}
\]
Multiplying first and then averaging (which yields 4) is not the same as averaging first and then multiplying (which yields 3.75), essentially because $(3)(2)+(2)(1)\neq(3+2)(2+1)$.



In general the order of transforming and averaging is not interchangeable.
Whether in the short run or the long run, in general
$$
\begin{align*}
\text{Average of $g(X)$} & \neq g(\text{Average of $X$})\\
\text{Average of $g(X, Y)$} & \neq g(\text{Average of $X$}, \text{Average of $Y$})
\end{align*}
$$
Many common mistakes in probability result from not heeding this principle, so we will introduce many related examples to help you practice your understanding.




### Long run averages {#sim-lra}





```{r, echo = FALSE}

n = 10 ^ (1:3)

last_n = 1000

r = 4

x_n = matrix(sample(2:8, size = last_n * r, replace = TRUE, prob = c(1, 2, 3, 4, 3, 2, 1) / 16), ncol = r)

x_n[1:n[1], 1] = x

xbar_n = matrix(rep(NA, last_n * r), ncol = r)


for (rs in 1:r) {
  xbar_n[, rs] = cumsum(x_n[, rs]) / (1:last_n)
}



xbar_n = xbar_n %>% as.data.frame() %>%
  mutate(i_n = row_number()) %>%
  pivot_longer(cols = !i_n,
                      names_to = "set",
                      values_to = "average") %>%
  mutate(set = str_remove(set, "V"))



```

Now let's see what happens in the long run. Let $X$ be the sum of two rolls of a fair four-sided die.
Table \@ref(tab:dice-lra-table) displays the results of `r n[1]` pairs of rolls of a fair four-sided die.
The first column is the repetition number (first pair, second pair, and so on) and the second column represents $X$, the sum of the two rolls.
The third column displays the *running sum of $X$ values*, and the fourth column the *running average of $X$ values*.
Of course, the results depend on the particular sequence of rolls.
We encourage you to roll the dice and compare your results.

 

```{r dice-lra-table, echo = FALSE}

knitr::kable(
  data.frame(i_n[1:n[1]],
             x_n[1:n[1]],
             cumsum(x_n[1:n[1]]),
             xbar_n %>% filter(set == "1") %>% filter(i_n <= n[1]) %>% select(average)),
  align = "r",
  col.names = c("Repetition", "Value of X",
                "Running sum of X",
                "Running average of X"),
  digits = 3,
  booktabs = TRUE,
  caption = 'Results and running average of $X$, the sum of two rolls of a fair four-sided die.'
)

```


(ref:dice-lra-plot-cap) Running average of $X$ for the 10 pairs of rolls in Table \@ref(tab:dice-lra-table).

```{r dice-lra-plot, echo = FALSE, fig.cap = '(ref:dice-lra-plot-cap)'}

ggplot(xbar_n %>%
         filter(set == "1") %>%
         filter(i_n <= n[1]),
       aes(x = i_n,
           y = average,
           col = set)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 5, lty = "dotted") +
  scale_x_continuous(breaks = 1:n[1]) +
  scale_y_continuous(limits = c(2, 8)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "Repetition",
       y = "Running average of X")
  

```

Now we'll perform 90 more repetitions for a total of 100.
The plot on the left in Figure \@ref(fig:dice-lra-plot2) summarizes the results, while the plot on the right also displays the results for 3 additional sets of 100 pairs of rolls.
The running average fluctuates considerably in the early stages, but settles down and tends to get closer to 5 as the number of flips increases.
However, each of the fours sets results in a different average of $X$ after 100 pairs of rolls: `r xbar_n %>% filter(set == "1", i_n == n[2]) %>% pull(average)` (gray), `r xbar_n %>% filter(set == "2", i_n == n[2]) %>% pull(average)` (orange), `r xbar_n %>% filter(set == "3", i_n == n[2]) %>% pull(average)` (blue), `r xbar_n %>% filter(set == "4", i_n == n[2]) %>% pull(average)` (green).
Even after 100 pairs of rolls the running average of $X$ still fluctuates.

(ref:dice-lra-plot-cap2) Running average of $X$, the sum of two rolls of a fair four-sided die, for four sets of 100 pairs of rolls.

```{r dice-lra-plot2, echo = FALSE, fig.cap = '(ref:dice-lra-plot-cap2)', fig.show="hold", out.width="50%"}


ggplot(xbar_n %>%
         filter(set == "1") %>%
         filter(i_n <= n[2]),
       aes(x = i_n,
           y = average,
           col = set)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 5, lty = "dotted") +
  scale_x_continuous(breaks = seq(0, n[2], 10)) +
  scale_y_continuous(limits = c(2, 8)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "Repetition",
       y = "Running average of X")


ggplot(xbar_n %>%
         filter(i_n <= n[2]),
       aes(x = i_n,
           y = average,
           col = set)) +
  geom_line(aes(linetype = set)) +
  geom_point(aes(shape = set)) +
  geom_hline(yintercept = 5, lty = "dotted") +
  scale_x_continuous(breaks = seq(0, n[2], 10)) +
  scale_y_continuous(limits = c(2, 8)) +
  theme_classic() +
  labs(x = "Repetition",
       y = "Running average of X")



```

Now we'll add 900 more repetitions for a total of 1000 in each set.
The plot on the left in Figure \@ref(fig:dice-lra-plot3) summarizes the results for our original set, while the plot on the right also displays the results for the three additional sets.
Again, the running average of $X$ fluctuates considerably in the early stages, but settles down and tends to get closer to 5 as the number of repetitions increases.
Compared to the results after 100 flips, there is less variability between sets in the running average of $X$ after 1000 flips: `r xbar_n %>% filter(set == "1", i_n == n[3]) %>% pull(average)` (gray), `r xbar_n %>% filter(set == "2", i_n == n[3]) %>% pull(average)` (orange), `r xbar_n %>% filter(set == "3", i_n == n[3]) %>% pull(average)` (blue), `r xbar_n %>% filter(set == "4", i_n == n[3]) %>% pull(average)` (green).
Now, even after 1000 repetitions the running average of $X$ isn't guaranteed to be exactly 5, but we see a tendency for the running average of $X$ to get closer to 5 as the number of repetitions increases.

(ref:dice-lra-plot-cap3) Running average of $X$, the sum of two rolls of a fair four-sided die, for four sets of 1000 pairs of rolls.

```{r dice-lra-plot3, echo = FALSE, fig.cap = '(ref:dice-lra-plot-cap3)', fig.show="hold", out.width="50%"}

ggplot(xbar_n %>%
         filter(set == "1"),
       aes(x = i_n,
           y = average,
           col = set)) +
  geom_line() +
  geom_hline(yintercept = 5, lty = "dotted") +
  scale_x_continuous(breaks = seq(0, n[3], 100)) +
  scale_y_continuous(limits = c(2, 8)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "Repetition",
       y = "Running average of X")

ggplot(xbar_n ,
       aes(x = i_n,
           y = average,
           col = set)) +
  geom_line(aes(linetype = set)) +
  geom_hline(yintercept = 5, lty = "dotted") +
  scale_x_continuous(breaks = seq(0, n[3], 100)) +
  scale_y_continuous(limits = c(2, 8)) +
  theme_classic() +
  labs(x = "Repetition",
       y = "Running average of X")

```

Recall the marginal distribution of $X$, depicted in Figure \@ref(fig:dice-sum-dist-plot2).
The plot shows that 5 is the "balance point" of the distribution, so we might expect the "true" long run average value of $X$ to be 5.
It is, and our simulation results agree.
We will discuss later how to compute the "true" long run average value.
For now, we'll rely on simulation: we can approximate the long run average variable of a random variable $X$ by simulating many values of $X$ and finding the average in the usual way.

In Symbulate, we first simulate and store 10000 values of $X$.

```{python}
P = BoxModel([1, 2, 3, 4], size = 2)

X = RV(P, sum)

x = X.sim(10000)

```



We can approximate the long run average value of $X$ by computing the average --- a.k.a., *mean* --- of the 10000 simulated values in the usual way: sum the 10000 simulated values stored in `x` and divide by 10000. Here are a few ways of computing the mean of the simulated values.

```{python}

x.sum() / 10000

```

```{python}

x.sum() / x.count()

```


```{python}

x.mean()

```

Recall that a probability is a theoretical long run relative frequency.
A probability can be approximated by a relative frequency from a large number of simulated repetitions, but there is some simulation margin of error.

Likewise, the average value of $X$  after a large number of simulated repetitions is only an approximation to the theoretical long run average value of $X$, and there is margin of error due to natural variability in the simulation.
The margin of error is also on the order of $1/\sqrt{N}$ where $N$ is the number of independently simulated values used to compute the average.
However, the degree of variability of the random variable itself also influences the margin of error when approximating long run averages.
In particular, if $\sigma$ is the standard deviation of the random variable, then the margin of error for the average is on the order of $\sigma / \sqrt{N}$.


Remember that the long run average value is just one feature of a marginal distribution.
There is much more to the long run pattern of variability of a random variable that just its average value.
We are also interested in percentiles, degree of variability, and quantities that measure relationships between random variables.
Two random variables can have the same long run average value but very different distributions.
For example, the [average temperature in both Phoenix, AZ and Miami, FL is around 75 degrees F](https://en.wikipedia.org/wiki/List_of_cities_by_average_temperature#North_America), but the distribution of temperatures is not the same.

Next we introduce a few useful properties of averages.

### Linearity of averages


```{example dice-sim-tactile-ev-linearity}

Recall your tactile simulation from Example \@ref(exm:dice-sim-tactile).
Let $U_1$ be the result of the first roll, and $U_2$ the result of the second, so the sum is $X = U_1 + U_2$.

```

1. Donny Don't says: "$X=U_1+U_2$, so I can find the average value of $X$ by finding the average value of $U_1$, the average value of $U_2$, and adding the two averages".  Do you agree? Explain.
1. Donny Don't says: "$U_1$ and $U_2$ have the same distribution, so they have the same average value, so I can find the average value of $X$ by multiplying the average value of $U_1$ by 2". Do you agree? Explain.
1. Donny Don't says: "$U_1$ and $U_2$ have the same distribution, so $X=U_1+U_2$ has the same distribution as $2U_1 = U_1 + U_1$". Do you agree?  Explain.



```{solution dice-sim-tactile-ev-linearity-sol}

to Example \@ref(exm:dice-sim-tactile-ev-linearity).  


```


<!-- NEED TO FIGURE OUT HOW TO SHOW/HIDE THE STUFF BELOW THAT HAS BOTH R AND TEXT -->

1. Donny is correct! Our simulation results are in Table \@ref(tab:dice-sim-tactile-results-ev).
The average value of $U_1$ is
\[
\frac{`r paste(u1, collapse=" + ")`}{10} = `r round(mean(u1), 3)`
\]
The average value of $U_2$ is
\[
\frac{`r paste(u2, collapse=" + ")`}{10} = `r round(mean(u2), 3)`
\]
The sum of these two values is equal to the average value of $X$.
To see why, suppose we had just performed two repetitions, resulting in the *last* two rows of Table \@ref(tab:dice-sim-tactile-results-ev).
\[
{\scriptsize
\text{Average of $(U_1+U_2)$} = \frac{(1 + 2) + (3 + 4)}{2} = 5 = 2 + 3= \left(\frac{1 + 3}{2}\right)+\left(\frac{2 + 4}{2}\right) = (\text{Average of $U_1$}) + (\text{Average of $U_1$}) 
}
\]
We discuss further below.
1. Donny is correct that $U_1$ and $U_2$ have the same distribution, and he has some good ideas about averages.
But we should remind Donny that a distribution represents the *long run* pattern of variability.
With only 10 repetitions, the results for $U_1$ will not necessarily follow the same pattern as those for $U_2$.
In our simulation, the average value of $U_1$ is `r mean(u1)` and the average value of $U_2$ is `r mean(u2)`.
Multiplying neither one of these numbers by 2 yields the average value of $X$.
    Donny would have been correct if he were talking about *long run* average values. Since $U_1$ and $U_2$ have the same distribution, the long run average value of $U_1$ is equal to the long run average value of $U_2$, and so the long run average value of $X$ is equal to the long run average value of $U_1$ multiplied by two.
1. Donny is not correct.  In particular, $X$ and $2U_1$ do not have the same possible values; for example, $X$ can be 3 but $2U_1$ cannot.
The long run average value is just one feature of a distribution.
Just because $X$ and $2U_1$ have the same long run average value does not necessarily mean they have the same full long run pattern of variability.
In particular, relationships between random variables will affect distributions of transformations of them.
$U_1$ and $U_2$ have the same marginal distribution, but the joint distribution of $(U_1, U_2)$ is not the same as that of $(U_1, U_1)$, and so the distribution of $U_1+U_2$ is not the same as that of $U_1+U_1$.



In general the order of transforming and averaging is not interchangeable.
However, the order is interchangeable for *linear* transformations.
If $X$ and $Y$ are random variables and $a$ and $b$ are non-random constants,
whether in the short run or the long run,
$$
\begin{align*}
\text{Average of $a+bX$} & = a+b(\text{Average of $X$})\\
\text{Average of $X+Y$} & = \text{Average of $X$} +\text{Average of $Y$}
\end{align*}
$$
These properties are referred to as **linearity of averages.**
Averaging involves adding and dividing.
Linear transformations involve only adding/subtracting and multiplying/dividing.
The ability to interchange the order of averaging and *linear* transformations follows simply from basic properties of arithmetic (commutative, associative, distributive).

Note that the average of the sum of $X$ and $Y$ is the sum of the average of $X$ and the average of $Y$ *regardless of the relationship between $X$ and $Y$.*
We will explore this idea in more detail later.



### Averages of indicator random variables

Recall that indicators are the bridge between events and random variables.
Indicators are also the bridge between relative frequencies and averages.


```{example dice-sim-tactile-ev-indicator}

Recall your tactile simulation from Example \@ref(exm:dice-sim-tactile).
Let $A$ be the event that the first roll is 3 and $\ind_A$ the corresponding indicator random variable.
Based only on the results of your simulation, approximate the long run average value of each of $\ind_A$.
What do you notice?

```



```{solution dice-sim-tactile-ev-indicator-sol}

to Example \@ref(exm:dice-sim-tactile-ev-indicator).  


```

```{r, echo = FALSE}

indA = as.numeric(u1 == 3)

```

<!-- NEED TO FIGURE OUT HOW TO SHOW/HIDE THE STUFF BELOW THAT HAS BOTH R AND TEXT -->

Our simulation results are in Table \@ref(tab:dice-sim-tactile-results-ev).
Approximate the long run average value of $\ind_A$ by summing the 10 simulated values of $\ind_A$ and dividing by 10.
\[
\frac{`r paste(indA, collapse=" + ")`}{10} = \frac{`r sum(indA)`}{10}
\]
The average of $\ind_A$ is the relative frequency of event $A$!
When we sum the 1/0 values of $\ind_A$ we count the repetitions on which $A$ occurs.
That is, the numerator in the average calculation for $\ind_A$ is the frequency of event $A$, and dividing by the number of repetitions yields the relative frequency of event $A$. 



If $\ind_A$ is the indicator random variable of an event $A$, whether in the short run or the long run,
$$
\begin{align*}
\text{Average of $\ind_A$} & = \text{Relative frequency of $A$}
\end{align*}
$$

## Standard deviation {#sd}


The long run average value is just one feature of a distribution.
Random variables vary, and the distribution describes the entire pattern of variability.
We can summarize this degree of variability by listing some percentiles (10th, 25th, 50th, 75th, 90th, etc); the more percentiles we provide the clearer the picture, but the less of a summary.
It is also convenient to measure overall degree of variability in a single number.

Consider again a random variable $X$ that follows a Normal(30, 10) distribution (like Regina's arrival time in one case of the meeting problem).

```{python}
X = RV(Normal(30, 10))

x = X.sim(10000)
x
```


```{python, eval = FALSE}
x.plot()
Normal(30, 10).plot() # plot the density curve

```

```{python, echo = FALSE}

plt.figure()
x.plot()
Normal(30, 10).plot() # plot the density curve
plt.show()

```

```{python}
x.mean()
```


The simulation results suggest that 30 is the balance point of the distribution, and 30 seems like a reasonable value of the long run average based on average of the simulated values.
It is.
Any Normal distribution has two parameters, one of which is the mean, a.k.a. long run average value.
A Normal(30, 10) distribution is a Normal distribution with mean 30 and standard deviation 10.

Standard deviation is a measure of overall degree of variability.  While the long run average of $X$ is 30, the values vary about that average.
Many values are close to the average, but some are farther away.  The standard deviation measures, roughly, the average distance of the values from their mean.  Calling `x.sd()` will compute the standard deviation of the simulated values in `x`; we see that it's about 10.

```{python}

x.sd()

```

Roughly, standard deviation measures the average distance from the mean. First, for each simulated value compute its absolute distance from the mean.

```{python}

abs(x - x.mean())

```

Then average these distances.

```{python}

abs(x - x.mean()).mean()

```


Unfortunately, the above calculation yields roughly 8 rather than the value of roughly 10 that `x.sd()` returns.
The above calculation illustrates the concept of standard deviation as average distance from the mean, but the actual calculation of standard deviation is a little more complicated.
Technically, you must first *square* all the distances and  then average; the result is the *variance*.
The **standard** deviation is then  the *square root of the variance*.


\begin{align*}
\text{Variance of } X & = \text{Average of } [(X - \text{Average of X})^2]\\
\text{Standard deviation of } X & = \sqrt{\text{Variance of } X}
\end{align*}


The standard deviation is measured in the measurement units of the random variable.
For example, if the random variable is measured in inches, then standard deviation is also measured in inches, while variance is measured in square-inches.
We will see the theory of variance and standard deviation later.

The following code shows the "long way" of computing standard deviation.
First, find the squared distance between each simulated value and the mean.

```{python}

(x - x.mean()) ** 2 

```

Then average the squared distances to obtain  the variance.
The average is computed in the usual way: sum all the values and divide by the number of values^[If you have some familiarity with statistics, you might have seen a formula for standard deviation that includes dividing by *one less* than the number values ($n-1$). Dividing by $n$ or $n-1$ could make a difference in a small sample of data.  However, we will always be interested in *long run* averages, and it typically won't make any practical difference whether we divide by say 10000 or 9999.].
But now each value included in the average is the squared deviation of `x` from the mean, rather than the value of `x` itself.

```{python}

((x - x.mean()) ** 2).mean()

```

Compare with

```{python}
x.var()
```

Now take the square root to get the standard deviation.
The value is the same as the one returned by `x.sd()`.

```{python}

sqrt(((x - x.mean()) ** 2).mean())

```


```{example normal-uniform-sd-compare}
We'll compare long run average and standard deviation for the Uniform(0, 60) distribution and the Normal(30, 10) distribution.

```

1. Make an educated guess for the long run average value of a Uniform(0, 60) distribution.
1. Will the standard deviation for a Uniform(0, 60) distribution be greater than, less than, or equal to 10, the standard deviation for a Normal(30, 10) distribution? Explain without doing any calculations.
1. Make an educated guess for the standard deviation of a Uniform(0, 60) distribution.

```{solution normal-uniform-sd-compare-sol}

to Example \@ref(exm:normal-uniform-sd-compare).  

```

```{asis fold.chunk = TRUE}

1. It seems reasonable that the long run average value of a Uniform(0, 60) distribution is 30, the balance point of the distribution.
1. While the Uniform(0, 60) and Normal(30, 10) distributions have the same mean of 30, the Uniform(0, 60) has a larger standard deviation than the Normal(30, 10) distribution.  In comparison to a Normal(30, 10) distribution, a Uniform(0, 60) distribution will give higher probability to ranges of values near the extremes of 0 and 60, as well as lower probability to ranges of values near 30.  Thus, there will be more values far from the mean of 30 and fewer values close, and so the average distance from the mean and hence standard deviation will be larger for the Uniform(0, 60) distribution than for the Normal(30, 10) distribution. So the standard deviation of a Uniform(0, 60) distribution will be greater than 10.
1. In a Uniform(0, 60) distribution, values are "evenly spread" from 0 to 60, so distances from the mean are "evenly spread" from 0 (for 30) to 30 (for 0 and 60).  We might expect the standard deviation --- that is, the average distance from the mean --- to be about 15, halfway between 0 and 30.
It turns out that the standard deviation is about 17. 
While the "average distance" interpretation helps our conceptual understanding of standard deviation, the process of squaring the distances, then averaging, and then taking the square root makes guessing the actual value of standard deviation difficult.

```

```{python, eval = FALSE}

RV(Normal(30, 10)).sim(10000).plot() # plot simulated values
Normal(0, 30).plot() # plot the theoretical density

RV(Uniform(0, 60)).sim(10000).plot() # plot simulated values
Uniform(0, 60).plot() # plot the theoretical density

```


```{python, echo = FALSE}
plt.figure()

RV(Normal(30, 10)).sim(10000).plot() # plot simulated values
Normal(30, 10).plot() # plot the theoretical density

RV(Uniform(0, 60)).sim(10000).plot() # plot simulated values
Uniform(0, 60).plot() # plot the theoretical density

plt.show()

```


```{python}
RV(Uniform(0, 60)).sim(10000).sd()
```



### Standardization

Standard deviation provides a "ruler" by which we can judge a particular realized value of a random variable relative to the distribution of values.
This idea is particularly useful when comparing random variables with different measurement units but whose distributions have similar shapes.

```{example sat-z-score}
SAT scores have, approximately, a Normal distribution with a mean of 1050 and a standard deviation of 200.
ACT scores have, approximately, a Normal distribution with a mean of 21 and a standard deviation of 5.5.
Darius's score on the SAT is 1500.  Alfred's score on the ACT is 31.
Who scored relatively better on their test?

```

1. Compute the deviation from the mean for Darius's SAT score.  How does this compare to the average deviation from the mean for SAT scores?
1. Compute the deviation from the mean for Alfred's ACT score.  How does this compare to the average deviation from the mean for ACT scores?
1. Who scored relatively better on their test?

```{solution sat-z-score-sol}
to Example \@ref(exm:sat-z-score)
```

```{asis, fold.chunk = TRUE}

1. Darius's score is $1500-1050 = 450$ points above the mean SAT score.  The average deviation of SAT scores from the mean is 200 points.  So the deviation of Darius's score from the mean is $450/200 = 2.25$ times larger the average deviation.  
1. Alfred's score is $31-21 = 10$ points above the mean ACT score.  The average deviation of ACT scores from the mean is 5.5 points.  So the deviation of Alfred's score from the mean is $10/5.5 = 1.82$ times larger than the average deviation.
1. Both scores are above average, but Darius's is farther above average than Alfred's.  Both distributions are Normal, so the probability that an SAT score is greater than Darius's is smaller than the probability that an ACT score is greater than Alfred's. That is, Darius scored relatively better. See Figure \@ref(fig:sat-z-score-plot).

```

(ref:cap-sat-z-score-plot) Comparison of the Normal distributions in Example \@ref(exm:sat-z-score). The blue mark indicates Darius's SAT score and the orange mark indicates Alfred's ACT score.


```{r, sat-z-score-plot, echo=FALSE, fig.cap="(ref:cap-sat-z-score-plot)", fig.width=8}


par(mfrow=c(2, 1))

za = seq(-3, 3, 0.5)
z = seq(-3, 3, 0.001)

x1a = 1050 + 200 * za 

plot(z, dnorm(z), xlim=range(z), ylim=c(0,dnorm(0)),
     type = "l",
      yaxt='n', xaxt='n', ylab = "", xlab = "", lwd = 2,
      main = "SAT scores: Normal distribution with mean 1050 and SD 200") 
axis(1, at = za, labels = x1a, line=0.5,cex.axis=1.0)
axis(1, at = za, labels = za, line=2.7,cex.axis=1.0)
# axis(1, at = xa, labels = seq(-4, 4, 1), line=2.7,cex.axis=1.0)
mtext("SDs", cex = 0.80, side = 1, line = 3.5, at = -3.5)
mtext("Score", cex = 0.80, side = 1, line = 1.3, at= -3.5)
mtext(c("|"), side=1, line=0, at=c((1500 - 1050) / 200), cex = 2, col = "blue")

x2a = 21 + 5.5 * za 

plot(z, dnorm(z), xlim=range(z), ylim=c(0,dnorm(0)),
     type = "l",
      yaxt='n', xaxt='n', ylab = "", xlab = "", lwd = 2,
      main = "ACT scores: Normal distribution with mean 21 and SD 5.5") 
axis(1, at = za, labels = x2a, line=0.5,cex.axis=1.0)
axis(1, at = za, labels = za, line=2.7,cex.axis=1.0)
# axis(1, at = xa, labels = seq(-4, 4, 1), line=2.7,cex.axis=1.0)
mtext("SDs", cex = 0.80, side = 1, line = 3.5, at = -3.5)
mtext("Score", cex = 0.80, side = 1, line = 1.3, at= -3.5)
mtext(c("|"), side=1, line=0, at=c((31 - 21) / 5.5), cex = 2, col = "orange")

```


Standard deviation provides a "ruler" by which we can judge a particular realized value of a random variable relative to the distribution of values.  Consider the plot for SAT scores in Figure \@ref(fig:sat-z-score-plot). There are two scales on the variable axis: one representing the actual measurement units, and one representing "standardized units".  In the standardized scale, values are measured in terms of standard deviations away from the mean:

- The mean corresponds to a value of 0.
- A one unit increment on the standardized scale corresponds to an increment equal to the standard deviation in the measurement unit scale.

For example, each one unit increment in the standardized scale corresponds to a 200 point increment in the measurement unit scale for SAT scores, and a 5.5 point increment in the measurement unit scale for ACT scores.  An SAT score of 1250 is "1 standard deviation above the mean"; an ACT score of 10 is "2 standard deviations below the mean". Given a specific distribution, the more standard deviations a particular value is away from its mean, the more extreme or "unusual" it is.

The **standardized value** is

$$
\text{Standardized value} = \frac{\text{Value - Mean}}{\text{Standard deviation}}
$$

Standardization --- that is, subtracting the mean and dividing by the standard deviation --- is a linear rescaling.  Therefore, the shape of the distribution of the standardized random variable will be the same as the shape of the distribution of the original random variable. However, the possible values will be different, and so will the mean and standard deviation. Regardless of the original measurement units, a standardized random variable has mean 0 and standard deviation 1.


Standardization is useful when comparing random variables with different measurement units but whose distributions have similar shapes.  However, standardized values are only based on two features of a distribution --- mean and standard deviation --- rather than the complete pattern of variability.  Distributions with different shapes have different patterns of variability. Therefore, when comparing distributions with different shapes, it is better to compare percentiles rather than standardized values to determine what is "extreme" or "unusual".


Any random variable can be standardized, but keep in mind that just how extreme any particular standardized value is depends on the shape of the distribution.
Standardization is most natural for random variables that follow a Normal distribution.

### Normal distributions and the empirical rule {#sec-empirical-rule}


Any Normal distribution follows the "empirical rule" which determines the percentiles that give a Normal distribution its particular bell shape.
For example, for any Normal distribution the 84th percentile is about 1 standard deviation above the mean, the 16th percentile is about 1 standard deviation below the mean, and about 68% of values lie within 1 standard deviation of the mean.
The table below lists some percentiles of a Normal distribution.

```{r, echo = FALSE}

p = c(0.001, 0.005, 0.01, 0.025, 0.10, pnorm(-1), 0.25, pnorm(-0.5), 0.5, pnorm(0.5), 0.75, pnorm(1), 0.90, 0.975, 0.99, 0.995, 0.999)

z = round(qnorm(p), 2)

data.frame(p = paste(round(100 * p, 1), "%", sep = ""),
           z = paste(abs(z), "SDs", if_else(z<0, "below", "above"), "the mean")) %>%
  kbl(col.names = c("Percentile", "SDs away from the mean"),
      caption = "Empirical rule for Normal distributions",
      align = c('r', 'l')) %>%
  kable_styling(fixed_thead = TRUE)

```

The table only lists select percentiles. An even more compact version: For a Normal distribution

- 68% of values are within 1 standard deviation of the mean
- 95% of values are within 2 standard deviations of the mean
- 99.7% of values are within 3 standard deviations of the mean


```{r normal-empirical-rule, echo=FALSE}

knitr::include_graphics(c("_graphics/normal_empirical.png"))

```

The "standard" Normal distribution is a Normal(0, 1) distribution, with a mean 0 and a standard deviation of 1. Notice how the empirical rule corresponds to the standard Normal spinner below.


(ref:cap-standard-normal-spinner) A "standard" Normal(0, 1) spinner. The same spinner is displayed on both sides, with different features highlighted on the left and right. Only selected rounded values are displayed, but in the idealized model the spinner is infinitely precise so that any real number is a possible outcome. Notice that the values on the axis are *not* evenly spaced.

```{r standard-normal-spinner, echo=FALSE, fig.cap="(ref:cap-standard-normal-spinner)", fig.align='center', fig.show="hold", out.width="50%"}

n = 16

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner1 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("", round(0 + 1 * qnorm(xp$x[-1]), 2))) +
  theme(axis.text.x=element_text(size=12, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
  ggtitle(paste("Standard Normal(0, 1) model", sep=""))

spinner1


x = c("","<-2",-1, 0, 1, ">2")
p = c(pnorm(-2), pnorm(-1:2) - pnorm(-2:1), 1-pnorm(2))

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))



plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner2 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white", linetype=1) + 
  # coord_polar("y", start=0) +
  coord_curvedpolar("y", start = 0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = cdf[-c(1, length(cdf))], labels=c(-2, -1, 0, 1, 2)) +
  # theme(axis.text.x=element_text(angle=c(90-180/50*(0:49), -90-180/50*(50:99)), size=8)) +
  theme(axis.text.x=element_text(angle = 0, size=12, face="bold")) +
  annotate(geom="segment", y=(0:19)/20+0.000, yend = (0:19)/20+0.000,
           x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
    geom_text(aes(y = plotp,
                label = percent(p, accuracy = 0.1)), size=4, color=c("white",rep("black",4), rep("white",1))) +
  ggtitle(paste("Standard Normal(0, 1) model", sep=""))

spinner2
```


## Joint distributions


Most interesting problems involve two or more^[We mostly focus on the case of two random variables, but analogous definitions and concepts apply for more than two (though the notation can get a bit messier).] random variables defined on the same probability space.
In these situations, we can consider how the variables vary together, or jointly, and study their relationship.
The *joint distribution* of random variables $X$ and $Y$ (defined on the same probability space) is a probability distribution on $(x, y)$ *pairs*.  (Remember that he distribution of one of the variables alone is a "marginal distribution".)
Think of a joint distribution as being represented by a spinner that returns *pairs* of values.



### Joint distributions of two discrete random variables



```{example dice-joint-dist}

Roll a four-sided die twice; recall the sample space in Example \@ref(exm:dice-rv) and Table \@ref(tab:dice-rv-sol-table). One choice of probability measure corresponds to assuming that the die is fair and that the 16 possible outcomes are equally likely.  Let $X$ be the sum of the two dice, and let $Y$ be the larger of the two rolls (or the common value if both rolls are the same).

```




1. Construct a "flat" table displaying the distribution of $(X, Y)$ pairs, with one pair in each row.
1. Construct a two-way displaying the joint distribution on $X$ and $Y$.
1. Sketch a plot depicting the joint distribution of $X$ and $Y$.
1. Starting with the two-way table, how could you obtain the marginal distribution of $X$? of $Y$?
1. Starting with the marginal distribution of $X$ and the marginal distribution of $Y$, could you necessarily construct the two-way table of the joint distribution? Explain.

```{solution dice-joint-dist-sol}

to Example \@ref(exm:dice-joint-dist)

```


```{asis fold.chunk = TRUE}

1. Construct a table with each row corresponding to a possible $(X, Y)$ pair.  Basically, collapse Table \@ref(tab:dice-joint-dist-flat). For example $\IP((X, Y) = (4, 3))=\IP(X = 4, Y=3) = \IP(\{(1, 3), (3, 1)\}) = 2/16$. See Table \@ref(tab:dice-joint-dist-flat22)
1. Just rearrange the table from the previous part. The possible $x$ values will go along one heading, the possible $y$ values along the other, and the interior cells will contain the probability of each $(x, y)$ pair. See Table \@ref(tab:dice-joint-dist-twoway22)
1. See the tile plot in Figure \@ref(fig:dice-joint-dist-tile22).
1. The total row and column correspond to the marginal distributions. For each possible value $x$ of $X$ sum the values in the corresponding row to find $\IP(X=x)$.  For example,
\begin{align*}
\IP(X=4) & = \IP(X=4, Y=1) + \IP(X=4, Y=2) + \IP(X=4, Y=3) + \IP(X=4, Y=4)\\
& = 0 + 1/16 + 2/16 + 0=3/16.
\end{align*}
Similarly, for each possible value $y$ of $Y$ sum the values in the corresponding column to find $\IP(Y = y)$.
1. No, you could not construct the two-way table of the distribution of $(X, Y)$ pairs based on the distributions of $X$ and $Y$ alone.  Essentially, just because you know the row totals and column totals doesn't necessarily mean you know the values of the interior cells.


``` 



```{r, dice-joint-dist-flat22, echo = FALSE}
x = c(2, 3, 4, 4, 5, 5, 6, 6, 7, 8)
y = c(1, 2, 2, 3, 3, 4, 3, 4, 4, 4)
p = c(1, 2, 1, 2, 2, 2, 1, 2, 2, 1) / 16

kbl(
  data.frame(paste("(", x, ", ", y, ")", sep=""), p),
  col.names = c("(x, y)", "P(X = x, Y = y)"),
  booktabs = TRUE,
  caption = 'Table representing the joint distribution of sum ($X$) and larger ($Y$) of two rolls of a four-sided die',
  digits = 4
) %>%
  kable_styling()

```

Table \@ref(tab:dice-joint-dist-twoway22) reorganizes Table \@ref(tab:dice-joint-dist-flat22) into a two-way table with rows corresponding to possible values of $X$ and columns corresponding to possible values of $Y$.

Table: (\#tab:dice-joint-dist-twoway22) Two-way table representation of the joint distribution of $X$ and $Y$,  the sum and the larger (or common value if a tie) of two rolls of a fair four-sided die.  Possible values $x$ of $X$ are in the leftmost column; possible values $y$ of $Y$ are in the top row. Interior cells contain the probability of each $(x, y)$ pair.

|             |       |       |       |       |
|------------	|-----:	|-----:	|-----:	|-----:	|
| $x$ \\ $y$ 	|    1 	|    2 	|    3 	|    4 	|
| 2          	| 1/16 	|    0 	|    0 	|    0 	|
| 3          	|    0 	| 2/16 	|    0 	|    0 	|
| 4          	|    0 	| 1/16 	| 2/16 	|    0 	|
| 5          	|    0 	|    0 	| 2/16 	| 2/16 	|
| 6          	|    0 	|    0 	| 1/16 	| 2/16 	|
| 7          	|    0 	|    0 	|    0 	| 2/16 	|
| 8          	|    0 	|    0 	|    0 	| 1/16 	|

We could represent the joint distribution with a 3-dimensional impulse plot, with a base displaying $(x, y)$ pairs and spikes with heights representing probabilities. 

```{r, echo = FALSE}

scatterplot3d(x, y, p, pch=16,
  type="h", main="3D Impulse Plot",
  zlab = "Probability",
  zlim = c(0, 0.15),
  xlim = c(2 - 0.000, 8 + 0.000),
  ylim = c(1 - 0.000, 4 + 0.000),
  color = "skyblue")
```


However, 3-d plots can be difficult to visualize. Instead we prefer a tile plot which uses a color scale to represent probability.

(ref:cap-dice-joint-dist-tile2) Tile plot representation of the joint distribution of $X$ and $Y$,  the sum and the larger (or common value if a tie) of two rolls of a fair four-sided die.  Probability is indicated by the color scale.

```{r dice-joint-dist-tile22, echo = FALSE, fig.cap="(ref:cap-dice-joint-dist-tile2)"}

data.frame(x, y, p) %>%
  ggplot(aes(x = x,
              y = y,
              fill = p)) +
  geom_tile(colour = "grey50") +
  geom_text(aes(label = p), col = "white") +
  scale_x_continuous(breaks = 2:8) +
  labs(fill = "Probability") +
  theme_classic()

```




Whichever representation we use, the key is to recognize that the joint distribution of two random variables summarizes the possible *pairs* of values and their relative likelihoods.

In the context of multiple random variables, the distribution of any one of the random variables is called a **marginal distribution**. In Example \@ref(exm:dice-joint-dist), we can obtain the marginal distributions of $X$ and $Y$ from the joint distribution by summing rows and columns.  Think of adding a total column (for $X$) and a total row (for $Y$) in the "margins" of the table.

It is possible to obtain marginal distributions from a joint distribution.  However, in general you cannot recover the joint distribution from the marginal distributions alone. Just because you know the row and column totals doesn't mean you know all the values of the interior cells in the joint distribution table. The joint distribution reflects the relationship between $X$ and $Y$, while the marginal distributions only reflect how each variable behaves in isolation.

Consider the following two-way table representing the joint distribution of two random variables $X$ and $Y$.  You can check that the marginal distributions are the same as in the dice problem.  However, clearly the joint distribution of $X$ and $Y$ is not the same as in Table \@ref(tab:dice-joint-dist-flat).

|             |       |       |       |       |
|------------	|-----:	|-----:	|-----:	|-----:	|
| $x$ \\ $y$ 	|    1 	|    2 	|    3 	|    4 	|
| 2          	|    0 	| 1/16 	|    0 	|    0 	|
| 3          	| 1/16 	| 1/16 	|    0 	|    0 	|
| 4          	|    0 	| 1/16 	| 2/16 	|    0 	|
| 5          	|    0 	|    0 	| 1/16 	| 3/16 	|
| 6          	|    0 	|    0 	| 1/16 	| 2/16 	|
| 7          	|    0 	|    0 	|    0 	| 2/16 	|
| 8          	|    0 	|    0 	| 1/16 	|    0 	|


**In general, marginal distributions alone are not enough to determine a joint distribution.**
(The exception is when random variables are *independent.*)




```{example dice-spinners-ex22}
Continuing the dice rolling example, construct a spinner representing The joint distribution of $X$ and $Y$.
```



```{solution dice-spinners-ex22-sol}
to Example \@ref(exm:dice-spinners-ex22).
```


```{asis, fold.chunk = TRUE}


The spinner in Figure \@ref(fig:dice-spinners-sum-max) represents the joint distribution of $X$ and $Y$.
For example, the spinner returns the pair $(4, 2)$ with probability 1/16 and the pair $(4, 3)$ with probability 2/16.
See Table \@ref(tab:dice-joint-dist-twoway22) or Table \@ref(tab:dice-joint-dist-flat22).
Remember, a joint distribution of two random variables is a distribution of *pairs* on values.

``` 

(ref:cap-dice-spinners-sum-max) Spinner representing the joint distribution of $X$ and $Y$, the sum and the larger of two rolls of a fair four-sided die.

```{r dice-spinners-sum-max, echo=FALSE, fig.cap="(ref:cap-dice-spinners-sum-max)", fig.align='center'}

knitr::include_graphics(c(
  "_graphics/spinner-dice-sum-max.png"))

```
                      



We now have two ways to simulate an $(X, Y)$ pair that has the distribution in Table \@ref(tab:dice-joint-dist-twoway22).

1. Simulate two rolls of a fair four sided die.  Let $X$ be the sum of the two values and let $Y$ be the larger of the two rolls (or the common value if a tie).
1. Spin the spinner in Figure \@ref(fig:dice-spinners-sum-max) once and record the resulting $(X, Y)$ pair.  (Recall that this spinner returns a pair of values.) Of course, this method requires that the joint distribution of $(X, Y)$ is known.


Below is the Symbulate code for simulating directly from the joint distribution in Table \@ref(tab:dice-joint-dist-twoway22).   Note that the tickets in `BoxModel` correspond to the possible $(X, Y)$ pairs, which are not equally likely (even though the 16 pairs of rolls are).  We specify the probability of each ticket by using the `probs` option.  To generate a single $(X, Y)$ pair --- like spinning the spinner in Figure \@ref(fig:dice-spinners-sum-max) once --- we draw one ticket from the box of pairs; this is why^[By default `size = 1`, so we could have omitted it, but we include it for emphasis. Drawing 1 ticket from this box model returns an $(X, Y)$ *pair*.] `size = 1`.

```{python}

xy_pairs = [(2, 1), (3, 2), (4, 2), (4, 3), (5, 3), (5, 4), (6, 3), (6, 4), (7, 4), (8, 4)]
pxy = [1/16, 2/16, 1/16, 2/16, 2/16, 2/16, 1/16, 2/16, 2/16, 1/16]

P = BoxModel(xy_pairs, probs = pxy, size = 1)

P.sim(10)

```

We can now define random variables $X$ and $Y$. An outcome of `P` is a pair of values.  Recall that a Symbulate `RV` is always defined in terms of a probability space and a function `RV(probspace, function)`.  The default function is the identity: $g(\omega) = \omega$.  Therefore, `RV(P)` would just correspond to the pair of values generated by `P`. The sum $X$ corresponds to the first coordinate in the pair and the max $Y$ corresponds to the second.  We can define these random variables in Symbulate by "unpacking" the pair as in the following^[Since `P` returns pairs of outcomes, `Z = RV(P)` is a random *vector*.  Components of a vector can be indexed with brackets `[]`; e.g., the first component is `Z[0]` and the second is `Z[1]`.  (Remember: Python uses zero-based indexing.)  So the "unpacked" code is an equivalent but simpler version of `Z = RV(P); X = Z[0]; Y = Z[1]`.]

```{python}
X, Y = RV(P)
```

Then we can simulate many $(X, Y)$ pairs and summarize as we have done previously.


```{python}
(RV(P) & X & Y).sim(10)

```



```{python, eval = FALSE}

(X & Y).sim(10000).plot('tile')
```

```{python, echo = FALSE}
plt.figure()
(X & Y).sim(10000).plot('tile')
plt.show()
```



<!-- ```{python} -->

<!-- xy = (X & Y).sim(16000) -->

<!-- plt.figure() -->
<!-- xy.plot(['tile', 'marginal']) -->
<!-- plt.show() -->
<!-- xy.tabulate() -->

<!-- ``` -->







<!-- ```{example dd-dice-marginal-sim, name='(ref:ddwddd)'} -->

<!-- Donny says "Forget the joint distribution spinner in Figure \@ref(fig:dice-spinners-sum-max). -->
<!-- I can simulate an $(X, Y)$ pair just by spinning the spinner in Figure \@ref(fig:dice-spinners-sum) to generate $X$ and the one in Figure \@ref(fig:dice-spinners-max) to generate $Y$." -->
<!-- Is Donny correct? -->
<!-- If not, can you help him see why not? -->

<!-- ``` -->

<!-- ```{solution dd-dice-marginal-sim-sol} -->
<!-- to Example \@ref(exm:dd-dice-marginal-sim) -->

<!-- ``` -->


<!-- ```{asis, fold.chunk = TRUE} -->

<!-- Donny is not correct. -->
<!-- Yes, spinning the $X$ spinner in Figure \@ref(fig:dice-spinners-sum) will generate values of $X$ according to the proper marginal distribution, and similarly with Figure \@ref(fig:dice-spinners-max) and $Y$. -->
<!-- However, spinning each of the spinners will *not* produce $(X, Y)$ pairs with the correct *joint* distribution. -->
<!-- For example, Donny's method could produce $X=2$ and $Y=4$, which is not a possible $(X, Y)$ pair. -->
<!-- Donny's method treats the values of $X$ and $Y$ as if they were *independent*; the result of the $X$ spin would not change what could happen with the $Y$ spin (since the spins are physically independent). -->
<!-- However, the $X$ and $Y$ values are related. -->
<!-- For example, if $X=2$ then $Y$ must be 1; if $X=4$ then $Y$ must be 2 or 3; etc. -->
<!-- The joint distribution spinner in Figure \@ref(fig:dice-spinners-sum-max) correctly reflects the relationship between $X$ and $Y$. -->
<!-- But as we discussed in Section \@ref(dist-intro), in general you cannot recover the joint distribution from the marginal distributions, which is what Donny is attempting to do. -->

<!-- ``` -->

<!-- Donny's method corresponds to (1) rolling the die twice and summing to get $X$, then (2) rolling the die two more times and finding the larger roll to get $Y$.  Essentially, Donny is not using the same probability space for $X$ and $Y$, and therefore events involving both random variables cannot be studied.  In Symbulate, Donny's code --- *which would produce an error* --- would look like -->

<!-- ``` -->
<!-- X = RV(BoxModel([1, 2, 3, 4], size = 2), sum) -->
<!-- Y = RV(BoxModel([1, 2, 3, 4], size = 2), max) -->

<!-- (X & Y).sim(10000) -->

<!-- ### Error: Events must be defined on same probability space. -->
<!-- ``` -->

<!-- In Donny's code, his random variables are defined on different probability spaces; one box model is used to generate the rolls for $X$ and a separate box model is used to generate the rolls for $Y$.  As we have mentioned a few times, random variables (and events) must all be defined on the same probability space^[If Donny *really* wanted to simulate two independent pairs of rolls, one to compute $X$ and one to compute $Y$, he would still need define the random variables on the same probability space, using `BoxModel([1, 2, 3, 4], size = 2) ** 2` for which an example outcome would be ((3, 2), (1, 1)).  Then he could define `X=RV(P)[0].apply(sum)` and `Y=RV(P)[1].apply(max)`.  But it's hard to justify why Donny would want to do this.]. -->

```{example dd-dice-joint-sim}

Donny Don't says "Now I see why we need the spinner in Figure \@ref(fig:dice-spinners-sum-max) to simulate $(X, Y)$ pairs.
So then forget the marginal spinners in Figure \@ref(fig:dice-spinners-sum) and Figure \@ref(fig:dice-spinners-max).
If I want to simulate $X$ values, I could just spin the joint distribution spinner in Figure \@ref(fig:dice-spinners-sum-max) and ignore the $Y$ values."
Is Donny's method correct?
If not, can you help him see why not?

```

```{solution dd-dice-joint-sim-sol}
to Example \@ref(exm:dd-dice-joint-sim)

```

```{asis, fold.chunk = TRUE}

Donny is correct!
The joint distribution spinner in Figure \@ref(fig:dice-spinners-sum-max) correctly produces $(X, Y)$ pairs according to the joint distribution in Table \@ref(tab:dice-joint-dist-twoway22).
Ignoring the $Y$ values is like "summing the rows" and only worrying about what happens in total for $X$.
For example, in the long run, 1/16 of spins will generate (4, 2) and 2/16 of spins will generate (4, 3), so ignoring the $y$ values, 3/16 of spins will return an $x$ value of 4.
From the joint distribution  you can always find the marginal distributions (e.g., by finding row and column totals).

Now, Donny's method does work, but it does require more work than necessary.
If we really only needed to simulate $X$ values, we only need the distribution of $X$ and not the joint distribution of $X$ and $Y$, so you could use the $X$ spinner in Figure \@ref(fig:dice-spinners-sum).
But if we wanted to study anything about the relationship between $X$ and $Y$ then we would need the joint distribution spinner.

```



### Joint distributions of two continuous random variables


Consider the meeting problem where Regina ($R$) and Cady ($Y$) are each assumed to arrive according to a Normal(30, 10) distribution, independently of each other.
We can approximate the joint distribution of $R$ and $Y$ by simulating many $(R, Y)$ pairs.

```{python}
R, Y = RV(Normal(30, 10) ** 2)

r_and_y = (R & Y).sim(10000)
```


```{python, eval = FALSE}
r_and_y.plot()
```

```{python, echo = FALSE}

plt.figure()
r_and_y.plot()
plt.show()
```

Unfortunately, a scatterplot does not provide the best visual of the relative frequencies of different regions of $(R, Y)$ pairs.
Instead, we can summarize the simulated values in a 2-dimensional histogram which chops the region of possible values into rectangular "bins" and displays the relative frequency of pairs falling in each bin with a color scale.


```{python, eval = FALSE}

r_and_y.plot('hist')
```

```{python, eval = FALSE, echo = FALSE, warning = FALSE, error = TRUE, message = FALSE}
plt.figure()
r_and_y.plot('hist')
plt.show()
```


```{r, echo=FALSE, fig.align='center', fig.height = 12}

knitr::include_graphics(c(
  "_graphics/meeting-RY-hist.png"))

```

We see that pairs near (30, 30) occur more frequently than those near the "corners".

A 2-d histogram is a flat representation of a 3-d plot; each bin would have a bar with a density height chosen so that the volume of the bar is its relative frequency. If we simulate more and more values, and make the bins smaller and smaller, a smooth "density" surface would emerge. 

```{r, echo = FALSE}
x = seq(0, 60, 1)
y = seq(0, 60, 1)
fxy = function(x, y) dbvn(x, y, waiting_mean, waiting_mean, waiting_sd, waiting_sd, 0)

z = outer(x, y, fxy)

persp(x, y, z, theta = -30, phi = 25, 
      shade = 0.75, col = "skyblue", expand = 0.5, r = 2, 
      ltheta = 25, ticktype = "detailed", 
      xlab = "R",
      ylab = "Y",
      zlab = "Density")
```

Three-dimensional plots are hard to visualize so tend to use heat maps or contour plots to represent joint distributions of two continuous random variables.

```{r meeting-probmeasure22, echo = FALSE, warning = FALSE, message = FALSE}
# | echo: false
# | label: fig-meeting-independent-uniform
# | fig-cap: "add"
# | fig-alt: "add"

ggplot(unit_box_prob,
       aes(x = x,
           y = y, 
           z = independent_normal)) +
  geom_raster(aes(fill = independent_normal), interpolate = TRUE) +
  geom_contour(color = "white") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_fill_viridis(limits = c(0, unit_box_max_prob)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)",
       fill = "Density") +
  coord_fixed() +
  theme_classic()

```

The marginal distribution of a single continuous random variable can be described by a probability density function, for which areas under the curve determine probabilities.
Likewise, the joint distribution of two continuous random variables can be described by a probability density function, for which volumes under the surface determine probabilities.
The "density" height is whatever it needs to be so that volumes under the surface represent appropriate probabilities.

Continuing the meeint problem, let $T=\min(R, Y)$ be the time at which the first person arrives, and let $W=|R-Y|$ be the amount of time the first person to arrive has to wait for the second person to arrive.
We can use simulation to investigate the joint distribution of $T$ and $W$.
We see that there appears to be a *negative association* between $T$ and $W$: larger values of $T$ tend to occur with smaller values of $W$.
We can measure the strength of this association with the *correlation*, which we will investigate further in the next section.

```{python}

W = abs(R - Y)

T = (R & Y).apply(min)

t_and_w = (T & W).sim(10000)
```


```{python, eval = FALSE}
t_and_w.plot('hist')

```



```{r, echo=FALSE, fig.align='center', fig.height = 12}

knitr::include_graphics(c(
  "_graphics/meeting-TW-hist.png"))

```



```{python}

t_and_w.corr()
```


We can obtain marginal distributions from a joint distribution by "stacking"/"collapsing"/"aggregating" out the other variable.
Imagine a joint histogram with blocks of different heights for each $(x, y)$ bin. To find the marginal distribution of $X$ for a particular $x$ interval stack all the bars for the $(x, y)$ bins --- same $x$ but different $y$ --- on top of each other, "collapsing out" the $y$'s.

```{python, eval = FALSE}
t_and_w.plot(['hist', 'marginal'])
```

```{r, echo=FALSE, fig.align='center', fig.height = 12}

knitr::include_graphics(c(
  "_graphics/meeting-TW-hist-marginal.png"))

```

## Correlation {#sec-sim-corr}



Quantities like long run average, variance, and standard deviation summarize characteristics of the *marginal* distribution of a single random variable.
When there are multiple random variables their *joint* distribution is of interest.
Covariance and correlation summarize in a single number a characteristic of the joint distribution of two random variables, namely, the degree to which they "co-deviate from the their respective means".


Covariance is the average of the paired deviations from the respective means.  Let $T$ and $W$ be the first arrival time and waiting time random variables from the meeting problem in the previous section. 
Recall that we have already simulated some $(T, W)$ pairs

```{python}
t_and_w
```

The simulated long run averages are (notice that calling `mean` returns the pair of averages)

```{python}
t_and_w.mean()
```

Now within each simulated $(t, w)$ pair, we subtract the mean of the $T$ values from $t$ and the mean of the $W$ values from $w$,
We can do this subtract in a "vectorized" way: the following code subtracts the first component of `t_and_w.mean()` (the average of the $T$) from the first component of each of the `t_and_w` values (the individual $t$ value), and similarly for the second.

```{python}
t_and_w - t_and_w.mean()
```


For each simulated pair, we now have a pair of deviations from the respective mean.
To measure the interaction between these deviations, we take the product of the deviations within each pair.
First, we split the paired deviations into two separate columns and then take the product within each row

```{python}
t_deviations = (t_and_w - t_and_w.mean())[0] # first column (Python 0)
w_deviations = (t_and_w - t_and_w.mean())[1] # second column (Python 1)

t_deviations * w_deviations

```


Now we take the average of the products of the paired deviations.

```{python}
(t_deviations * w_deviations).mean()
```

This value is the covariance, which we could have arrived at more quickly by calling `cov` on the simulated pairs.

```{python}
t_and_w.cov()

```

The **covariance** of random variables $X$ and $Y$ is defined as the long run average of the product of the paired deviations from the respective means

$$
\text{Covariance($X$, $Y$)} = \text{Average of} ((X - \text{Average of }X)(Y - \text{Average of }Y))
$$
It turns out that covariance is equivalent to the average of the product minus the product of the averages.

$$
\text{Covariance($X$, $Y$)} = \text{Average of} XY  - (\text{Average of }X)(\text{Average of }Y)
$$

```{python}
(t_and_w[0] * t_and_w[1]).mean() - (t_and_w[0].mean()) * (t_and_w[1].mean()) 
```

Unfortunately, the value of covariance is hard to interpret as it depends heavily on the measurement unit scale of the random variables.
In the meeting problem, covariance is measured in squared-minutes. If we converted the $T$ values from minutes to seconds, then the value of covariance would change accordingly and would be measured in (minutes $\times$ seconds).

But the sign of the covariance does have practical meaning.

- A positive covariance indicate an overall *positive association*: above average values of $X$ tend to be associated with above average values of $Y$
- A negative covariance indicates am overall *negative association*: above average values of $X$ tend to be associated with below average values of $Y$
- A covariance of zero indicates that the random variables are  **uncorrelated**: there is no overall positive or negative association. But be careful: if $X$ and $Y$ are  uncorrelated there can still be a relationship between $X$ and $Y$. We will see examples later that demonstrate that being uncorrelated does not necessarily imply that random variables are independent.



```{example, dice-covariance}
Consider the probability space corresponding to two rolls of a fair four-sided die.  Let $X$ be the sum of the two rolls, $Y$ the larger of the two rolls, $W$ the number of rolls equal to 4, and $Z$ the number of rolls equal to 1.
Without doing any calculations, determine if the covariance between each of the following pairs of variables is positive, negative, or zero.
Explain your reasoning conceptually.

``` 


1. $X$ and $Y$
1. $X$ and $W$ 
1. $X$ and $Z$ 
1. $X$ and $V$, where $V = W + Z$.
1. $W$ and $Z$


```{solution dice-covariance-sol}
to Example \@ref(exm:dice-covariance)
```

```{asis, fold.chunk = TRUE}

1. Positive. There is an overall positive association; above average values of $X$ tend to be associated with above average values of $Y$ (e.g., (7, 4), (8, 4)), and below average values of $X$ tend to be associated with below average values of $Y$ (e.g., (2, 1), (3, 2)).
1. Positive.  If $W$ is large (roll many 4s) then $X$ (sum) tends to be large.
1. Negative.  If $Z$ is large (roll many 1s) then $X$ (sum) tends to be small.
1. Zero.. Basically, the positive association between $X$ and $W$ cancels out with the negative association of $X$ and $Z$.  $V$ is large when there are many 1s or many 4s, or some mixture of 1s and 4s.
So knowing that W is large doesn't really tell you anything about the sum.
1. Negative.  There is a fixed number of rolls.  If you roll lots of 4s ($W$ is large) then there must be few rolls of 1s ($Z$ is small).



```




The numerical value of the covariance depends on the measurement units of both variables, so interpreting it can be difficult. Covariance is a measure of joint association between two
random variables that has many nice theoretical properties, but the *correlation
(coefficient)* is often a more practical measure. (We saw a similar idea with variance and standard deviation. Variance has many nice theoretical properties. However, standard
deviation is often a better practical measure of variability.)



  
The correlation for two random variables is the covariance between the corresponding standardized random variables.  Therefore, correlation is a *standardized* measure of the association between two random variables.

Returning to the $(T, W)$ example, we carry out the same process, but first standardize each of the simulated values of $T$ by subtracting their mean and dividing by their standardization, and similarly for $W$.

```{python}
t_and_w.standardize()
```

Now we compute the covariance of the standardized values.

```{python}
t_and_w.standardize().cov()
```

Recall that we already computed the correlation in the previous section.

```{python}
t_and_w.standardize().corr()
```


We see that the correlation is the covariance of the standardized random variables.

$$
\text{Correlation}(X, Y) = \text{Covariance}\left(\frac{X- \text{Average of }X}{\text{Standard Deviation of }X}, \frac{Y- \text{Average of }Y}{\text{Standard Deviation of }Y}\right)
$$


When standardizing, subtracting the means doesn't change the scale of the possible pairs of values; it merely shifts the center of the joint distribution. Therefore, **correlation** is the covariance divided by the product of the standard deviations.

$$
\text{Correlation}(X, Y) = \frac{\text{Covariance}(X, Y)}{(\text{Standard Deviation of }X)(\text{Standard Deviation of }Y)}
$$


A correlation coefficient has no units and is measured on a universal scale. Regardless of the original measurement units of the random variables $X$ and $Y$
\[
-1\le \textrm{Correlation}(X,Y)\le 1
\]

- $\textrm{Correlation}(X,Y) = 1$ if and only if $Y=aX+b$ for some $a>0$
- $\textrm{Correlation}(X,Y) = -1$ if and only if $Y=aX+b$ for some $a<0$


Therefore, correlation is a standardized measure of the strength of the *linear* association between two random variables.
The closer the correlation is to 1 or $-1$, the closer the joint distribution of $(X, Y)$ pairs hugs a straight line, with positive or negative slope.



Because correlation is computed between standardized random variables,  correlation is not affected by a linear rescaling of either variable.  One standard deviation above the mean is one standard deviation above the mean, whether that's measured in feet or inches or meters.



### Joint Normal distributions {#sec-sim-bvn}

Just as Normal distributions are commonly assumed for marginal distributions of individual random variables, joint Normal distributions are often assumed for joint distributions of several random variables. We'll focus "Bivariate Normal" distributions which describe a specific pattern of joint variability for *two* random variables.

```{example dd-bvn-spinner}

Donny Don't has just completed a problem where it was assumed that SAT Math scores follow a Normal(500, 100) distribution. Now a follow up problem asks Donny how he could simulate a single (Math, Reading) pair of scores. Donny says: "That's easy; just spin the Normal(500, 100) twice, once for Math and once for Reading."
Do you agree?
Explain your reasoning.

```


```{solution dd-bvn-spinner-sol}
to Example \@ref(exm:dd-bvn-spinner)
```

```{asis, fold.chunk = TRUE}

You should not agree with Donny, for two reasons.

- It's possible that the distribution of SAT Math scores follow a different pattern than SAT Reading scores.  So we might need one spinner to simulate a Math score, and a second spinner to simulate the Reading score.  (In reality, SAT Math and Reading scores do follow pretty similar distributions.  But it's possible that they could follow different distributions.)
- Furthermore, there is probably some relationship between scores.  It is plausible that students who do well on one test tend to do well on the other.  For example, students who score over 700 on Math are probably more likely to score above than below average on Reading.  If we simulate a pair of scores by spinning one spinner for Math and a separate spinner for Reading, then there will be no relationship between the scores because the spins are physically independent.

```

In the previous problem, hat we really need is a spinner that generates a pair of scores simultaneously to reflect their association.  This is a little harder to visualize, but we could imagine spinning a "globe" with lines of latitude corresponding to SAT Math score and lines of longitutde to SAT Reading score.  But this would not be a typical globe:

- The lines of latitude would not be equally spaced, since SAT Math scores are not equally likely.  (We have seen similar issues for one-dimensional Normal spinners.) Similarly for lines of longitude.
- The scale of the lines of latitude would not necessarily match the scale of the lines of longitude, since Math and Reading scores could follow difference distributions.  For example, the equator (average Math) might be 500 while the prime meridian (average Reading) might be 520.
- The "lines" would be tilted or squiggled to reflect the relationship between the scores.  For example, the region corresponding to Math scores near 700 and Reading scores near 700 would be larger than the region corresponding to Math scores near 700 but Reading scores near 200. 

So we would like a model that

- Simulates Math scores that follow a Normal distribution pattern, with some mean and some standard deviation.
- Simulates Reading scores that follow a Normal distribution pattern, with possibly a different mean and standard deviation.
- Reflects how strongly the scores are associated.

Such a model is called a "Bivariate Normal" distribution.  There are five parameters: the two means, the two standard deviations, and the *correlation* which reflects the strength of the association between the two scores.

We have already encountered a Bivariate Normal model.
Recall that in one case of the meeting problem we assumes that Regina and Cady tend to arrive around the same time.
One way to model pairs of values that have correlation is with a BivariateNormal distribution, like in the following.
We assumed a Bivariate Normal distribution for the $(R, Y)$ pairs, with mean (30, 30), standard deviation (10, 10) and correlation 0.7. (We could have assumed different means and standard deviations.)


```{python, eval = FALSE}
R, Y = RV(BivariateNormal(mean1 = 30, sd1 = 10, mean2 = 30, sd2 = 10, corr = 0.7))

(R & Y).sim(1000).plot()
```

```{python, echo = FALSE}
R, Y = RV(BivariateNormal(mean1 = 30, sd1 = 10, mean2 = 30, sd2 = 10, corr = 0.7))

plt.figure()
(R & Y).sim(1000).plot()
plt.show()
```

Now we see that the points tend to follow the $R = Y$ line, so that Regina and Cady tend to arrive near the same time, similar to Figure \@ref(fig:meeting-probmeasure3).


Recall that in some of the previous examples the shapes of one-dimensional histograms could be approximated with a smooth density curve.   Similarly, a two-dimensional histogram can sometimes be approximated with a smooth density surface.  As with histograms, the height of the density surface at a particular $(X, Y)$ pair of values can be represented by color intensity.  A marginal Normal distribution is a "bell-shaped curve"; a Bivariate Normal distribution is a "mound-shaped" curve --- imagine a pile of sand.  (Symbulate does not yet have the capability to display densities in a three-dimensional-like plot such as [this plot](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#/media/File:Multivariate_Gaussian.png).)

```{r meeting-probmeasure33, echo = FALSE, warning = FALSE, message = FALSE, fig.cap="A Bivariate Normal distribution"}
# | echo: false
# | label: fig-meeting-bivariate-normal
# | fig-cap: "add"
# | fig-alt: "add"

ggplot(unit_box_prob,
       aes(x = x,
           y = y,
           z = bivariate_normal)) +
  geom_raster(aes(fill = bivariate_normal), interpolate = TRUE) +
  geom_contour(color = "white") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_fill_viridis(limits = c(0, unit_box_max_prob)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)",
       fill = "Density") +
  coord_fixed() +
  theme_classic()

```






## Conditional distributions

The *joint distribution* of random variables $X$ and $Y$ (defined on the same probability space) is a probability distribution on $(x, y)$ *pairs*, and describes how the values of $X$ and $Y$ vary together or jointly.
We can also study *conditional distributions* of random variables given the values of some random variables.
How does the distribution of $Y$ change for different values of $X$ (and vice versa)?

### Conditional distributions of discrete random variables



```{example, dice-conditional}
Roll a fair four-sided die twice.  Let $X$ be the sum of the two rolls, and let $Y$ be the larger of the two rolls (or the common value if a tie). We have previously found the joint and marginal distributions of $X$ and $Y$, displayed in the two-way table below.

```

| $(x, y)$         |      |      |      |      |            |
|------------------|-----:|-----:|-----:|-----:|-----------:|
|                  |    1 |    2 |    3 |    4 |      Total |
| 2                | 1/16 |    0 |    0 |    0 |       1/16 |
| 3                |    0 | 2/16 |    0 |    0 |       2/16 |
| 4                |    0 | 1/16 | 2/16 |    0 |       3/16 |
| 5                |    0 |    0 | 2/16 | 2/16 |       4/16 |
| 6                |    0 |    0 | 1/16 | 2/16 |       3/16 |
| 7                |    0 |    0 |    0 | 2/16 |       2/16 |
| 8                |    0 |    0 |    0 | 1/16 |       1/16 |
| Total            | 1/16 | 3/16 | 5/16 | 7/16 |            |


1. Compute $\IP(X=6|Y=4)$.
1. Construct a table, plot, and spinner to represent the conditional distribution of $X$ given $Y=4$.
1. Construct a table, plot, and spinner to represent the conditional distribution of $X$ given $Y=3$.
1. Construct a table, plot, and spinner to represent the conditional distribution of $X$ given $Y=2$.
1. Construct a table, plot, and spinner to represent the conditional distribution of $X$ given $Y=1$.
1. Compute $\IP(Y=4|X=6)$.
1. Construct a table, plot, and spinner to represent the distribution of $Y$ given $X=6$.
1. Construct a table, plot, and spinner to represent the distribution of $Y$ given $X=5$.
1. Construct a table, plot, and spinner to represent the distribution of $Y$ given $X=4$.




```{solution, dice-conditional-sol}
to Example \@ref(exm:dice-conditional)
```

```{asis, fold.chunk = TRUE}


1. Remember that $\{X=6\}$ and $\{Y=4\}$ are events, so we use the definition of conditional probability for events.
    \[
     \IP(X = 6 | Y = 4) =\frac{\IP(X = 6, Y = 4)}{\IP(Y=4)} = \frac{2/16}{7/16} = 2/7 
    \]
1. Slice the column of the joint distribution table corresponding to $Y=4$, and then renormalize.  If $Y = 4$ then $X$ is either 5, 6, 7, or 8; $Y$ is equally likely to be 5, 6, or 7, and each of those values is twice as likely as 8.   The table below displays the conditional distribution of $X$ given $Y=4$. (See further down for plots and spinners.) Note well that this is a distribution of values of $X$.

    | $x$ |   $\IP(X = x|Y = 4)$ |
    |----:|---------------------:|
    |   5 |                  2/7 |
    |   6 |                  2/7 |
    |   7 |                  2/7 |
    |   8 |                  1/7 |
  
1. Slice the column of the joint distribution table corresponding to $Y=3$, and then renormalize. Given $Y=3$, $X$ is equally likely to be either 4 or 5, and each of those values is twice as likely as 6. Note that changing the condition from $\{Y=4\}$ to $\{Y=3\}$ changes the possible values of $X$, along with their probabilities.

    | $x$ |   $\IP(X = x | Y = 3)$ |
    |----:|-----------------------:|
    |   4 |                    2/5 |
    |   5 |                    2/5 |
    |   6 |                    1/5 |
  

1. Slice the column of the joint distribution table corresponding to $Y=3$, and then renormalize. Given $Y=2$, $X$ is twice as likely to be 3 than 4.

    | $x$ |   $\IP(X = x | Y = 2)$ |
    |----:|-----------------------:|
    |   3 |                    2/3 |
    |   4 |                    1/3 |

1. Given $Y=1$, $X$ is equal to 2 with probability 1: $\IP(X = 2 | Y = 1)=1$. 


1. Remember that $\IP(X = 6 | Y = 4)$ and $\IP(Y = 4 | X = 6)$ measure different probabilities.
    \[
     \IP(Y = 4 | X = 6) =\frac{\IP(X = 6, Y = 4)}{\IP(X=6)} = \frac{2/16}{3/16} = 2/3 
    \]
1. Slice the row of the joint distribution table corresponding to $X=6$, and then renormalize.  If $X = 6$ then $Y$ is either 3 or 4, and $Y$ is twice as likely to be 4 than 3. The table below represents the conditional distribution of $Y$ given $X = 6$. Note that this is a distribution of $Y$ values.

    | $y$ |   $\IP(Y = y | X = 6)$ |
    |----:|-----------------------:|
    |   3 |                    1/3 |
    |   4 |                    2/3 |
  
1. Slice the row of the joint distribution table corresponding to $X=5$, and then renormalize. If $X=5$ then $Y$ is equally likely to be 3 or 4.

    | $y$ |   $\IP(Y = y | X = 5)$ |
    |----:|-----------------------:|
    |   3 |                    1/2 |
    |   4 |                    1/2 |
  
1. Slice the row of the joint distribution table corresponding to $X=4$, and then renormalize.  If $X=4$ then $Y$ is twice as likely to be 3 than 2. Note that changing the condition from $\{X=5\}$ to $\{X=4\}$ changes the possible values of $Y$, along with their probabilities.

    | $y$ |   $\IP(Y = y | X = 5)$ |
    |----:|-----------------------:|
    |   2 |                    1/3 |
    |   3 |                    2/3 |
  

```


(ref:cap-dice-conditional-plots) Impulse plots representing the family of conditional distributions of $X$ given $Y$. Each plot represents a conditional distribution of $X$ given $Y=y$ for a particular value of $y= 1, 2, 3, 4$.

```{r dice-conditional-plots, echo = FALSE, fig.cap = "(ref:cap-dice-conditional-plots)", warning = FALSE, message = FALSE}

x = c(2, 3, 4, 4, 5, 6, 5, 6, 7, 8)
y = c(1, 2, 2, 3, 3, 3, 4, 4, 4, 4)
p = c(1, 2 / 3, 1 / 3, 2 / 5, 2 / 5, 1 / 5, 2 / 7, 2 / 7, 2 / 7, 1 / 7)

facet_labels = paste("Conditional distribution of X given Y = ", sort(unique(y)), sep="")

facet_labeller <- function(variable,value){
  return(facet_labels[value])
}

ggplot(data.frame(y, p),
       aes(x = x,
           xend = x,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 2:8, limits = c(2, 8)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "P(X = x | Y = y)") +
  facet_wrap(~y, scales= "free", labeller = facet_labeller )

```

(ref:cap-dice-conditional-spinners) Spinners representing the family of conditional distributions of $X$ given $Y$. Each spinner represents a conditional distribution of $X$ given $Y=y$ for a particular value of $y= 1, 2, 3, 4$.

```{r dice-conditional-spinners, echo = FALSE, fig.show='hold', fig.cap="(ref:cap-dice-conditional-spinners)"}

make_discrete_spinner <- function(x, p, title = ""){
  xp <- data.frame(x, p)
  cdf = c(0, cumsum(xp$p))
  plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  ggplot(xp, aes(x="", y=p, fill=x))+
    geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
    coord_polar("y", start=0) +
    theme_void() +
    # plot the possible values on the outside
    scale_y_continuous(breaks = plotp, labels=xp$x, limits = c(0, 1)) +
    theme(axis.text.x=element_text(size=16, face="bold")) +
    # plot the probabilities as percents inside
    geom_text(aes(y = plotp,
                  label = format(percent(round(p, 3)),0)), size=4) +
    labs(title = title)
}



spinner1 = make_discrete_spinner(2, 1, "Distribution of X given Y = 1")

spinner2 = make_discrete_spinner(3:4, c(2, 1) / 3, "Distribution of X given Y = 2")

spinner3 = make_discrete_spinner(4:6, c(2, 2, 1) / 5, "Distribution of X given Y = 3")

spinner4 = make_discrete_spinner(5:8, c(2, 2, 2, 1) / 7, "Distribution of X given Y = 4")

grid.arrange(spinner1, spinner2, spinner3, spinner4, ncol = 2)

```




(ref:cap-dice-mosaic) Mosaic plots for Example \@ref(exm:dice-conditional), where $X$ is the sum and $Y$ is the max of two rolls of a fair four-sided die. The plot on the left represents conditioning on values of the sum $X$; color represents values of $Y$. The plot on the right represents conditioning on values of the max $Y$; color represents values of $X$.

```{r dice-mosaic, echo=FALSE, fig.cap="(ref:cap-dice-mosaic)", out.width='50%', fig.show='hold'}

knitr::include_graphics(c("_graphics/dice-mosaic1.png", "_graphics/dice-mosaic2.png"))

```

The **conditional distribution of $Y$ given $X=x$** is the distribution of $Y$  values over only those outcomes for which $X=x$.   It is a distribution on values of $Y$ only; treat $x$ as a fixed constant when conditioning on the event $\{X=x\}$.





Conditional distributions can be obtained from a joint distribution by *slicing and renormalizing*. The conditional distribution of $Y$ given $X=x$, where $x$ represents a particular number, can be thought of as:

- the slice of the joint distribution corresponding to $X=x$, a distribution on values of $Y$ alone with $X=x$ fixed
- renormalized so that the slice accounts for 100% of the probability over the values of $Y$ 

The *shape* of the conditional distribution of $Y$ given $X=$ is determined by the shape of the slice of the joint distribution over values of $Y$ for the fixed $x$.

For each fixed $x$, the conditional distribution of $Y$ given $X=x$ is a different distribution on values of the random variable $Y$.  There is not one "conditional distribution of $Y$ given $X$", but rather a *family of conditional distributions of $Y$ given different values of $X$.*

Each conditional distribution is a distribution, so we can summarize its characteristics like mean and standard deviation.
The  conditional mean and standard deviation of $Y$ given $X=x$ represent, respectively, the long run average and variability of values of $Y$ over only $(X, Y)$ pairs with $X=x$.
Since each value of $x$ typically corresponds to a different conditional distribution of $Y$ given $X=x$, the conditional mean and standard deviation will typically be functions of $x$.

```{example dice-conditional-sim}
We have already discussed two ways for simulating an $(X, Y)$ pair in the dice rolling example: simulate a pair of rolls and measure $X$ (sum) and $Y$ (max), or spin the joint distribution spinner for $(X, Y)$ once.

```


1. Now describe another way for simulating an $(X, Y)$ pair using the spinners in Figure \@ref(fig:dice-conditional-spinners). (Hint: you'll need one more spinner in addition to the four from the previous example.)
1. Describe in detail how you can simulate $(X, Y)$ pairs and use the results to approximate $\IP(X = 6 | Y = 4)$.
1. Describe in detail how you can simulate $(X, Y)$ pairs and use the results to approximate the conditional distribution of $X$ given $Y = 4$.
1. Describe in detail how you can simulate values from the conditional distribution of $X$ given $Y=4$ without simulating $(X, Y)$ pairs.
1. We have seen that the long run average value of $X$ is 5.
Would you expect the conditional long run average value of $X$ given $Y= 4$ to be greater than, less than, or equal to 5? 
Explain without doing any calculations. What about given $Y = 2$?
1. How could you use simulation to approximate the conditional long run average value of $X$ given $Y = 4$?


```{solution, dice-conditional-sim-sol}
to Example \@ref(exm:dice-conditional-sim)
```

```{asis, fold.chunk = TRUE}

1. Simulate a value of $Y$ from its marginal distribution (e.g., using the spinner in Figure \@ref(fig:dice-spinners-max).) Then given that $y$ is the simulated value of $Y$, simulate a value of $X$ from the conditional distribution of $X$ given $Y=y$ (e.g., using the appropriate spinner from the family of spinners in Figure \@ref(fig:dice-conditional-spinners)). For example, if $Y=4$, simulate a value of $X$ from a distribution that returns the values 5, 6, 7, 8 with probability 2/7, 2/7,2/7, 1/7.
1. Simulate many $(X, Y)$ pairs (using any appropriate method). Discard any pairs for which $Y\neq 4$. Among the remaining pairs (all with $Y = 4$) count the number of pairs in which $X = 6$, then divide by the total number of pairs with $Y = 4$ to obtain the conditional relative frequency that $X=6$ given $Y=4$. The margin of error is based on the number of simulated repetitions for which $Y=4$.
1. Continue as in the previous part, but find the conditional relative frequency of each possible $X$ value. We condition on the value of $Y$, but we summarize the simulated $X$ values to approximate the conditional distribution of $X$ given the value of $Y$.
1. Construct a spinner according to the conditional distribution of $X$ given $Y=4$ (like in Figure \@ref(fig:dice-conditional-spinners)) and spin it. Of course, this requires us to first find the conditional distribution of $X$ given $Y=4$, but in some problems conditional distributions are provided directly.
1. Unconditionally, $X$ takes values from 2 to 8. Given $Y = 4$, $X$ can only take values 5 to 8. $X$ has a tendency to be larger when $Y=4$ than overall, so we expect the conditional long run average value of $X$ when $Y = 4$ to be greater than 5, the overall average value of $X$.  On the other hand, when $Y=2$, $X$ tends to be smaller than overall, so we expect the conditional long run average value of $X$ given $Y=2$ to be less than 5.
1. Simulate many values of $X$ from the conditional distribution of $X$ given $Y=4$ (using one of the methods described above) and average the simulated values. Notice that we are conditioning on the value of $Y$ but we are averaging the simulated $X$ values.

```


Rather than directly simulating from a joint distribution, we can simulate an $(X, Y)$ pair in two stages:

- Simulate a value of $X$ from its marginal distribution. Call the simulated value $x$.
- Given $x$, simulate a value of $Y$ from the conditional distribution of $Y$ given $X = x$. There will be a different distribution (spinner) for each possible value of $x$.

This "marginal then conditional" process is essentially implementing the multiplication rule
\[
\text{joint} = \text{conditional}\times\text{marginal}
\]

In the dice rolling problem, the "marginal then conditional" method is a lot more trouble than its worth. However, in many problems a joint distribution is described by specifying the marginal distribution of $X$ and the family of conditional distributions of $Y$ given values of $X$, and so the "marginal then conditional" method is the natural way of simulating from a joint distribution.

Recall that we can implement conditioning in Symbulate using `|`.

```{python}

P = DiscreteUniform(1, 4) ** 2

X = RV(P, sum)
Y = RV(P, max)

( (X & Y) | (Y == 4) ).sim(10)

```


```{python}

(X | (Y == 4) ).sim(10000).tabulate()

```

```{python}

(X | (Y == 4) ).sim(10000).tabulate(normalize = True)

```

Below we simulate from the conditional distribution of $X$ given $Y=y$ for each of the values $y = 2, 3, 4$.
Each of the three simulations below approximates a separate conditional distribution of $X$ values.


```{python}

x_given_Y_eq4 = (X | (Y == 4) ).sim(10000)

x_given_Y_eq3 = (X | (Y == 3) ).sim(10000)

x_given_Y_eq2 = (X | (Y == 2) ).sim(10000)

```

We plot the three distributions on a single plot to compare how the distribution of $X$ changes depending on the given value of $Y$.

```{python, eval = FALSE}

x_given_Y_eq4.plot()
x_given_Y_eq3.plot(jitter = True) # shifts the spikes a little
x_given_Y_eq2.plot(jitter = True) # so they don't overlap

```


```{python, echo = FALSE}

plt.figure()
x_given_Y_eq4.plot()
x_given_Y_eq3.plot(jitter = True) # shifts the spikes a little
x_given_Y_eq2.plot(jitter = True) # so they don't overlap
plt.show()

```


Each conditional distribution is a distribution, so we can summarize its characteristics like mean and standard deviation.
Notice how the conditional mean and standard deviation of $X$ depend on the given value of $Y$.

```{python}
x_given_Y_eq4.mean(), x_given_Y_eq4.sd()
```

```{python}
x_given_Y_eq3.mean(), x_given_Y_eq3.sd()
```

```{python}
x_given_Y_eq2.mean(), x_given_Y_eq2.sd()
```


### Conditional distributions of continuous random variables

For two continuous random variables $X$ and $Y$, the conditional distribution of $Y$ given a particular value $x$ of $X$ can be obtained by from the slice of the joint density surface corresponding to $x$.
However, care must be taken when simulating and interpreting conditional distributions given the values of a continuous random variable.

Throughout this section we consider again the case of the meeting problem which assumes that the $(R, Y)$ pairs of arrival times follow a Bivariate Normal distribution with means (30, 30), standard deviations (10, 10) and correlation 0.7.
The joint distribution of $R$ and $Y$ is depicted in Figure \@ref(fig:meeting-probmeasure33).

```{example, meeting-bvn-guess}
Provide answers to the following questions without doing any calculations.
It helps to think of the "long run" here as Regina and Cady meeting for lunch each day over many days.
```

1. Interpret $\IP(Y < 30 | R = 40)$ in context. Is $\IP(Y < 30 | R = 40)$ greater than, less than, or equal to $\IP(Y < 30)$?
1. Interpret $\IP(Y < 30 | R = 15)$ in context. Is $\IP(Y < 30 | R = 15)$ greater than, less than, or equal to $\IP(Y < 30)$?
1. Interpret $\IP(Y < R | R = 40)$ in context. Is $\IP(Y < R | R = 40)$ greater than, less than, or equal to $\IP(Y < R)$?
1. Interpret the conditional long run average value of $Y$ given $R= 40$ in context. Is it greater than, less than, or equal to 30?
1. Interpret the conditional long run average value of $Y$ given $R= 15$ in context. Is it greater than, less than, or equal to 30?
1. Interpret the conditional standard deviation of $Y$ given $R= 40$ in context. Is it greater than, less than, or equal to 10?
1. Interpret the conditional standard deviation of $Y$ given $R= 15$ in context. Is it greater than, less than, or equal to 10?



```{solution, meeting-bvn-guess-sol}
to Example \@ref(exm:meeting-bvn-guess)
```

```{asis, fold.chunk = TRUE}
1. $\IP(Y < 30 | R = 40)$ is the conditional probability that Cady arrives before 12:30 given that Regina arrives at 12:40. Among the days on which Regina arrives at 12:40, what proportion of these days does Cady arrive before 12:30? $\IP(Y < 30 | R = 40)$ is less than $\IP(Y < 30)=0.5$ since if Regina arrives later (after the mean of 12:30) then Cady tends to arrive later too.  So the relative frequency of days on which Cady arrives before 12:30 is less among only days where Regina arrives at 12:40 than among all days. (Look at the slice of the joint distribution corresponding to $R=15$; almost all of the density along this slice lies above $Y < 30$.)
1. $\IP(Y < 30 | R = 15)$ is the conditional probability that Cady arrives before 12:30 given that Regina arrives at 12:15. Among the days on which Regina arrives at 12:15, what proportion of these days does Cady arrive before 12:30? $\IP(Y < 30 | R = 15)$ is greater than $\IP(Y < 30) = 0.5$ since if Regina arrives earlier (before the mean of 12:30) then Cady tends to arrive earlier too.  So the relative frequency of days on which Cady arrives before 12:30 is greater among only days where Regina arrives at 12:15 than among all days. (Look at the slice of the joint distribution corresponding to $R=15$; almost all of the density along this slice lies below $Y < 30$.)
1. $\IP(Y < R | R = 40)$ is the conditional probability that Cady arrives before Regina given that Regina arrives at 12:40. Among the days on which Regina arrives at 12:40, what proportion of these days is Cady the first to arrive? $\IP(Y < R | R = 40)$ is greater than $\IP(Y < R) = 0.5$ since if Regina arrives later (after the mean of 12:30) then Cady has "more room" to arrive early.  (Look at the slice of the joint distribution corresponding to $R=40$; a larger portion of the density along this slice lies below $Y = 40$ than above.)
So the relative frequency of days on which Cady arrives before Regina is greater among only days where Regina arrives at 12:40 than among all days.
1. $\IP(Y < R | R = 15)$ is the conditional probability that Cady arrives before Regina given that Regina arrives at 12:15. Among the days on which Regina arrives at 12:15, what proportion of these days is Cady the first to arrive? $\IP(Y < R | R = 15)$ is less than $\IP(Y < R) = 0.5$ since if Regina arrives earlier (before the mean of 12:30) then Cady has "less room" to arrive early.  (Look at the slice of the joint distribution corresponding to $R=15$; a larger portion of the density along this slice lies above $Y = 15$ than below.)
So the relative frequency of days on which Cady arrives before Regina is smaller among only days where Regina arrives at 12:15 than among all days.
1. The conditional long run average value of $Y$ given $R= 40$ is the average of Cady's arrival times on days where Regina arrives at 12:40. It is greater than 30 --- Cady's average arrival time overall days --- since if Regina arrives later (after the mean of 30) then Cady also tends to arrive later.
1. The conditional long run average value of $Y$ given $R= 15$ is the average of Cady's arrival times on days where Regina arrives at 12:15. It is less than 30 --- Cady's average arrival time overall all days --- since if Regina arrives earlier (before the mean of 30) then Cady also tends to arrive earlier.
1. The conditional standard deviation of $Y$ given $R= 40$ measures the variability of Cady's arrival times on days where Regina arrives at 12:40. It is less than 10 --- the overall variability of Cady's arrival times --- since days on which Regina arrives at 12:40 form a more homogeneous group of days. Compare the overall range of $Y$ values with the range of $Y$ values which accounts for almost all of the density along the $R=40$ slice of the joint distribution.
1. The conditional standard deviation of $Y$ given $R= 15$ measures the variability of Cady's arrival times on days where Regina arrives at 12:15. It is less than 10 --- the overall variability of Cady's arrival times --- since days on which Regina arrives at 12:15 form a more homogeneous group of days. Compare the overall range of $Y$ values with the range of $Y$ values which account for almost all of the density along the $R=15$ slice of the joint distribution.

```


We repeat here some of the ideas we introduced for discrete random variables.
The concepts are analogous for continuous random variables.
The main difference is that continuous random variables are represented by density curves (or surfaces) where areas (or volumes) determine probabilities.

The **conditional distribution of $Y$ given $X=x$** is the distribution of $Y$  values over only those outcomes for which $X=x$.   It is a distribution on values of $Y$ only; treat $x$ as a fixed constant when conditioning on the event $\{X=x\}$.





Conditional distributions can be obtained from a joint distribution by *slicing and renormalizing*. The conditional distribution of $Y$ given $X=x$, where $x$ represents a particular number, can be thought of as:

- the slice of the joint distribution corresponding to $X=x$, a distribution on values of $Y$ alone with $X=x$ fixed
- renormalized so that the slice accounts for 100% of the probability over the values of $Y$. The slice now represents the distribution of a single continuous random variable $Y$. Remember that the distribution of a continuous random variable is represented by a density where *area* determines probability. 

The *shape* of the conditional distribution of $Y$ given $X=$ is determined by the shape of the slice of the joint distribution over values of $Y$ for the fixed $x$.

For each fixed $x$, the conditional distribution of $Y$ given $X=x$ is a different distribution on values of the random variable $Y$.  There is not one "conditional distribution of $Y$ given $X$", but rather a *family of conditional distributions of $Y$ given different values of $X$.*

Each conditional distribution is a distribution, so we can summarize its characteristics like mean and standard deviation.
The  conditional mean and standard deviation of $Y$ given $X=x$ represent, respectively, the long run average and variability of values of $Y$ over only $(X, Y)$ pairs with $X=x$.
Since each value of $x$ typically corresponds to a different conditional distribution of $Y$ given $X=x$, the conditional mean and standard deviation will typically be functions of $x$.


The plot below depicts the joint distribution of $R$ and $Y$ with the slices for $R = 40$ (orange) and $R=15$ (purple) highlighted.


```{r meeting-bvn-conditional, echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE, fig.cap="A Bivariate Normal distribution with two slices highlighted"}
# | echo: false
# | label: fig-meeting-bivariate-normal
# | fig-cap: "add"
# | fig-alt: "add"

ggplot(unit_box_prob,
       aes(x = x,
           y = y,
           z = bivariate_normal)) +
  geom_raster(aes(fill = bivariate_normal), interpolate = TRUE) +
  geom_contour(color = "white") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_fill_viridis(limits = c(0, unit_box_max_prob)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)",
       fill = "Density") +
  coord_fixed() +
  theme_classic() +
  geom_vline(xintercept = c(15, 40), linetype = "solid", 
                 color = c("#CC79A7", "#E69F00"), size = 2, alpha = 0.5)
```


The slices in the previous plot determine the shapes of the conditional distributions of $Y$ given $R=40$ and given $R=15$. 
In the joint distribution, each of the slices corresponds to a certain area.
Now for each slice we rescale the density so that the rescaled area represents 100% of the probability.
The plot below depicts the two conditional distributions. Pay attention to the horizonal axis!
These are distributions over values of $Y$; values of $Y$ are represented on the horizontal axis below.

It is hard to visualize the exact shape of the slices in the joint distribution.
However, from the joint distribution picture we can get an idea of how the conditional mean of $Y$ changes with the values of $R$, and that the conditional standard deviation of $Y$ given any particular value of $R$ is less than the overall standard deviation of $Y$.
From the density color along a vertical slice, we see that density tends to be highest around the mean for the slice (conditional mean of $Y$ given $R=r$) and then decreases relatively symmetrically, suggest at least a symmetric conditional density.
(If turns out that is $X$ and $Y$ have a Bivariate Normal distribution, then the conditional distribution of $Y$ given $X=x$ is Normal, for any $x$.)


```{r meeting-bvn-conditional-slice, echo = FALSE}

ggplot(data = data.frame(u = c(0, 60)),
       mapping = aes(x = u)) +
  stat_function(mapping = aes(colour = "r = 40"),
                fun = dnorm,
                args = list(mean = 30 + 0.7 * (40 - 30),
                            sd = 10 * sqrt(1 - 0.7 ^ 2)),
                size = 1.2) +
  stat_function(mapping = aes(colour = "r = 15"),
                fun = dnorm,
                args = list(mean = 30 + 0.7 * (15 - 30),
                            sd = 10 * sqrt(1 - 0.7 ^ 2)),
                size = 1.2) +
  scale_colour_manual(values = c("#CC79A7", "#E69F00")) +
  scale_x_continuous(breaks = seq(0, 60, 5)) +
  labs(x = "y",
       y = "Density",
       col = "Given value of R",
       title = "Conditional densities of Y given R = r") +
  theme_classic()

```




Many concepts are analogous for discrete and continuous random variables.
However, extra care is required when simulating and interpreting conditional distributions given the values of a continuous random variable.

```{example, uniform-sum-max-conditional-sim-forever}

Donny Don't writes the following Symbulate code to approximate the conditional distribution of $Y$ given $R=40$, the conditional distribution of Cady's arrival time given Regina arrives at 12:40.
What do you think will happen when Donny runs his code?
 
```

```{python, eval = FALSE}
R, Y = RV(BivariateNormal(mean1 = 30, sd1 = 10, mean2 = 30, sd2 = 10, corr = 0.7))

(Y | (R == 40) ).sim(10000)

```







```{solution, uniform-sum-max-conditional-sim-forever-sol}
to Example \@ref(exm:uniform-sum-max-conditional-sim-forever)
```

```{asis, fold.chunk = TRUE}

Donny's code will run forever! Remember, $R$ is a *continuous* random variable, so $\IP(R = 40)=\IP(R = 40.00000000000000000\ldots) = 0$.
The simulation will never return a single value of $(R, Y)$  with $Y$ equal to $40.0000000000000\ldots$, let alone 10000 of them.

```

**Be careful** when conditioning with continuous random variables. Remember that the probability that a continuous random variable is equal to a particular value is 0; that is, for continuous $X$, $\IP(X=x)=0$. Mathematically, when we condition on $\{X=x\}$ we are really conditioning on $\{|X-x|<\ep\}$ --- the event that the random variable $X$ is within $\ep$ of the value $x$ --- and seeing what happens in the idealized limit when $\ep\to0$.
Practically, $\ep$ represents our "close enough" degree of precision, e.g., $\ep=0.01$ if "within 0.01" is close enough.
In the meeting problem, when we say "Regina arrives at 12:40", we really mean something like "Regina arrives within one minute of 12:40".
When conditioning on a continuous random variable $X$ in a simulation, *never* condition on $\{X=x\}$; rather, condition on $\{|X-x|<\ep\}$ where $\ep$ represents the suitable degree of precision.


To avoid conditioning on the event $\{X = x\}$, which has 0 probability, we need to condition on an event like $\{|X-x|<\ep\}$ which often has low probability.
Conditioning on low probability events introduces some computational challenges.
Remember that the margin of error in using a relative frequency to approximate a probability is based on the number of independently simulated repetitions used to compute the relative frequency.
When approximating a conditional probability with a conditional relative frequency, the margin of error is based only on the number of repetitions that satisfy the condition. (Likewise for using simulated averages to approximate long run averages.)
Naively discarding repetitions that do not satisfy the condition can be horribly inefficient.
For example, when conditioning on an event with probability 0.01 we would need to run about 1,000,000 repetition to achieve 10,000 that satisfy the condition.
There are more sophisticated and efficient methods (e.g. "Markov chain Monte Carlo (MCMC)" methods), but they are beyond the scope of this book.
When conditioning on the value of a continuous random variable with `|` in Symbulate, be aware that you might need to change either the number of repetitions (but try not to go below 1000) or the degree of precision $\ep$.

The code below approximates conditioning on the event $\{R = 40\}$ by conditioning instead on the event that $R$ rounded to the nearest minute is 40 , $\{|R - 40| < 0.5\} = \{39.5<R<40.5\}$, an event which has probability 0.04 (since $R$ has a Normal(30, 10) distribution.)


```{python}

R, Y = RV(BivariateNormal(mean1 = 30, sd1 = 10, mean2 = 30, sd2 = 10, corr = 0.7))

( (R & Y) | (abs(R - 40) < 0.5) ).sim(10)

```


Now we approximate the conditional distribution of $Y$ given $R=40$ and some of its features.
Running `(Y | (abs(R - 40) < 0.5) ).sim(1000)` requires about 25000 repetitions in the background to achieve the 1000 the satisfy the condition.
The margin of error for approximating conditional probabilities is roughly $1/\sqrt{1000} = 0.03$.

```{python}

y_given_R_eq40 = ( Y | (abs(R - 40) < 0.5) ).sim(1000)

```

The above simulates many $Y$ values given the condition.
We can visualize the simulated values in a histogram; compare to the orange density in \@ref(fig:meeting-bvn-conditional-slice).

```{python, eval = FALSE}
y_given_R_eq40.plot()
```

```{python, echo = FALSE}
plt.figure()
y_given_R_eq40.plot()
plt.show()
```

Now we approximate some features of the conditional distribution relating to Example \@ref(exm:meeting-bvn-guess).
For example, the simulation suggests that $\IP(Y < 30 | R = 40)$ is less than $\IP(Y < 30) = 0.5$. (But keep in mind that the simulated relative frequencies are really $\pm 0.03$.)
Compare the simulated values below to our responses to Example \@ref(exm:meeting-bvn-guess).

Approximate $\IP(Y < 30 | R = 40)$ is less than $\IP(Y < 30) = 0.5$:

```{python}
y_given_R_eq40.count_lt(30) / y_given_R_eq40.count()
```

Approximate $\IP(Y < R | R = 40)$ is greater than $\IP(Y < R) = 0.5$:

```{python}
y_given_R_eq40.count_lt(40) / y_given_R_eq40.count()
```


Approximate long run average of $Y$ given  $R = 40$ is greater than 30 (overall average value of $Y$):


```{python}
y_given_R_eq40.mean()
```


Approximate standard deviation of $Y$ given  $R = 40$ is less than 10 (overall standard deviation of $Y$):

```{python}
y_given_R_eq40.sd()
```


Now we condition on $R= 15$ and compare.
(We're conditioning on an even lower probability event now.)


```{python}

y_given_R_eq15 = ( Y | (abs(R - 15) < 0.5) ).sim(1000)

```


We can visualize the simulated values in a histogram; compare to the densities in \@ref(fig:meeting-bvn-conditional-slice).
We see that the conditional distribution of $Y$ given $R=r$ changes based on the value of $r$.

```{python, eval = FALSE}
y_given_R_eq40.plot()
y_given_R_eq15.plot()

```

```{python, echo = FALSE}
plt.figure()
y_given_R_eq40.plot()
y_given_R_eq15.plot()
plt.show()
```


Approximate $\IP(Y < 30 | R = 15)$ is greater than $\IP(Y < 30) = 0.5$:

```{python}
y_given_R_eq15.count_lt(30) / y_given_R_eq15.count()
```

Approximate $\IP(Y < R | R = 15)$ is less than $\IP(Y < R) = 0.5$:

```{python}
y_given_R_eq15.count_lt(15) / y_given_R_eq15.count()
```


Approximate long run average of $Y$ given  $R = 15$ is less than 30 (overall average value of $Y$):


```{python}
y_given_R_eq15.mean()
```


Approximate standard deviation of $Y$ given  $R = 15$ is less than 10 (overall standard deviation of $Y$):

```{python}
y_given_R_eq15.sd()
```


In some problems, the joint distribution is naturally specified by describing the marginal distribution of one of the variables, and the family of conditional distributions of the other variable given values of the first.

```{example meeting-conditional-then-marginal}

In the meeting problem, assume that $R$ follows a Normal(30, 10) distribution.
For any value $r$, assume that the conditional distribution of $Y$ given $R=r$ is a Normal distribution with mean $30 + 0.7(r - 30)$ and standard deviation 7.14 minutes.
```




1. How can we simulate a value of $R$ using only the standard Normal spinner from Figure \@ref(fig:standard-normal-spinner)?
1. Suppose the simulated value of $R$ is 40. What is the distribution that we want to simulate the corresponding $Y$ value from?
1. How can we simulate a value of $Y$ from the distribution in the previous part using only the standard Normal spinner?
1. Now suppose the simulated value of $R$ is 15. What is the distribution that we want to simulate the corresponding $Y$ value from?
1. How can we simulate a value of $Y$ from the distribution in the previous part using only the standard Normal spinner?
1. Suggest a general method for simulating an $(R, Y)$ pair.
1. Simulate many $(R, Y)$ pairs and summarize the results, including the correlation. (See code below.)
How does the simulated joint distribution compare to the Bivariate Normal distribution from Example \@ref(exm:meeting-bvn-guess)?
1. What is the approximate marginal distribution of $Y$?


```{solution, meeting-conditional-then-marginal-sol}
to Example \@ref(exm:meeting-conditional-then-marginal)
```


1. The standard Normal spinner simulates standardized values.
For example, if the spinner returns a value of 1, that means 1 standard deviation above the mean, or $30+(1)10=40$ in the meeting problem.
Spin the standard Normal spinner once and let the result be $Z_1$, then set $R = 30 + 10Z_1$.
Then the simulated $R$ values will follow a Normal(30, 10) distribution.
1. The assumed conditional distribution of $Y$ given $R=40$ is Normal with mean $30 + 0.7(40 - 30) = 37$ and standard deviation 7.14 minutes.
So if $R=40$ we want to simulate $Y$ from a Normal(37, 7.14) distribution.
1. Spin the standard Normal spinner again, call the result $Z_2$, and set $Y= 37 + 7.14Z_2$. $Y$ values simulated in this way will follow a Normal(37, 7.14) distribution.
1. The assumed conditional distribution of $Y$ given $R=15$ is Normal with mean $30 + 0.7(15 - 30) = 19.5$ and standard deviation 7.14 minutes.
So if $R=15$ we want to  simulate a value from a Normal(19.5, 7.14) distribution.
1. Spin the standard Normal spinner again, call the result $Z_2$, and set $Y= 19.5 + 7.14Z_2$. $Y$ values simulated in this way will follow a Normal(19.5, 7.14) distribution.
1. Spin the standard Normal spinner twice, and call the results $Z_1, Z_2$. Set $R = 30 + 10Z_1$. Whatever $R$ is, we want the corresponding conditional mean of $Y$ to be $30 + 0.7(R - 30)$, so set $Y = 30 + 0.7(R - 30) + 7.14Z_2$.
1. See the results below. The correlation is about 0.7 and the joint distribution of $R$ and $Y$ looks pretty similar to the Bivariate Normal distribution we had assumed previously.
1. See the simulation results below. The marginal distribution of $Y$ is approximately Normal(30, 10).


```{python}
Z1, Z2 = RV(Normal(0, 1) ** 2)

R = 30 + 10 * Z1

Y = 30 + 0.7 * (R - 30) + 7.14 * Z2

r_and_y = (R & Y).sim(10000)

r_and_y.corr()
```

The correlation is approximately 0.7. The marginal means are both approximately 30.

```{python}
r_and_y.mean()
```


And the marginal standard deviations are both approximately 10.

```{python}
r_and_y.sd()
```


Plots of simulated values look similar to the Bivariate Normal distribution case.

```{python, eval = FALSE}
r_and_y.plot()
```

```{python, echo = FALSE}
plt.figure()
r_and_y.plot()
plt.show()
```


```{python, eval = FALSE}
r_and_y.plot('hist')
```

```{python, echo = FALSE, error = TRUE, warning = FALSE, message = FALSE}
plt.figure()
r_and_y.plot('hist')
plt.show()
```

The marginal distribution of $Y$ is approximately Normal with mean 30 and standard deviation 10.
Notice that while the conditional mean of $Y$ changes with $R$, the overall marginal mean of $Y$ is 30.
Also, while the variability of $Y$ for any given value of $R$ is 7.14, the overall variability of $Y$ is 10.

```{python}
y = r_and_y[1]

y.mean(), y.sd()
```


```{python, eval = FALSE}
y.plot()

```


```{python, echo = FALSE}
plt.figure()
y.plot()
plt.show()
```


The above example provides one method for simulating from a Bivariate Normal distribution using only a standard Normal spinner.
We willl study Bivariate Normal distributions in more detail later.


Be sure to distinguish between joint, conditional, and marginal distributions.

- The joint distribution is a distribution on $(X, Y)$ pairs. A mathematical expression of a joint distribution is a function of both values of $X$ and values of $Y$.
- The conditional distribution of $Y$ given $X=x$ is a distribution on $Y$ values (among $(X, Y)$ pairs with a fixed value of $X=x$). A mathematical expression of a conditional distribution will involve both $x$ and $y$, but $x$ is treated like a fixed constant and $y$ is treated as the variable.  Note: the possible values of $Y$ might depend on the value of $x$.
- The marginal distribution of $Y$ is a distribution on $Y$ values only, regardless of the value of $X$. A mathematical expression of a marginal distribution will have only values of the single variable in it; for example, an expression for the marginal distribution of $Y$ will only have $y$ in it (no $x$, not even in the possible values).

We have focused on the conditional distribution of one random variable $Y$ given the value $x$ of another random variable $X$.
But we can also consider how joint distributions change with conditioning.
For example, the following simulates values of $T$ and $W$ (first time, waiting time) in the meeting problem given $R= 40$.

```{python}
W = abs(R - Y)

T = (R & Y).apply(min)

t_and_w_given_R_eq40 = ( (T & W) | (abs(R - 40) < 0.5) ).sim(1000)

t_and_w_given_R_eq40.corr()
```


```{python, eval = FALSE}
t_and_w_given_R_eq40.plot()

```


```{python, echo = FALSE}
plt.figure()
t_and_w_given_R_eq40.plot()
plt.show()
```


We can also condition on events other than "equals to" events, and events involving multiple random variables.
Below we simulate the joint distribution of $T$ and $W$ given $R = 40$ and $Y < 30$.
(Note that `&` plays a different role on either side of the conditioning `|`.)


```{python}
W = abs(R - Y)

T = (R & Y).apply(min)

t_and_w_given_R_eq40_and_Y_lt30 = ( (T & W) | ( (abs(R - 40) < 0.5) & (Y < 30) ) ).sim(1000)

t_and_w_given_R_eq40_and_Y_lt30.corr()
```


```{python, eval = FALSE}
t_and_w_given_R_eq40_and_Y_lt30.plot()

```


```{python, echo = FALSE}
plt.figure()
t_and_w_given_R_eq40_and_Y_lt30.plot()
plt.show()
```



## A more interesting example: Matching problem {#sim-matching-n}

Dice rolling provides a simple scenario for us to introduce ideas, but it's not very exciting.
Now we'll apply ideas from this section to investigate a more interesting problem: the matching problem.
We'll also see how we can combine Symulate and Python code.

Our version is from [FiveThirtyEight](https://fivethirtyeight.com/features/everythings-mixed-up-can-you-sort-it-all-out/).
A geology museum in California has $n$ different rocks sitting in a row on a shelf, with labels on the shelf telling what type of rock each is.
An earthquake hits and the rocks all fall off the shelf and get mixed up.
A janitor comes in and, wanting to clean the floor, puts the rocks back on the shelf in uniformly random order; each rock is equally likely to be placed in any spot.


Let $Y$ be the number of rocks put in the correct spot (i.e., the number of matches).
We considered the case $n=4$ in Example \@ref(exm:matching-probspace), where we found the distribution of $Y$; see Table \@ref(tab:matching-probspace-table).
When $n=4$ the probability of at least one match is 0.625.

When $n=4$ we could enumerate the $4!=24$ possible outcomes, but such a strategy is not feasible for a general $n$.
For example, if $n=60$ then there are $60! \approx 10^{82}$ possible outcomes, which is about the total number of atoms in the observable universe.
Therefore we need other strategies to investigate the problem.

We'll use simulation to investigate, for general $n$:

- the probability that at least one rock is placed in the correct spot (i.e., the probability of at least one match)
- the long run average number of rocks placed in the correct spot (i.e., the long run average number of matches)
- how these quantities depend on $n$.


Before proceeding, stop to think: what do you expect?
How do you expect the probability of at least one match to depend on $n$?
Will the probability increase, decrease, or stay about the same as $n$ gets larger?
What about the long run average number of matches?
It's always a good idea to think about a problem and make some initial guesses before just jumping into calculations or simulations.

```{example, matching-sim-describe, name='Matching problem'}

For a given $n$, describe in detail how you would use simulation to approximate:
  
1. The distribution of $Y$
1. The probability of at least one match.
1. The long run average number of matches

```


```{solution matching-sim-describe-sol}
to Example \@ref(exm:matching-sim-describe)

```





```{asis, fold.chunk = TRUE}

We could use a box model.

- The box has $n$ tickets, labeled $1, 2, \ldots, n$, one for each rock.
- An outcome is simulated by selecting $n$ tickets from the box *without* replacement and recording their order, e.g., (2, 1, 3, 4) if $n=4$.
- Let $y$ be the number of matches for the simulated outcome, e.g., $y=2$ for outcome (2, 1, 3, 4).
- The above two steps consist of one repetition, yielding one realized value of the random variable $Y$.
- Repeat these steps many times to obtain many simulated values of $Y$.

1. Summarize the simulated values of $Y$ and their relative frequencies to approximate the distribution of $Y$.
1. Count the number of repetitions on which $Y>0$ and divide by the total number of repetitions to approximate $\IP(Y>0)$, the probability of at least one match.
1. Compute the average of the simulated $Y$ values --- sum all simulated $Y$ values and divide by the number of simulated $Y$ values --- to approximate the long run average value of $Y$.

```

We'll start by coding a simulation for $n=4$.
We can compare our simulation results to the analytical results to check that our simulation process is working correctly.
Since Python uses zero-based indexing, we label the rocks $0, 1, \ldots, n-1$.

```{python}

n = 4

labels = list(range(n)) # list of labels [0, ..., n-1]
labels

```


Now we define the box model and simulate a few outcomes.  Note that `replace = False`.

```{python}

P = BoxModel(labels, size = n, replace = False)

P.sim(5)

```

We simulate many outcomes to check that they are roughly equally likely.

```{python}

P.sim(24000).tabulate()

```


Remember that a random variable $Y$ is a function whose inputs are the sample space outcomes.
In this example the function "counts matches", so would like to define $Y$ as `Y = RV(P, count_matches)`.
Unfortunately, such a function isn't built in like `sum` or `max`, but we can write a custom `count_matches` Python function ourselves.
The `count_matches` function below starts a counter at 0 and then goes spot-by-spot through each spot in the outcome and increments the counter by 1 any time there is a match.
Don't worry too much about the Python syntax yet.
What's important is that we have defined a *function* that we can use to define a random variable.


```{python}

def count_matches(omega):
    count = 0
    for i in range(0, n, 1):
        if omega[i] == labels[i]:
            count += 1
    return count
  
omega = (1, 0, 2, 3) # an example outcome, with 2 matches
count_matches(omega) # the function evaluated for the example outcome

```


Now we can use the function `count_matches` to define a Symbulate `RV` just like we have used `sum` or `max`.

```{python}

Y = RV(P, count_matches)

Y((1, 0, 2, 3))

```

We can simulate many values of $Y$ and use the simulated values to approximate the distribution of $Y$, the probability of at least one match, and the long run average value of $Y$.

```{python}

y = Y.sim(10000)

y.tabulate(normalize = True)

```

```{python}

y.count_gt(0) / y.count()

```

```{python}

y.mean()

```

The simulated distribution of $Y$ is close to the true distribution in Table \@ref(tab:matching-probspace-table); in particular, the simulated values are within the margin of error (about 0.01-0.02 for 10000 simulated values) of the true values.
We also see that the long run average value of $Y$ is around 1.


It appears that our simulation is working properly for $n=4$.
To investigate a different value of $n$, we simply need to revise the line `n=4`.
Because we want to investigate many values of $n$, we wrap all the above code in a Python function which takes $n$ as an input and outputs our objects of interest.

```{python}

def matching_sim(n):
    labels = list(range(n))
    def count_matches(omega):
        count = 0
        for i in range(0, n, 1):
            if omega[i] == labels[i]:
                count += 1
        return count
    
    P = BoxModel(labels, size = n, replace = False)
    Y = RV(P, count_matches)
    
    y = Y.sim(10000)
    
    plt.figure()
    y.plot('impulse')
    plt.show()
    
    return y.count_gt(0) / y.count(), y.mean()

```

For example, for $n=4$ we simply call

```{python}

matching_sim(4)

```

Now we can easily investigate different values of $n$. For example, for $n=10$ we see that the probability of at least one match is around 0.63 and the long run average number of matches is around 1.

```{python}

matching_sim(10)

```

We can use a for loop to automate the process of changing the value of $n$, running the simulation, and recording the results.
If `ns` is the list of $n$ values of interest we basically just need to run

```
for n in ns:
    matching_sim(n)
```

In Python we can also use list comprehension

```
[matching_sim(n) for n in ns]
```

The table below summarizes the simulation results for $n=4, \ldots, 10$.
The first line defines the values of $n$, and the second line implements the for loop.
The `tabulate` code just adds a little formatting to the table.
Note that we have temporarily redefined `matching_sim` to remove the lines that produced the plot, but we have not displayed the revised code here.
(We will get bring the plot back soon.)


```{python, echo = FALSE}

def matching_sim(n):
    labels = list(range(n))
    def count_matches(omega):
        count = 0
        for i in range(0, n, 1):
            if omega[i] == labels[i]:
                count += 1
        return count
    
    P = BoxModel(labels, size = n, replace = False)
    Y = RV(P, count_matches)
    
    y = Y.sim(10000)
    
    # plt.figure()
    # y.plot()
    # plt.show()
    
    return y.count_gt(0) / y.count(), y.mean()

```


```{python}

ns = list(range(4, 11, 1))

results = [matching_sim(n) for n in ns]

print(tabulate({'n': ns,
                'P(Y > 0), LRA': results},
               headers = 'keys', floatfmt=".3f"))
               
```

Stop and look at the table; what do you notice?
How do the probability of at least one match and the long run average value depend on $n$?
They don't!
Well, maybe they do, but they don't appear to change very much with $n$ after we take into account simulation margin of error^[The simulation margin of error for a single probability of at least one match is about 0.01 based on 10000 simulated values. We haven't discussed simulation margins of error for long run averages yet. In this case, the simulation margin of error for a single long run average based on 10000 simulated values is about 0.02.] of about 0.01-0.02.
It appears that regardless of the value of $n$, the probability of at least one match is around 0.63 and the long run average number of matches is around 1.

If we're interested in more values of $n$, we just repeat the same process with a longer list of `ns`.
The code below uses [Matplotlib](https://matplotlib.org/) to create a plot of the probability of at least one match and the long run average number of matches versus $n$.
While there is some natural simulation variability, we see that the probability of at least one match (about 0.63) and the long run average value (about 1) basically do not depend on $n$!

```{python, cache = TRUE}

ns = list(range(4, 101, 1))

results = [matching_sim(n) for n in ns]

plt.figure()
plt.plot(ns, results)
plt.legend(['P(Y > 0)', 'LRA'])
plt.xlabel('n')
plt.ylabel('value')
plt.show()

```

What about the distribution of $Y$?
Clicking on the picture below will launch a Colab notebook that contains code for investigating how the distribution of $Y$ depends on $n$.
The basic simulation code is identical to what we have already seen.
The notebook adds a few lines to create a Jupyter widget, which produces an interactive plot (with some additional formatting) of the distribution with a slider; you can change the slider to see how the distribution of $Y$ changes with $n$.
Take a few minutes to play with the slider; what do you see?

<script src="https://gist.github.com/kevindavisross/b6b6adeb13c3b10f1fa82c5261d7a931.js"></script>

You should see that unless $n$ is really small (like 4 or 5) the distribution of $Y$ is basically the same for any value of $n$!
And the distribution appears to follow a distribution called the Poisson(1) distribution.
In particular, the probability of exactly 0 matches is approximately equal to the probability of exactly 1 match.


Summarizing, our simulation investigation of the matching problem reveals that, unless $n$ is really small,

- the probability of at least one match does not depend on $n$, and is approximately 0.632.
- the long run average number of matches is approximately 1
- the distribution of the number of matches is approximately the same for all values of $n$
- the distribution of the number of matches is approximately the Poisson(1) distribution.

We will investigate these observations further later.
For now, marvel at the fact that no matter if there are 10 or 10 thousand or 10 million rocks and spots, there is about a 63% probability that at least one rock will be in the correct spot.
Amazing!

### Summary

- We can use Python code to
    - define functions to define random variables
    - define for loops to investigate changing parameters
    - customize plots produced by the Symbulate `plot` command (add axis labels, legends, etc)
    - summarize results of multiple simulations in tables and Matplotlib plots (e.g., for different values of problem parameters)
- Simulation provides an effective way for investigation probability problems
- Probability problems can have surprising results!


