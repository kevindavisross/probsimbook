# Distributions {#distchap}

<!-- \newcommand{\IP}{\textrm{P}} -->

<!-- \newcommand{\IQ}{\textrm{Q}} -->

<!-- \newcommand{\E}{\textrm{E}} -->

<!-- \newcommand{\Var}{\textrm{Var}} -->

<!-- \newcommand{\SD}{\textrm{SD}} -->

<!-- \newcommand{\Cov}{\textrm{Cov}} -->

<!-- \newcommand{\Corr}{\textrm{Corr}} -->

<!-- \newcommand{\Xbar}{\bar{X}} -->

<!-- \newcommand{\Ybar}{\bar{X}} -->

<!-- \newcommand{\xbar}{\bar{x}} -->

<!-- \newcommand{\ybar}{\bar{y}} -->

<!-- \newcommand{\ind}{\textrm{I}} -->

<!-- \newcommand{\dd}{\text{DDWDDD}} -->

<!-- \newcommand{\epsilon}{\epsilonsilon} -->

<!-- \newcommand{\reals}{\mathbb{R}} -->

Random variables can potentially take many different values, usually with some values or intervals of values more likely than others.
We have used simulation to investigate the pattern of variability of random variables.
Plots and summary statistics like the ones encountered in the previous chapters summarize distributions of random variables.

The **(probability) distribution** of a random variable specifies the possible values of the random variable and a way of determining corresponding probabilities.
We will see several ways to describe distributions, some of which depend on the number and types of the random variables under investigation.

Commonly encountered random variables can be classified as discrete or continuous (or a mixture of the two[^distribution-1]).

[^distribution-1]: There is another type of weird random variable which has a "singular" distribution, like the [Cantor distribution](https://en.wikipedia.org/wiki/Cantor_distribution), but we're counting these random variables as not commonly encountered.

-   A **discrete** random variable can take on only countably many isolated points on a number line. These are often counting type variables. Note that "countably many" includes the case of countably infinite, such as $\{0, 1, 2, \ldots\}$.
-   A **continuous** random variable can take any value within some uncountable interval, such as $[0, 1]$, $[0,\infty)$, or $(-\infty, \infty)$. These are often measurement type variables.

We will see a few ways of specifying a distribution.

-   A well labeled *plot*
-   A *table* of possible values and corresponding probabilities for discrete random variables. This could be a two-way table for the joint distribution of two discrete random variables.
-   A *probability mass function* for discrete random variables or a *probability density function* for continuous random variables which maps possible values $x$ --- or $(x, y)$ pairs, etc --- to their respective probability (for discrete) or density (for continuous).
-   A *cumulative distribution function*, which provides all the percentiles of the distribution
-   By *name, including values of relevant parameters*, e.g., "Exponential(1)", "Normal(500, 100)", "Binomial(5, 0.3)", "BivariateNormal(500, 500, 100, 100, 0.8)". Some probabilistic situations are so common that the corresponding distributions have special names. Always be sure to specify values of relevant parameters, e.g., "Normal(500, 100) distribution" rather than just "Normal distribution". Note that different named distributions have different identifying parameters. For example, the parameters 0 and 1 mean something different for the Uniform(0, 1) distribution than for the Normal(0, 1) distribution.

## Do not confuse a random variable with its distribution {#distribution}

Heed the title.
A random variable measures a numerical quantity which depends on the outcome of a random phenomenon.
The distribution of a random variable specifies the long run pattern of variation of values of the random variable over many repetitions of the underlying random phenomenon.
The distribution of a random variable can be approximated by simulating an outcome of the random process, observing the value of the random variable for that outcome, repeating this process many times, and summarizing the results.
But a random variable is not its distribution.

A distribution is determined by:

-   The underlying probability measure $\IP$, which represents all the assumptions about the random phenomenon.
-   The random variable $X$ itself, that is, the function which maps sample space outcomes to numbers.

Changing either the probability measure or the random variable itself can change the distribution of the random variable.
For example, consider the sample space of two rolls of a fair four-sided die.
In each of the following scenarios, the random variable $X$ has a different distribution.

1.  The die is fair and $X$ is the sum of the two rolls
2.  The die is fair and $X$ is the larger of the two rolls
3.  The die is weighted to land on 1 with probability 0.1 and 4 with probability 0.4 and $X$ is the sum of the two rolls.

In particular, in (1) $X$ takes the value 2 with probability $1/16$, in (2) $X$ takes the value 2 with probability $3/16$, and in (3) $X$ takes the value 2 with probability 0.01.
In (1) and (2), the probability measure is the same (fair die) but the function defining the random variable is different (sum versus max).
In (1) and (3), the function defining the random variable is the same, and the sample space of outcomes is the same, but the probability measure is different.

We often specify the distribution of a random variable directly without explicit mention of the underlying probability space or function defining the random variable.
For example, in the meeting problem we might assume that arrival times follow a Uniform(0, 60) or a Normal(30, 10) distribution.
In situations like this, you can think of the probability space as being the distribution of the random variable and the function defining the random variable to be the identity function.
This idea corresponds to the "simulate from the distribution method": construct a spinner corresponding to the distribution of the random variable and spin it to simulate a value of the random variable.

```{example dd-same-distribution}

Donny Dont is thoroughly confused about the distinction between a random variable and its distribution.  Help him understand by by providing a simple concrete example of *two different random variables* $X$ and $Y$ that have the *same distribution*. Can you think of $X$ and $Y$ for which $\IP(X = Y) = 0$?  How about a discrete example and a continuous example?

```

```{solution dd-same-distribution-sol}
to Example \@ref(exm:dd-same-distribution)

```

```{asis, fold.chunk = TRUE}

Flip a fair coin 3 times and let $X$ be the number of heads and $Y$ be the number of tails.  Then $X$ and $Y$ have the same distribution, because they have the same long run pattern of variability. Each variable takes values 0, 1, 2, 3 with probability 1/8, 3/8, 3/8, 1/8.
But they are not the same random variable; they are measuring different things.
If the outcome is HHT then $X$ is 2 but $Y$ is 1. In this case $\IP(X = Y)=0$; in an odd number of flips it's not possible to have the same number of heads and tails on any single outcome.

In some cases of the meeting time problem we assumed the distribution of Regina's arrival time $R$ is Uniform(0, 60) and the distribution of Cady's arrival time $Y$ is Uniform(0, 60).  So $R$ and $Y$ have the same distribution.
But these are two random variables; one measures Regina's arrival time and one measure Cady's.  If Regina and Cady met every day for a year, then the day-to-day pattern of Regina's arrival times would look like the day-to-day pattern of Cady's arrival times.  But on any given day, their arrival times would not be the same, since $R$ and $Y$ are continuous random variables so $\IP(R = Y) = 0$.



```

A distribution, like a spinner, is a blueprint for simulating values of the random variable.
If two random variables have the same distribution, you could use the same spinner to simulate values of either random variable.
But a distribution is not the random variable itself.
(In other words, "the map is not the territory.")

Two random variables can have the same (long run) distribution, even if the values of the two random variables are never equal on any particular repetition (outcome).
If $X$ and $Y$ have the same distribution, then the spinner used to simulate $X$ values can also be used to simulate $Y$ values; in the long run the patterns would be the same.

At the other extreme, two random variables $X$ and $Y$ are the same random variable only if for every outcome of the random phenomenon the resulting values of $X$ and $Y$ are the same.
That is, $X$ and $Y$ are the same random variable only if they are the same *function*: $X(\omega)=Y(\omega)$ for all $\omega\in\Omega$.
It is possible to have two random variables for which $\IP(X=Y)$ is large, but $X$ and $Y$ have different distributions.

Many commonly encountered distributions have special names.
For example, the distribution of $X$, the number of heads in 3 flips of a fair coin, is called the "Binomial(3, 0.5)" distribution.
If a random variable has a Binomial(3, 0.5) distribution then it takes the possible values 0, 1, 2, 3, with respective probability 1/8, 3/8, 3/8, 1/8.
The random variable in each of the following situations has a Binomial(3, 0.5) distribution.

-   $Y$ is the number of Tails in three flips of a fair coin
-   $Z$ is the number of even numbers rolled in three rolls of a fair six-sided die
-   $W$ is the number of female babies in a random sample of three births at a hospital (assuming boys and girls are equally likely[^distribution-2])

[^distribution-2]: [Which isn't quite true](https://www.npr.org/sections/health-shots/2015/03/30/396384911/why-are-more-baby-boys-born-than-girls).

Each of these situations involves a different sample space of outcomes (coins, dice, births) with a random variable which counts different things (Heads, Tails, evens, boys).
But all the scenarios have some general features in common:

-   There are 3 "trials" (3 flips, 3 rolls, 3 babies)
-   Each trial can be classified as "success"[^distribution-3] (Tails, even, female) or "failure".
-   Each trial is equally likely to result in success or not (fair coin, fair die, assuming boys and girls are equally likely)
-   The trials are independent. For coins and dice the trials are physically independent. For births independence follows from random selection from a large population.
-   The random variable counts the number of successes in the 3 trials (number of T, number of even rolls, number of female babies).

[^distribution-3]: There is no value judgment; sSuccess" just refers to whatever we're counting. Did the thing we're counting happen on this trial ("success) or not ("failure").
    Success isn't necessarily good.

These examples illustrate that knowledge that a random variable has a specific distribution (e.g., Binomial(3, 0.5)) does not necessarily convey any information about the underlying outcomes or random variable (function) being measured.
(We will study Binomial distributions in more detail later.)

The scenarios involving $W, X, Y, Z$ illustrate that two random variables do not have to be defined on the same sample space in order to determine if they have the same distribution.
This is in contrast to computing quantities like $\IP(X=Y)$: $\{X=Y\}$ is an event which cannot be investigated unless $X$ and $Y$ are defined for the same outcomes.
For example, you could not estimate the probability that a student has the same score on both SAT Math and Reading exams unless you measured pairs of scores for each student in a sample.
However, you could collect SAT Math scores for one set of students to estimate the marginal distribution of Math scores, and collect Reading scores for a separate set of students to estimate the marginal distribution of Reading scores.

A random variable can be defined explicitly as a function on a probability space, or implicitly through its distribution.
The distribution of a random variable is often assumed or specified directly, without mention of the underlying probabilty space or the function defining the random variable.
For example, a problem might state "let $Y$ have a Binomial(3, 0.5) distribution" or "let $Y$ have a Normal(30, 10) distribution".
But remember, such statements do not necessarily convey any information about the underlying sample space outcomes or random variable (function) being measured.
In Symbulate the `RV` command can also be used to define a RV implicitly via its distribution.
A definition like `X = RV(Binomial(3, 0.5))` effectively defines a random variable `X` on an unspecified probability space via an unspecified function.

```{python}

W = RV(Binomial(3, 0.5))

plt.figure()
W.sim(10000).plot()
plt.show()

```

```{example dd-same-joint-distribution}

Suppose that $X$, $Y$, and $Z$ all have the same distribution.  Donny Dont says

1. The pair $(X, Y)$ has the same joint distribution as the pair $(X, Z)$.
1. $X+Y$ has the same distribution as $X+Z$.
1. $X+Y$ has the same distribution as $X+X=2X$.

Determine if each of Donny's statements is correct.  If not, explain why not using a simple example.


```

```{solution dd-same-joint-distibution-sol}
to Example \@ref(exm:dd-same-joint-distribution)
```

```{asis, fold.chunk = TRUE}

First of all, Donny's statements wouldn't even make sense unless the random variables were all defined on the same probability space.  For example, if $X$ is SAT Math score and $Y$ is SAT reading score it doesn't makes sense to consider $X+Y$ unless $(X, Y)$ pairs are measured for the same students.  But even assuming the random variables are defined on the same probability space, we can find counterexamples to Donny's statements.

As just one example, flip a fair coin 4 times and let

- $X$ be the number of heads in flips 1 through 3
- $Y$ be the number of tails in flips 1 through 3
- $Z$ be the number of heads in flips 2 through 4.

1. The joint distribution of $(X, Y)$ is not the same as the joint distribution of $(X, Z)$. For example, $(X, Y)$ takes the pair $(3, 3)$ with probability 0, but $(X, Z)$ takes the pair $(3, 3)$ with nonzero probability (1/16).
1. The distribution of $X+Y$ is not the same as the distribution of $X+Z$; $X+Y$ is 3 with probability 1, but the probability that $X+Z$ is 3 is less than 1 (4/16). 
1. The distribution of $X+Y$ is not the same as the distribution of $2X$; $X+Y$ is 3 with probability 1, but $2X$ takes values 0, 2, 4, 6 with nonzero probability. 



```

Remember that a joint distribution is a probability distribution on pairs of values.
Just because $X_1$ and $X_2$ have the same marginal distribution, and $Y_1$ and $Y_2$ have the same marginal distribution, doesn't necessary imply that $(X_1, Y_1)$ and $(X_2, Y_2)$ have the same joint distributions.
In general, information about the marginal distributions alone is not enough to determine information about the joint distribution.
We saw a related two-way table example in Section \@ref(dist-intro).
Just because two two-way tables have the same totals, they don't necessarily have the same interior cells.

The distribution of any random variable obtained via a transformation of multiple random variables will depend on the joint distribution of the random variables involved; for example, the distribution of $X+Y$ depends on the joint distribution of $X$ and $Y$.

```{example uniform-same-dist}

Consider the probability space corresponding to two spins of the Uniform(0, 1) spinner and let $U_1$ be the result of the first spin and $U_2$ the result of the second.  For each of the following pairs of random variables, determine whether or not they have the same distribution as each other.  No calculations necessary; just think conceptually.

```

1.  $U_1$ and $U_2$
2.  $U_1$ and $1-U_1$
3.  $U_1$ and $1+U_1$
4.  $U_1$ and $U_1^2$
5.  $U_1+U_2$ and $2U_1$
6.  $U_1$ and $1-U_2$
7.  Is the joint distribution of $(U_1, 1-U_1)$ the same as the joint distribution of $(U_1, 1 - U_2)$?

```{solution uniform-same-dist-sol}
to Example \@ref(exm:uniform-same-dist)
```

```{asis, fold.chunk = TRUE}

1. Yes, each has a Uniform(0, 1) distribution.
1. Yes, each has a Uniform(0, 1) distribution.  For $u\in[0, 1]$, $1-u\in[0, 1]$, so $U_1$ and $1-U_1$ have the same possible values, and a linear rescaling does not change the shape of the distribution. Changing from $U_1$ to $1-U_1$ essentially amounts to switching the [0, 1] labels on the spinner from clockwise to counterclockwise.
1. No, the two variables do not have the same possible values.  The shapes would be similar though; $1+U_1$ has a Uniform(1, 2) distribution.
1. No, a non-linear rescaling generally changes the shape of the distribution.  For example, $\IP(U_1\le0.49) = 0.49$, but $\IP(U_1^2 \le 0.49) = \IP(U_1 \le 0.7) = 0.7$  Squaring a number in [0, 1] makes the number even smaller, so the distribution of $U_1^2$ places higher density on smaller values than $U_1$ does.
1. No, $U_1+U_2$ has a triangular shaped distribution on (0, 2) with a peak at 1. (The shape is similar to that of the distribution of $X$ in Section \@ref(sim-transform-joint), but the possible values are (0, 2) rather than (2, 8).) But $2U_1$ has a Uniform(0, 2) distribution.  Do not confuse a random variable with its distribution.  Just because $U_1$ and $U_2$ have the same distribution, you cannot replace $U_2$ with $U_1$ in transformations. The random variable $U_1+U_2$ is not the same random variable as $2U_1$; spinning a spinner and adding the spins will not necessarily produce the same value as spinner a spinner once and multiplying the value by 2.
1. Yes, just like $U_1$ and $1-U_2$ have the same distribution.
1. No. The marginal distributions are the same, but the joint distribution of $(U_1, 1-U_1)$ places all density along a line, while the joint density of $(U_1, 1-U_2)$ is distributed over the whole two-dimensional region $[0, 1]\times[0,1]$.

```

Do not confuse a random variable with its distribution.
This is probably getting repetitive by now, but we're emphasizing this point for a reason.
Many common mistakes in probability problems involve confusing a random variable with its distribution.
For example, we will soon that if a continuous random variable $X$ has probability density function $f(x)$, then the probability density function of $X^2$ is NOT $f(x^2)$ nor $(f(x))^2$.
Mistakes like these, which are very common, essentially involve confusing a random variable with its distribution.
Understanding the fundamental difference between a random variable and its distribution will help you avoid many common mistakes, especially in problems involving a lot of calculus or mathematical symbols.

## Discrete random variables: Probability mass functions

Discrete random variables take at most countably many possible values (e.g. $0, 1, 2, \ldots$).
They are often, but not always, counting variables (e.g., $X$ is the number of Heads in 10 coin flips).
We have seen in several examples that the distribution of a discrete random variable can be specified via a table listing the possible values of $x$ and the corresponding probability $\IP(X=x)$.
Always be sure to specify the possible values of $X$.

In some cases, the distribution has a "formulaic" shape and $\IP(X=x)$ can be written explicitly as a function of $x$.
For example, let $X$ be the sum of two rolls of a fair four-sided die.
The distribution of $X$ is displayed below.
The probabilities of the possible $x$ values follow a clear triangular pattern as a function of $x$.

(ref:cap-dice-sum-dist-table22) The marginal distribution of $X$, the sum of two rolls of a fair four-sided die.

```{r, dice-sum-dist-table22, echo = FALSE}
y = 2:8
p = c(1, 2, 3, 4, 3, 2, 1) / 16
knitr::kable(
  data.frame(y, p),
  col.names = c("x", "P(X=x)"),
  booktabs = TRUE,
  caption = "(ref:cap-dice-sum-dist-table22)",
  digits = 4
)
```

(ref:cap-dice-sum-dist-plot22) The marginal distribution of $X$, the sum of two rolls of a fair four-sided die.

```{r dice-sum-dist-plot22, echo = FALSE, fig.cap = "(ref:cap-dice-sum-dist-plot2)"}

ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 2:8) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "P(X = x)")

```

For each possible value $x$ of the random variable $X$, $\IP(X=x)$ can be obtained from the following formula

$$
p(x) =
\begin{cases}
\frac{4-|x-5|}{16}, & x = 2, 3, 4, 5, 6,7, 8,\\
0, & \text{otherwise.}
\end{cases}
$$

That is, $\IP(X = x) = p(x)$ for all $x$.
For example, $\IP(X = 2) = 1/16 = p(2)$; $\IP(X=5)=4/16=p(5)$; $\IP(X=7.5)=0=p(7.5)$.
To specify the distribution of $X$ we could provide Table \@ref(tab:dice-sum-dist-table22), or we could just provide the function $p(x)$ above.
Notice that part of the specification of $p(x)$ involves the possible values of $x$; $p(x)$ is only nonzero for $x=2,3, \ldots, 8$.
Think of $p(x)$ as a compact way of representing Table \@ref(tab:dice-sum-dist-table22).
The function $p(x)$ is called the *probability mass function* of the discrete random variable $X$.

```{definition pmf}

The **probability mass function (pmf)** (a.k.a., density (pdf)^[We use "pmf" for discrete distributions and reserve "pdf" for continuous probability density functions.  The terms "pdf" and "density" are sometimes used in both discrete and continuous situations even though the objects the terms represent differ between the two situations (probability versus density).  In particular, in R the `d` commands (`dbinom`, `dnorm`, etc) are used for both discrete and continuous distributions. In Symbulate, you can use `.pmf()` for discrete distributions.]) of a *discrete* RV $X$, defined on a probability space with probability measure $\IP$, is a function $p_X:\mathbb{R}\mapsto[0,1]$ which specifies each possible value of the RV and the probability that the RV takes that particular value: $p_X(x)=\IP(X=x)$ for each possible value of $x$.

```

The axioms of probability imply that a valid pmf must satisfy \begin{align*}
p_X(x) & \ge 0 \quad \text{for all $x$}\\
p_X(x) & >0 \quad \text{for at most countably many $x$ (the possible values, i.e., support)}\\
\sum_x p_X(x) & = 1
\end{align*}

The countable set of possible values of a discrete random variable $X$, $\{x: \IP(X=x)>0\}$, is called its **support**.

The pmf of a discrete random variable provides the probability of "equal to" events: $\IP(X = x)$.
Probabilities for other general events, e.g., $\IP(X \le x)$ can be obtained by summing the pmf over the range of values of interest.

We have seen that a distribution of a discrete random variable can be represented in a table, with a corresponding spinner.
Think of a pmf as providing a compact formula for constructing the table/spinner.

```{example, pmf-dice-max}

Let $Y$ be the larger of two rolls of a fair four-sided die.  Find the probability mass function of $Y$.

```

```{solution pmf-dice-max-sol}
to Example \@ref(exm:pmf-dice-max)
```

```{asis, fold.chunk = TRUE}

See Table \@ref(tab:dice-max-dist-table22) and  Figure \@ref(fig:dice-max-dist-plot22) below. As a function of $y=1, 2, 3, 4$, $\IP(Y=y)$ is linear with slope 2/16 passing through the point (1, 1/16).  The pmf of $Y$ is

\[
p_Y(y) =
\begin{cases}
\frac{2y-1}{16}, & y = 1, 2, 3, 4, \\
0, & \text{otherwise.}
\end{cases}
\]

For any $y$, $\IP(Y=y) = p_Y(y)$.  For example, $\IP(Y=2) = 3/16 = p_Y(2)$ and $\IP(Y = 3.3) = 0 = p_Y(3.3)$.

```

(ref:cap-dice-max-dist-table22) The marginal distribution of $Y$, the larger (or common value if a tie) of two rolls of a fair four-sided die.

```{r, dice-max-dist-table22, echo = FALSE}
y = 1:4
p = c(1, 3, 5, 7) / 16
knitr::kable(
  data.frame(y, p),
  col.names = c("y", "P(Y=y)"),
  booktabs = TRUE,
  caption = "(ref:cap-dice-max-dist-table2)",
  digits = 4
)
```

(ref:cap-dice-max-dist-plot22) The marginal distribution of $Y$, the larger (or common value if a tie) of two rolls of a fair four-sided die.

```{r dice-max-dist-plot22, echo = FALSE, fig.cap = "(ref:cap-dice-max-dist-plot2)"}

ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "orange") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 2:8) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "y",
       y = "P(Y = y)")

```

```{example dd-pmf-label}

Donny Dont provides two answers to Example \@ref(exm:pmf-dice-max).  Are his answers correct?  If not, why not?
  
1. $p_Y(y) = \frac{2y-1}{16}$
1. $p_Y(x) = \frac{2x-1}{16},\; x = 1, 2, 3, 4$, and $p_Y(x)= 0$ otherwise.


```

```{solution dd-pmf-label-sol}
to Example \@ref(exm:dd-pmf-label)
```

```{asis, fold.chunk = TRUE}

1. Donny's solution is incomplete; he forgot to specify the possible values. It's possible that someone who sees Donny's expression would think that $p_Y(2.5)=4/16$.  You don't necessarily always need to write "0 otherwise", but do always provide the possible values.
1. Donny's answer is actually correct, though maybe a little confusing.  The important place to put a $Y$ is the subscript of $p$: $p_Y$ identifies this function as the pmf of the random variable $Y$, as opposed to any other random variable that might be of interest.  The argument of $p_Y$ is just a dummy variable that defines the function.  As an analogy, $g(u)=u^2$ is the same function as $g(x)=x^2$; it doesn't matter which symbol defines the argument.  It is convenient to represent the argument of the pmf of $Y$ as $y$, and the argument of the pmf of $X$ as $x$, but this is not necessary.  Donny's answer does provide a way of constructing Table \@ref(tab:dice-max-dist-table). 

```

When there are multiple discrete random variables of interest, we usually identify their marginal pmfs with subscripts: $p_X, p_Y, p_Z$, etc.

<!-- For example, if $X$ is the number of heads in three flips of a fair coin, then $\IP(X=x)$ can be written in terms of the following formula^[This is an example of a Binomial probability mass function.  We will see the details behind this formula in Section @ref(sec-binomial).]. -->

<!-- \[ -->

<!-- \IP(X =x) = \frac{3!}{x!(3-x)!}(0.5)^{x}(0.5)^{3-x}, \qquad x = 0, 1, 2, 3. -->

<!-- \] -->

<!-- For example^[Recall factorial notation: if $k$ is a positive integer, then $k!=k(k-1)(k-2)\cdots (2)(1)$, e.g., $5! = 5(4)(3)(2)(1) = 120$.  Remember, $0!=1$ by definition.], $\IP(X=2) = \frac{3!}{2!(3-2)!}(0.5)^{2}(0.5)^{3-2}=3/8$. The above formula is an example of a *probability mass function*. -->

### Benford's law

We often specify the distribution of a random variable directly by providing its pmf.
Certain common distributions have special names.

```{example benford}

Randomly select a county in the U.S. Let $X$ be the leading digit in the county's population.  [For example](https://en.wikipedia.org/wiki/List_of_counties_in_California), if the county's population is 10,040,000 (Los Angeles County) then $X=1$; if 3,170,000 (Orange County) then $X=3$; if 283,000 (SLO County) then $X=2$; if 30,600 (Lassen County) then $X=3$.  The possible values of $X$ are $1, 2, \ldots, 9$.  You might think that $X$ is equally likely  to be any of its possible values.  However, a more appropriate model^[In a wide variety of data sets, the leading digit follows Benford's law.  This [Shiny app](http://shiny.calpoly.sh/BenfordData/) has a few examples.  Benford's law is often used in fraud detection.  In particular, if the leading digits in a series of values follows a distribution other than Benford's law, such as discrete uniform, then there is evidence that the values might have been fudged.  Benford's law has been used recently to test [reliability of reported COVID-19 cases and deaths](https://www.nature.com/articles/d41586-020-01565-5). [Here is a nice explanation](https://towardsdatascience.com/benfords-law-a-simple-explanation-341e17abbe75) of why leading digits might follow Benford's law in data sets that span multiple orders of magnitude] is to assume that  $X$ has pmf
\[
p_X(x) = 
\begin{cases}
\log_{10}(1+\frac{1}{x}), & x = 1, 2, \ldots, 9,\\
0, & \text{otherwise}
\end{cases}
\]
This distribution is known as [Benford's law](https://en.wikipedia.org/wiki/Benford's_law).

```

1.  Construct a table specifying the distribution of $X$, and the corresponding spinner.
2.  Find $\IP(X \ge 3)$

```{solution benford-sol}
to Example \@ref(exm:benford)
```

1.  Table \@ref(tab:benford-table) and the spinner in Figure \@ref(fig:spinner-benford) below specify the distribution of $X$.
2.  We can add the corresponding values from the pmf. $\IP(X \ge 3) = 1 - \IP(X <3) = 1 - (0.301 + 0.176) = 0.523$.

(ref:cap-benford-table) Benford's law.

```{r, benford-table, echo = FALSE}
y = 1:9
p = log(1 + 1 / y, base = 10)

knitr::kable(
  data.frame(y, p),
  col.names = c("x", "p(x)"),
  booktabs = TRUE,
  caption = "(ref:cap-benford-table)",
  digits = 3
)

```

(ref:cap-spinner-benford) Spinner corresponding to Benford's law.

```{r spinner-benford, echo=FALSE, fig.cap="(ref:cap-spinner-benford)"}


x = 1:9
p = log(1 + 1 / x, base = 10)

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  theme_void() +
  # plot the possible values on the outside
  scale_y_continuous(breaks = plotp, labels=xp$x) +
  theme(axis.text.x=element_text(size=14, face="bold")) +
  # plot the probabilities as percents inside
  geom_text(aes(y = plotp,
                label = percent(p)), size=3,
            color=c(rep("black", 9))) +
  ggtitle(paste("Benford's law", sep=""))

spinner

```

### Poisson distributions

Poisson distributions are often used to model random variables that count "relatively rare events".

```{example, homerun-poisson}

Let $X$ be the number of home runs hit (in total by both teams) in a randomly selected Major League Baseball game.  Technically, there is no fixed upper bound on what $X$ can be, so mathematically it is convenient to consider $0, 1, 2, \ldots$ as the possible values of $X$. Assume that the pmf of $X$ is

\[
p_X(x) =
\begin{cases}
e^{-2.3} \frac{2.3^x}{x!}, & x = 0, 1, 2, \ldots\\
0, & \text{otherwise.}
\end{cases}
\]

This is known as the Poisson(2.3) distribution.  

```

1.  Verify that $p_X$ is a valid pmf.
2.  Compute $\IP(X = 3)$, and interpret the value as a long run relative frequency.
3.  Construct a table and spinner corresponding to the distribution of $X$.
4.  Find $\IP(X \le 13)$, and interpret the value as a long run relative frequency. ([The most home runs ever hit in a baseball game is 13](https://www.mlb.com/news/d-backs-and-phillies-set-mlb-home-run-record).)
5.  Find and interpret the ratio of $\IP(X = 5)$ to $\IP(X = 3)$. Does the value $e^{-2.3}$ affect this ratio?
6.  Use simulation to find the long run average value of $X$, and interpret this value.
7.  Use simulation to find the variance and standard deviation of $X$.

```{solution homerun-poisson-sol}
to Example \@ref(exm:homerun-poisson)
```

```{asis, fold.chunk = TRUE}

1. We need to verify that the probabilities in the pmf sum to 1.
We can construct the corresponding table and just make sure the values sum to 1.
Technically, any value $0, 1, 2, \ldots$ is possible, but the probabilities will get closer and closer to 0 as $x$ gets larger.
So we can cut our table off at some reasonable upper bound for $X$, where the sum of the values in the table is close enough to 1 for practical purposes.
If we wanted to sum over all possible values of $X$, we need to use the [Taylor series expansion of $e^u$.](https://www.mathsisfun.com/algebra/taylor-series.html)
    \[
    \sum_{x=0}^\infty e^{-2.3} \frac{2.3^x}{x!} = e^{-2.3} \sum_{x=0}^\infty \frac{2.3^x}{x!} = e^{-2.3}e^{2.3} = 1   
    \]
The constant $e^{-2.3}\approx0.100$ simply ensures that the probabilities that follow the shape determined by $2.3^x/ x!$ sum to 1.
1.  Just plug $x=3$ into the pmf: $\IP(X=3)=p_X(3)=e^{-2.3}2.3^3/3! = 0.203$.  In the long run, 20.3% of baseball games have 3 home runs. 
1. See the table and Figure \@ref(fig:poisson-hr-pmf-plot) below.  Plug each value of $x$ into the pmf.  For example, $\IP(X = 5) =p_X(5)=e^{-2.3}2.3^5/5! = 0.054$.
1. Just sum the values of the pmf corresponding to the values $x = 0, 1, \ldots, 13$: $\IP(X \le 13) = \sum_{x=0}^{13} p_X(x)=0.9999998$.  There isn't any shorter way to do it. In the long run, almost all MLB games have at most 13 home runs. Even though the pmf assigns nonzero probability to all values 0, 1, 2, $\ldots$, the probability that $X$ takes a value greater than 13 is extremely small.
1. The ratio is
    \[
    \frac{\IP(X=3)}{\IP(X=5)} = \frac{p_X(3)}{p_X(5)} = \frac{e^{-2.3}2.3^3/3!}{e^{-2.3}2.3^5/5!} = \frac{2.3^3/3!}{2.3^5/5!} = 3.78
    \]
    Games with 3 home runs occur about 3.8 times more frequently than games with 5 home runs.  The constant $e^{-2.3}$ does not affect this ratio; see below for further discussion.
1. The simulation results below suggest that the long run average value of $X$ is equal to the parameter 2.3. Over many baseball games there are a total of 2.3 home runs per game on average.
1. The simulation results also suggest that variance of $X$ is equal to 2.3, and the standard deviation of $X$ is equal to $\sqrt{2.3}\approx 1.52$.

```

| $x$ |                         $p(x)$ |    Value |
|-----|-------------------------------:|---------:|
| 0   |     $e^{-2.3}\frac{2.3^0}{0!}$ | 0.100259 |
| 1   |     $e^{-2.3}\frac{2.3^1}{1!}$ | 0.230595 |
| 2   |     $e^{-2.3}\frac{2.3^2}{2!}$ | 0.265185 |
| 3   |     $e^{-2.3}\frac{2.3^3}{3!}$ | 0.203308 |
| 4   |     $e^{-2.3}\frac{2.3^4}{4!}$ | 0.116902 |
| 5   |     $e^{-2.3}\frac{2.3^5}{5!}$ | 0.053775 |
| 6   |     $e^{-2.3}\frac{2.3^6}{6!}$ | 0.020614 |
| 7   |     $e^{-2.3}\frac{2.3^7}{7!}$ | 0.006773 |
| 8   |     $e^{-2.3}\frac{2.3^8}{8!}$ | 0.001947 |
| 9   |     $e^{-2.3}\frac{2.3^9}{9!}$ | 0.000498 |
| 10  | $e^{-2.3}\frac{2.3^{10}}{10!}$ | 0.000114 |
| 11  | $e^{-2.3}\frac{2.3^{11}}{11!}$ | 0.000024 |
| 12  | $e^{-2.3}\frac{2.3^{12}}{12!}$ | 0.000005 |
| 13  | $e^{-2.3}\frac{2.3^{13}}{13!}$ | 0.000001 |
| 14  | $e^{-2.3}\frac{2.3^{14}}{14!}$ | 0.000000 |

: Table representing the Poisson(2.3) probability mass function.

(ref:cap-poisson-hr-pmf-plot) Impulse plot representing the Poisson(2.3) probability mass function.

```{r poisson-hr-pmf-plot, echo = FALSE, fig.cap = "(ref:cap-poisson-hr-pmf-plot)"}


y = 0:13
p = dpois(y, 2.3)
ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 0:12) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "p(x)")

```

Figure \@ref(fig:spinner-poisson-hr) displays a spinner corresponding to the Poisson(2.3) distribution.
To simplify the display we have lumped all values $6, 7, \ldots$ into one "6+" category.

(ref:cap-spinner-poisson-hr) Spinner corresponding to the Poisson(2.3) distribution.

```{r spinner-poisson-hr, echo=FALSE, fig.cap="(ref:cap-spinner-poisson-hr)"}


x = c(0:5, "6+")
p = c(dpois(0:5, 2.3), 1-ppois(5, 2.3))

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = plotp, labels=xp$x) +
  theme(axis.text.x=element_text(size=14, face="bold")) +
  # plot the probabilities as percents inside
  geom_text(aes(y = plotp,
                label = percent(p)), size=4,
            color=c(rep("black", 6), rep("white", 1))) +
  ggtitle(paste("Poisson(2.3) distribution", sep=""))

spinner

```

The constant $e^{-2.3}$ doesn't affect the shape of the probability mass function.
Rather, the constant $e^{-2.3}$ is what ensures that the probabilities sum to 1.
We could have written the pmf as

$$
p_X(x) \propto
\begin{cases}
\frac{2.3^x}{x!}, & x = 0, 1, 2, \ldots\\
0, & \text{otherwise.}
\end{cases}
$$

The symbol $\propto$ means "is proportional to".
This specification is enough to determine the shape of the distribution and relative likelihoods.
For example, the above is enough to determine that the probability that $X$ takes the value 3 is 3.78 times greater than the probability that $X$ takes the value 5.
Once we have the shape of the distribution, we can "renormalize" by multiplying all values by a constant, in this case $e^{-2.3}$, so that the values sum to 1.
We saw a similar idea in Example \@ref(exm:worldseries-proportional).
The constant is whatever it needs to be so that the values sum to 1; what's important is the relative shape.
The "is proportional to" specification defines the shape of the plot; the constant just rescales the values on the probability axis.

Why might we assume this particular Poisson(2.3) distribution for the number of home runs per game?
We'll discuss this point in more detail later.
For now we'll just present Figure \@ref(fig:poisson-hr-data) which displays the actual distribution of home runs over the 2431 games in the 2018 MLB season.
The spikes represent the observed relative frequencies; the connecting dots represent the theoretical Poisson(2.3) pmf.
We can see that the Poisson(2.3) distribution models the data reasonably well.

(ref:cap-poisson-hr-data) Data on home runs per game in the 2018 MLB season, compared with the Poisson(2.3) distribution.

```{r poisson-hr-data, echo=FALSE, fig.cap="(ref:cap-poisson-hr-data)"}

knitr::include_graphics("_graphics/poisson-hr-data.png")

```

A general Poisson distribution is defined by a single parameter $\mu>0$.
(In the home run example, $\mu=2.3$.)

```{definition, def-poisson}
A discrete random variable $X$ has a **Poisson distribution** with parameter^[The parameter for a Poisson distribution is often denoted $\lambda$.  However, we use $\mu$ to denote the parameter of a Poisson distribution, and reserve $\lambda$  to denote the rate parameter of a *Poisson process* (which has mean $\lambda t$ at time $t$).] $\mu>0$ if its probability mass function $p_X$ satisfies
\begin{align*}
p_X(x) & \propto \frac{\mu^x}{x!}, \;\qquad x=0,1,2,\ldots\\
& = \frac{e^{-\mu}\mu^x}{x!}, \quad x=0,1,2,\ldots
\end{align*}
The function $\mu^x / x!$ defines the shape of the pmf.  The constant $e^{-\mu}$ ensures that the probabilities sum to 1.


If $X$ has a Poisson($\mu$) distribution then
\begin{align*}
\text{Long run average value of $X$} & = \mu\\
\text{Variance of $X$} & = \mu\\
\text{SD of $X$} & = \sqrt{\mu}
\end{align*}

```

There is not one "Poisson distribution" but rather a family of Poisson distributions.
Each Poisson distribution follows the general pattern specified by the Poisson pmf, with the particular shape determined by the parameter $\mu$.
While the possible values of a variable that follows a Poisson distribution are nonnegative integers, the parameter $\mu$ can be any positive value.

```{python, eval = FALSE}
Poisson(1).plot()
Poisson(2.3).plot()
Poisson(2.5).plot()
```

```{python, echo = FALSE}
plt.figure()
Poisson(1).plot()
Poisson(2.3).plot()
Poisson(3.5).plot()
plt.xlim(left = -0.2);
plt.show();
```

We will see more properties and uses of Poisson distributions later.

In Symbulate we can define a random variable with a Poisson(2.3) distribution and simulate values.

```{python}
X =  RV(Poisson(2.3))

x = X.sim(10000)
x
```

The spikes in the plot below correspond to simulated relative frequencies.
The connecting dots displayed by `Poisson(2.3).plot()` are determined by the theoretical Poisson(2.3) pmf.

```{python, eval = FALSE}
x.plot() # plot the simulated values and their relative frequencies

Poisson(2.3).plot() # plot the theoretical Poisson(2.3) pmf
```

```{python, echo = FALSE}

plt.figure()
x.plot()
Poisson(2.3).plot()
plt.show()

```

The approximate long run average value and variance are both about equal to the parameter 2.3.

```{python}

x.mean(), x.var(), x.sd()
```

The Symbulate `pmf` method can be used to compute the pmf for named distributions.
The following compares the simulated relative frequency of $\{X = 3\}$ to the theoretical probability $p_X(3)$.

```{python}

x.count_eq(3) / x.count(), Poisson(2.3).pmf(3)

```

You can evaluate the pmf at multiple values.

```{python}

xs = list(range(5)) # the values 0, 1, ..., 4

Poisson(2.3).pmf(xs)

```

Below we use the Python package `tabulate` to construct a somewhat nicer table.
Don't get this tabulate confused with `.tabulate()` in Symbulate.

```{python}

xs = list(range(14)) # the values 0, 1, ..., 13

from tabulate import tabulate

print(tabulate({'x': xs,
                'p_X(x)': [Poisson(2.3).pmf(u) for u in xs]},
               headers = 'keys', floatfmt=".6f"))

```

We can obtain $\IP(X \le 13)$ by summing the corresponding probabilities from the pmf.
We can also find $\IP(X \le 13)$ by evaluating the `cdf` at 13.
(We will see more about *cumulative distribution functions* (cdfs) soon.)

```{python}

Poisson(2.3).pmf(xs).sum(), Poisson(2.3).cdf(13)

```

### Binomial distributions {#sec-binomial}

In some cases, a pmf of a discrete random variable can be derived from the assumptions about the underyling random phenomenon.

```{example, binomial-capture}
Capture-recapture sampling is a technique often used to estimate the size of a population.  Suppose you want to estimate $N$, the number of [monarch butterflies in Pismo Beach](https://www.parks.ca.gov/?page_id=30273).  (Assume that $N$ is a fixed but unknown number; the population size doesn't change over time.) You first capture a sample of $N_1$ butterflies, selected randomly, and tag them and release them.  At a later date, you then capture a second sample of $n$ butterflies, selected randomly^[We'll compare to the case of sampling without replacement later.] *with replacement*.  Let $X$ be the number of butterflies in the second sample that have tags (because they were also caught in the first sample).  (Assume that the tagging has no effect on behavior, so that selection in the first sample is independent of selection in the second sample.)

In practice, $N$ is unknown and the point of capture-recapture sampling is to estimate $N$.  But let's start with a simpler, but unrealistic, example where there are $N=52$ butterflies, $N_1 = 13$ are tagged and $N_0=52-13 = 39$ are not, and $n=5$ is the size of the second sample.

```

1.  Explain why it is reasonable to assume that the results of the five individual selections are independent.
2.  Compute and interpret $\IP(X=0)$.
3.  Compute the probability that the first butterfly selected is tagged but the others are not.
4.  Compute the probability that the last butterfly selected is tagged but the others are not.
5.  Compute and interpret $\IP(X=1)$.
6.  Compute and interpret $\IP(X=2)$.
7.  Find the pmf of $X$.
8.  Construct a table, plot, and spinner representing the distribution of $X$.
9.  Make an educated guess for the long run average value of $X$.\
10. How do the results depend on $N_1$ and $N_0$?

```{solution binomial-capture-sol}
to Example \@ref(exm:binomial-capture)
```

```{asis, fold.chunk = TRUE}

1. Since the selections are made with replacement, at the time of each selection there are 52 butterflies of which 13 are tagged, regardless of the results of previous selections.  That is, for each selection the conditional probability that a butterfly is tagged is 13/52 regardless of the results of other selections.
1. Let S (for "success") represent that a selected butterfly is tagged, and F (for "failure") represent that it's not tagged.
None of the 5 butterflies are tagged if the selected sequence is FFFFF.
Since the selections are independent
$$
\IP(X = 0) = 
\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right) = \left(\frac{39}{52}\right)^5  = 0.237
$$
Repeating the process in the long run means drawing many samples of size 5: 23.7% of samples of size 5 will have 0 tagged butterflies.
1. Since the selections are independent, the probability of the outcome SFFFF is
$$
\left(\frac{13}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right) = \left(\frac{13}{52}\right)\left(\frac{39}{52}\right)^4  = 0.079
$$
1. The probability of the outcome FFFFS is the same as in the previous part
$$
\left(\frac{13}{52}\right)\left(\frac{39}{52}\right)^4  = 0.079
$$
1. Each of the particular outcomes with 1 tagged butterfly has probability $\left(\frac{13}{52}\right)\left(\frac{39}{52}\right)^4=0.079$.  So we need to count how many outcomes result in 1 tagged butterfly. Since there are 5 "spots" where the 1 tagged butterfly can be, there are $\binom{5}{1}=5$ such sequences (SFFFF, FSFFF, FFSFF, FFFSF, FFFFS)
$$
\IP(X = 1) = \binom{5}{1}\left(\frac{13}{52}\right)\left(\frac{39}{52}\right)^4  = 5(0.079) = 0.3955
$$
39.6% of samples of size 5 will have exactly 1 tagged butterfly.
1. Each of the particular outcomes with 2 tagged butterflies (like SSFFF) has probability $\left(\frac{13}{52}\right)^2\left(\frac{39}{52}\right)^3=0.02637$.  Since there are 5 "spots" where the 2 tagged butterflies can be, there are $\binom{5}{2}=10$ such sequences.  
$$
\IP(X = 2) = \binom{5}{2}\left(\frac{13}{52}\right)^2\left(\frac{39}{52}\right)^3  = 10(0.02637) = 0.2637
$$
26.4% of samples of size 5 will have exactly 2 tagged butterflies.
1. Similar to the previous part. Any particular outcome with $x$ successes has $5-x$ failures.
The probability of any particular outcomes with exactly $x$ successes (and $5-x$ failures) is $(13/52)^x(39/52)^{5-x}$.
There are $\binom{5}{x}$ outcomes that result in exactly $x$ successes, so the total probability of $x$ successes is
$$
p_X(x) = \binom{5}{x}\left(\frac{13}{52}\right)^x\left(\frac{39}{52}\right)^{5-x}, \qquad x = 0, 1, 2, 3, 4, 5
$$
1. See below. Plug each possible value into the pmf from the previous part.
1. If 1/4 of the butterflies in the population are tagged, we would also expect 1/4 of the butterflies in a randomly selected sample to be tagged.
We would expect the long run average value to be $5(13/52) = 1.25$. 
1. The results only depend on $N_1$ and $N_0$ through the ratio $13/52 = N_1/(N_0+N_1)$.  That is, when the selections are made with replacement, only the population proportion is needed.
If we had only been told that 25% of the butterflies were tagged, instead of 13 out of 52, we still could have solved the problem and nothing would change.

```

The distribution of $X$ in the previous problem is called the Binomial(5, 0.25) distribution.
The parameter 5 is the size of the sample, and the parameter 0.25 is the proportion of successes in the population.

| $x$ |                             $p(x)$ |    Value |
|-----|-----------------------------------:|---------:|
| 0   | $\binom{5}{0}0.25^0(1-0.25)^{5-0}$ | 0.237305 |
| 1   | $\binom{5}{1}0.25^1(1-0.25)^{5-1}$ | 0.395508 |
| 2   | $\binom{5}{2}0.25^2(1-0.25)^{5-2}$ | 0.263672 |
| 3   | $\binom{5}{3}0.25^3(1-0.25)^{5-3}$ | 0.087891 |
| 4   | $\binom{5}{4}0.25^4(1-0.25)^{5-4}$ | 0.014648 |
| 5   | $\binom{5}{5}0.25^5(1-0.25)^{5-5}$ | 0.000977 |

: Table representing the Binomial(5, 0.25) probability mass function.

(ref:cap-binomial-butterfly-pmf-plot) Impulse plot representing the Binomial(5, 0.25) probability mass function.

```{r binomial-butterfly-pmf-plot, echo = FALSE, fig.cap = "(ref:cap-binomial-butterfly-pmf-plot)"}


y = 0:5
p = dbinom(y, 5, 0.25)
ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 0:5) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "p(x)")

```

Figure \@ref(fig:spinner-binomial-butterfly) displays a spinner corresponding to the Binomial(5, 0.25) distribution.
To simplify the display we have lumped 4 and 5 into one "4+" category.

(ref:cap-spinner-binomial-butterfly) Spinner corresponding to the Binomial(5, 0.25) distribution.

```{r spinner-binomial-butterfly, echo=FALSE, fig.cap="(ref:cap-spinner-binomial-butterfly)"}


x = c(0:3, "4+")
p = c(dbinom(0:3, 5, 0.25), 1-pbinom(3, 5, 0.25))

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = plotp, labels=xp$x) +
  theme(axis.text.x=element_text(size=14, face="bold")) +
  # plot the probabilities as percents inside
  geom_text(aes(y = plotp,
                label = percent(p)), size=4,
            color=c(rep("black", 4), rep("white", 1))) +
  ggtitle(paste("Binomial(5, 0.25) distribution", sep=""))

spinner

```

```{definition, binomial}

A discrete random variable $X$ has a **Binomial distribution** with parameters
$n$, a nonnegative integer, and $p\in[0, 1]$ if its probability mass function is
\begin{align*}
p_{X}(x) & = \binom{n}{x} p^x (1-p)^{n-x}, & x=0, 1, 2, \ldots, n
\end{align*}
If $X$ has a Binomial($n$, $p$) distribution then
\begin{align*}
\text{Long run average value of $X$} & = np\\
\text{Variance of $X$} & = np(1-p)\\
\text{SD of $X$} & = \sqrt{np(1-p)}
\end{align*}
```

Imagine a box containing tickets with $p$ representing the proportion of tickets in the box labeled 1 ("success"); the rest are labeled 0 ("failure").
Randomly select $n$ tickets from the box *with replacement* and let $X$ be the number of tickets in the sample that are labeled 1.
Then $X$ has a Binomial($n$, $p$) distribution.
Since the tickets are labeled 1 and 0, the random variable $X$ which counts the number of successes is equal to the sum of the 1/0 values on the tickets.
If the selections are made with replacement, the draws are independent, so it is enough to just specify the population proportion $p$ without knowing the population size $N$.

The situation in the previous paragraph and the butterfly example involves a sequence of **Bernoulli trials**.

-   There are only two possible outcomes, "success" (1) and "failure" (0), on each trial.
-   The unconditional/marginal probability of success is the same on every trial, and equal to $p$
-   The trials are independent.

If $X$ counts the number of successes in a *fixed number*, $n$, of Bernoulli($p$) trials then $X$ has a Binomial($n, p$) distribution.\
Careful: Don't confuse the number $p$, the probability of success on any single trial, with the probability mass function $p_X(\cdot)$ which takes as an input a number $x$ and returns as an output the probability of $x$ successes in $n$ Bernoulli($p$) trials, $p_X(x)=\IP(X=x)$.

(ref:cap-binom-plots-n) Probability mass functions for Binomial($n$, 0.4) distributions for $n = 5, 10, 15, 20$.

```{python, echo = FALSE, fig-binom-plots-n, fig.cap = "(ref:cap-binom-plots-n)"}

p = 0.4

ns = [5, 10, 15, 20]

plt.figure()

for n in ns:
  Binomial(n, p).plot()

plt.legend(ns, title = "n")

plt.show()

```

(ref:cap-binom-plots-p) Probability mass functions for Binomial(10, $p$) distributions for $p = 0.1, 0.3, 0.5, 0.7, 0.9$.

```{python, echo = FALSE, fig-binom-plots-p, fig.cap = "(ref:cap-binom-plots-p)"}

n = 10

ps = [0.1, 0.3, 0.5, 0.7, 0.9]

plt.figure()

for p in ps:
  Binomial(n, p).plot()

plt.legend(ps, title = "p", loc = "upper center");
plt.xlim(-0.2, n + 0.2);

plt.show()
```

We will study more properties and uses of Binomial distributions later.

Now we simulate a sample of size 5 with replacement from a box model with 52 tickets, 12 labeled 1 (success) and 39 labeled 0 (failure).
With 1/0 representing S/F, we can also obtain the number of successes with `X = RV(P, sum)`.

```{python, sym-cards-binomial}
P = BoxModel({1: 13, 0: 39}, size = 5, replace = True)

X = RV(P, count_eq(1))

x = X.sim(10000)

x
```

```{python, eval = FALSE}
x.plot() # plot the simulated values and their relative frequencies

Binomial(n = 5, p = 13 / 52).plot() # plot the theoretical Binomial(5, 0.25) pmf

```

```{python, echo = FALSE}
plt.figure()
x.plot()
Binomial(n = 5, p = 13 / 52).plot()
plt.show()

```

Compare the approximate $\IP(X = 2)$ to the theoretical value.

```{python}

x.count_eq(2) / 10000, Binomial(n = 5, p = 13 / 52).pmf(2)

```

Compare the simulated average value of $X$ to the theoretical long run average value.

```{python}

x.mean(), Binomial(n = 5, p = 13 / 52).mean()

```

Compare the simulated average value of $X$ to the theoretical long run average value.

```{python}

x.var(), Binomial(n = 5, p = 13 / 52).var()

```

## Continuous random variables: Probability density functions {#pdf}

The continuous analog of a probability mass function (pmf) is a *probability density function (pdf)*.
However, while pmfs and pdfs play analogous roles, they are different in one fundamental way; namely, a pmf outputs probabilities directly, while a pdf does not.
We have seen that a pmf of a discrete random variable can be summed to find probabilities of related events.
We will see now that a pdf of a continuous random variable must be *integrated* to find probabilities of related events.

In Section \@ref(sec-linear-rescaling) we introduced histograms to summarize simulated values of a continuous random variable.
In a histogram the variable axis is chopped into intervals of equal width, and the other axis is on the density scale, so that the *area* of each bar represents the relative frequency of values that lie in the interval.

We have seen examples like the Normal(30, 10) distribution where the shape of the histogram can be approximated by a smooth curve.
This curve represents an idealized model of what the histogram would look like if infinitely many values were simulated and the histogram bins were infinitesimally small.

Suppose we are interested in the random variable $X = - \log(1 - U)$ where $U$ has a Uniform(0, 1) distribution.
(We will see why we might be interested in this particular transformation soon.) Here is a histogram of 10000 simulated values of $X$.

```{python}
U = RV(Uniform(0, 1))

X = -log(1 - U)

x = X.sim(10000)

```

```{python, eval = FALSE}
x.plot()
```

```{python, echo = FALSE}
plt.figure()
x.plot()
plt.show()
```

Imagine that we

-   keep simulating more and more values, and
-   make the histogram bin widths smaller and smaller.

Then the "chunky" histogram would get "smoother".
The following plot summarizes the results of 100,000 simulated values of $X$ in a histogram with 1000 bins, each of width on the order of 0.01.
The command `Exponential(1).plot()` overlays the smooth curve modeling the theoretical shape of the distribution of $X$ (called the "Exponential(1)" distribution). This curve is an example of a pdf.

```{python, eval = FALSE}
X.sim(100000).plot(bins=1000) # histogram of simulated values

Exponential(1).plot() # overlays the smooth curve

```

(ref:cap-exponential-smooth-histogram) A histogram of simulated values of $X = -\log(1-U)$, where $U$ has a Uniform(0, 1) distribution. With many simulated values and very fine bins, the shape of the histogram is well approximated by a smooth curve, called the "Exponential(1) density".

```{python exponential-smooth-histogram, echo = FALSE, fig.cap="(ref:cap-exponential-smooth-histogram)"}

plt.figure()
X.sim(100000).plot(bins=1000)
Exponential(1).plot() # overlays the smooth curve
plt.show()

```

A pdf represents "relative likelihood" as a function of possible values of the random variable.
Just as area represents relative frequency in a histogram, area under a pdf represents probability.

```{definition pdf}

The **probability density function (pdf)** (a.k.a.\ density) of a *continuous* RV $X$, defined on a probability space with probability measure $\IP$, is a function $f_X:\mathbb{R}\mapsto[0,\infty)$ which satisfies
\begin{align*}
\IP(a \le X \le b) & =\int_a^b f_X(x) dx, \qquad \text{for all } -\infty \le a \le b \le \infty
\end{align*}

```

For a continuous random variable $X$ with pdf $f_X$, the probability that $X$ takes a value in the interval $[a, b]$ is the *area under the pdf over the region* $[a,b]$.

A pdf assigns zero probability to intervals where the density is 0.
A pdf is usually defined for all real values, but is often nonzero only for some subset of values, the possible values of the random variable.
We often write the pdf as $$
f_X(x) =
\begin{cases}
\text{some function of $x$}, & \text{possible values of $x$}\\
0, & \text{otherwise.}
\end{cases}
$$ The "0 otherwise" part is often omitted, but be sure to specify the range of values where $f$ is positive.

The axioms of probability imply that a valid pdf must satisfy \begin{align*}
f_X(x) & \ge 0 \qquad \text{for all } x,\\
\int_{-\infty}^\infty f_X(x) dx & = 1
\end{align*}

The total area under the pdf must be 1 to represent 100% probability.
Given a specific pdf, the generic bounds $(-\infty, \infty)$ in the above integral should be replaced by the range of possible values, that is, those values for which $f_X(x)>0$.

```{example exponential-pdf}

Let $X$ be a random variable with the "Exponential(1)" distribution, illustrated by the smooth curve in Figure \@ref(fig:exponential-smooth-histogram).  Then the pdf of $f_X$ is

\[
f_X(x) =
\begin{cases}
e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]

```

1.  Verify that $f_X$ is a valid pdf.
2.  Find $\IP(X\le 1)$.
3.  Find $\IP(X\le 2)$.
4.  Find $\IP(1 \le X< 2.5)$.
5.  Compute the 25th percentile of $X$.
6.  Compute the 50th percentile of $X$.
7.  Compute the 75th percentile of $X$.
8.  Start to construct a spinner representing the Exponential(1) distribution.

```{solution exponential-pdf-sol}

to Example \@ref(exm:exponential-pdf)

```

```{asis, fold.chunk = TRUE}


1. We need to check that the pdf integrates to 1: $\int_0^\infty e^{-x}dx = 1$.
1. $\IP(X\le 1) = \int_0^1 e^{-x}dx = 1-e^{-1}\approx 0.632$.  See the spinner in Figure \@ref(fig:exponential1-spinner); 63.2% of the area corresponds to $[0, 1]$. 
1. $\IP(X\le 2) = \int_0^2 e^{-x}dx = 1-e^{-2}\approx 0.865$.  See the spinner in Figure \@ref(fig:exponential1-spinner); 86.5% of the area corresponds to $[0, 2]$.
1. $\IP(1 \le X< 2.5) = \int_1^{2.5} e^{-x}dx = e^{-1}-e^{-2.5}\approx 0.286$.  See the illustration below.
1. We want to find $x>0$ such that $\IP(X \le x) =0.25$:
$$
0.25 = \IP(X \le x) = \int_0^{x} e^{-u}du = 1 - e^{-x}
$$
Solve to get $x = -\log(1-0.25) = 0.288$. 25% of values of $X$ are at most 0.288.
The value 0.288 goes at 3 o'clock on the spinner axis.
1. We want to find $x>0$ such that $\IP(X \le x) =0.5$:
$$
0.5 = \IP(X \le x) = \int_0^{x} e^{-u}du = 1 - e^{-x}
$$
Solve to get $x = -\log(1-0.5) = 0.693$. 50% of values of $X$ are at most 0.693.
The value 0.693 goes at 6 o'clock on the spinner axis.
1. We want to find $x>0$ such that $\IP(X \le x) =0.75$:
$$
0.75 = \IP(X \le x) = \int_0^{x} e^{-u}du = 1 - e^{-x}
$$
Solve to get $x = -\log(1-0.75) = 1.386$. 75% of values of $X$ are at most 1.386.
The value 1.386 goes at 9 o'clock on the spinner axis.
1. See the spinner in Figure \@ref(fig:exponential1-spinner). It's the same spinner on both sides, with different values highlighted.
Notice that intervals near 0 are stretched out, while intervals away from 0 are shrunk.

```

The shaded area under the curve below represents $\IP(1<X<2.5)$.

(ref:cap-exponential-pdf-area) Illustration of $\IP(1<X<2.5)$ for $X$ with an Exponential(1) distribution.

```{r exponential-pdf-area, echo=FALSE, fig.cap="(ref:cap-exponential-pdf-area)"}

x<-seq(0, 5, 0.001)
y<-dexp(x,1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>1 & x<2.5),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, 1), expand=c(0, 0)) +
  theme_classic() +
  xlab(expression(x)) +
  ylab(expression(f[X](x))) #+
#  ggtitle(paste("P(1<X<2.5) for X ~ Exponential(1)"))

```

(ref:cap-exponential1-spinner) An Exponential(1) spinner. The same spinner is displayed on both sides, with different features highlighted on the left and right. Only selected rounded values are displayed, but in the idealized model the spinner is infinitely precise so that any real number greater than 0 is possible. Notice that the values on the axis are *not* evenly spaced.

```{r exponential1-spinner, echo=FALSE, fig.show="hold", out.width="50%", fig.cap="(ref:cap-exponential1-spinner)"}

n = 16

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner1 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("...|0", round(qexp(xp$x[-1], rate = 1), 3))) +
  theme(axis.text.x=element_text(size=12, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
  ggtitle(paste("Exponential(1) distribution", sep=""))

spinner1


x = c(0.5, 1, 1.5, 2, 2.5, "...")
p = c(pexp(0.5), pexp(1) - pexp(0.5), pexp(1.5) - pexp(1), pexp(2) - pexp(1.5), pexp(2.5) - pexp(2), 1 - pexp(2.5))

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))



plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner2 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white", linetype=1) + 
  # coord_polar("y", start=0) +
  coord_curvedpolar("y", start = 0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = cdf[-c(length(cdf))], labels=c("... | 0", 0.5, 1.0, 1.5, 2.0, 2.5)) +
  # theme(axis.text.x=element_text(angle=c(90-180/50*(0:49), -90-180/50*(50:99)), size=8)) +
  theme(axis.text.x=element_text(angle = 0, size=12, face = "bold")) +
  annotate(geom="segment", y=(0:19)/20+0.000, yend = (0:19)/20+0.000,
           x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
    geom_text(aes(y = plotp,
                label = percent(p)), size=4, color=c(rep("black",5), rep("black",1))) +
  ggtitle(paste("Exponential(1) distribution", sep=""))

spinner2

```

### Uniform distributions

In general, a pdf $f_X(x)$ depends on the value $x$ of the random variable $X$.
Uniform distributions are a special case where the pdf is *constant* for all possible values.

```{example meeting-pdf1}
In the meeting problem assume that Regina's arrival time $X$ (minutes after noon) follows a Uniform(0, 60) distribution.

```

1.  Sketch a plot of the pdf of $X$.
2.  Donny Dont says that the pdf is $f_X(x) = 1/60$. Do you agree? If not, specify the pdf of $X$.
3.  Use the pdf to find the probability that Regina arrives before 12:15.
4.  Use the pdf to find the probability that Regina arrives after 12:45.
5.  Use the pdf to find the probability that Regina arrives between 12:15 and 12:45.
6.  Use the pdf to find the probability that Regina arrives between 12:15:00 and 12:16:00.
7.  Use the pdf to find the probability that Regina arrives between 12:15:00 and 12:15:01.
8.  Use the pdf to find the probability that Regina arrives at the exact time 12:15:00 (with infinite precision).

```{solution meeting-pdf1-sol}

to Example \@ref(exm:meeting-pdf1)

```

```{asis, fold.chunk = TRUE}
Note: in a situation like this, you should use properties of Uniform distributions, sketch pictures, and use geometry to solve problems. That is, you should *not* immediately resort to calculus.
However, we show the calculus below to illustrate ideas, and because we have tackled this problem previously with Uniform probability measures and geometry in Example \@ref(exm:meeting-probspace1d).


1. We expect the height of the pdf to be constant between 0 and 60, both because her arrival time is uniform over the interval so no one value should be more likely than another, and because when we simulated values the histogram bars had roughly constant height.  See the plot below.
1. We need the area under the curve over the interval $[0, 60]$ to be 1, representing 100% probability.  If the height of the density is $c$, a constant, then the area under the curve is the area of a rectangle with base 60 (length of the interval $[0, 60]$) and height $c$.  So $c$ needs to be 1/60 for the total area to be 1.  
    However, Donny Don't hasn't specified the possible values.  It's possible that someone who sees Donny's expression would think that $f_X(100)=1/60$.  But the pdf is only 1/60 over the range $[0, 60]$; it is 0 outside of this range.  A more precise expression is
    \[
      f_X(x) =
        \begin{cases}
      1/60, & 0\le x \le 60,\\
      0, & \text{otherwise.}
      \end{cases}
    \]
    You don't necessarily always need to write "0 otherwise", but do always provide the possible values.
1. Integrate the pdf over the range $[0, 15]$.  Since the pdf has constant height, areas under the curve just correspond to areas of rectangles.
    \[
    \IP(X \le 15) = \int_0^{15} (1/60) dx = (1/60)x\Big|_{x = 0}^{x = 15} = \frac{15}{60} = 0.25
    \]
1. Integrate the pdf over the range $[45, 60]$.  
    \[
    \IP(X \ge 45) = \int_{45}^{60} (1/60) dx = (1/60)x\Big|_{x = 45}^{x = 60} = \frac{15}{60} = 1-0.75 = 0.25
    \]
1. We could use the previous parts, but we'll intergrate the pdf over the range $[15, 45]$.
    \[
    \IP(15 \le X \le 45) = \int_{15}^{45} (1/60) dx = (1/60)x\Big|_{x = 15}^{x = 45} = \frac{30}{60} = 0.75 -0.25 = 0.5
    \].
1. Integrate the pdf over the range $[15, 15 + 1]$.  
    \[
    \IP(15 \le X \le 15 + 1) = (1/60)(1) = 1/60
    \]
1. Integrate the pdf over the range $[15, 15 + 1/60]$.  
    \[
    \IP(15 \le X \le 15 + 1/60) = (1/60)(1/60) = 1/3600
    \]
1. $\IP(X = 15) = 0$. Integrate the pdf over the range $[15, 15]$.  The region under the curve at this single point corresponds to a line segment which has 0 area. 
    \[
    \IP(X = 15) = \int_{15}^{15} (1/60) dx = 0
    \]

```

(ref:cap-uniform-pdf-plot) The pdf of a Uniform(0, 60) distribution. The blue line represents the pdf. The shaded orange region represents the probability of the interval [15, 45].

```{r uniform-pdf-plot, echo = FALSE, fig.cap="(ref:cap-uniform-pdf-plot)"}
ggplot(data.frame(x = c(0, 1)), aes(x)) +
  stat_function(fun = dunif, color = "skyblue", size = 2) + 
  stat_function(fun = dunif, 
                xlim = c(0.25, 0.75),
                geom = "area",
                fill = "orange", alpha = 0.2) +
  scale_x_continuous(breaks = seq(0, 1, 0.25), labels = seq(0, 1, 0.25) * 60) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1),
                     breaks = seq(0, 1, 0.25), labels = round(seq(0, 1, 0.25) / 60, 4)) +
  labs(x = "x",
       y = "Density") +
  theme_classic()
```

```{example uniform-prob-zero}

Suppose that SAT Math scores follow a Uniform(200, 800) distribution.  Let $U$ be the Math score for a randomly selected student.

```

1.  Identify $f_U$, the pdf of $U$.
2.  Donny Dont says that the probability that $U$ is 500 is 1/600. Do you agree? If not, explain why not.
3.  While modeling SAT Math score as a continuous random variable might be mathematically convenient, it's not entirely practical. Suppose that the range of values $[495, 505)$ corresponds to students who actually score 500. Find $\IP(495 \le X < 505)$.

```{solution uniform-density-prob-zero-sol}
to Example \@ref(exm:uniform-prob-zero)
```

```{asis, fold.chunk = TRUE}

1. The density still has constant height.  But now the height has to be 1/600 so that the total area under the pdf over the range of possible values $[200, 800]$ is 1.  So $f_U(u) = \frac{1}{600}, 200<u<800$ (and $f_U(u)=0$ otherwise).
1. It is true that $f_U(500)=1/600$.  However, $f_U(500)$ is NOT $\IP(U=500)$. The density (height) at a particular point is not the probability of anything.  Probabilities are determined by *integrating* the density. The "area" under the curve for the region $[500,500]$ is just a line segment, which has area 0, so $\IP(U=500)=0$.  Integrating, $\int_{500}^{500}(1/600)du=0$.  More on this point below.
1. $\IP(495 \le U < 505)=(505-495)(1/600) = 1/60$.  The integral $\int_{495}^{505}(1/600)du$ corresponds to the area of a rectangle with base $505-495$ and height 1/600, so the area is 1/60.

```

```{definition, def-uniform-pdf}
A continuous random variable $X$ has a **Uniform distribution** with parameters $a$ and $b$, with $a<b$, if its probability density function $f_X$ satisfies
\begin{align*}
f_X(x) & \propto \text{constant}, \quad & & a<x<b\\
& = \frac{1}{b-a}, \quad & &  a<x<b.
\end{align*}
If $X$ has a Uniform($a$, $b$) distribution then
\begin{align*}
\text{Long run average value of $X$} & = \frac{a+b}{2}\\
\text{Variance of $X$} & = \frac{|b-a|^2}{12}\\
\text{SD of $X$} & = \frac{|b-a|}{\sqrt{12}}
\end{align*}

```

The long run average value of a random variable that has a Uniform($a$, $b$) distribution is the midpoint of the range of possible values.
The degree of variability of a Uniform distribution is determined by the length of the interval.
Why is the standard deviation equal to the length of the interval multiplied by $0.289 \approx 1/\sqrt{12}$?
Since the deviations from the mean (midpoint) range uniformly from 0 to half the length of the interval, we might expect the average deviation to be about 0.25 times the length of the interval.
The factor $0.289 \approx 1/\sqrt{12}$, which results from the process of taking the square root of the average of the squared deviations, is not too far from 0.25.

The "standard" Uniform distribution is the Uniform(0, 1) distribution represented by the spinner in Figure \@ref(fig:uniform-spinner).
If $U$ has a Uniform(0, 1) distribution then $X = a + (b-a)U$ has a Uniform($a$, $b$) distribution.
Notice that the linear rescaling of $U$ to $a + (b-a)U$ does not change the basic shape of the distribution, just the region of possible values.
Therefore, we can construct a spinner for any Uniform distribution by starting with the Uniform(0, 1) and linearly rescaling the values on the spinner axis.

### Density is not probability

Plugging a value into the pdf of a continuous random variable does *not* provide a probability.
The pdf itself does not provide probabilities directly; instead a pdf must be integrated to find probabilities.

**The probability that a continuous random variable** $X$ equals any particular value is 0.
That is, if $X$ is continuous then $\IP(X=x)=0$ for all $x$.
Therefore, for a *continuous* random variable[^distribution-4], $\IP(X\le x) = \IP(X<x)$, etc.
A continuous random variable can take uncountably many distinct values.
Simulating values of a continuous random variable corresponds to an idealized spinner with an infinitely precise needle which can land on any value in a continuous scale.

[^distribution-4]: The same is *not* true for discrete random variables.
    For example, if $X$ is the number of heads in three flips of a fair coin then $\IP(X<1)= \IP(X=0)=1/8$ but $\IP(X \le 1)=\IP(X=0)+\IP(X=1) = 4/8$.

In the Uniform(0, 1) case, $0.500000000\ldots$ is different than $0.50000000010\ldots$ is different than $0.500000000000001\ldots$, etc.
Consider the spinner in Figure \@ref(fig:uniform-spinner).
The spinner in the picture is only labeled in 100 increments of 0.01 each; when we spin, the probability that the needle lands closest to the 0.5 tick mark is 0.01.
But if the spinner were labeled in increments 1000 increments of 0.001, the probability of landing closest to the 0.5 tick mark is 0.001.
And with four decimal places of precision, the probability is 0.0001.
And so on.
The more precise we mark the axis, the smaller the probability the spinner lands closest to the 0.5 tick mark.
The Uniform(0, 1) density represents what happens in the limit as the spinner becomes infinitely precise.
The probability of landing closest to the 0.5 tick mark gets smaller and smaller, eventually becoming 0 in the limit.

A density is an idealized mathematical model.
In practical applications, there is some acceptable degree of precision, and events like "$X$, rounded to 4 decimal places, equals 0.5" correspond to intervals that do have positive probability.
For continuous random variables, it doesn't really make sense to talk about the probability that the random value is *equal to* a particular value.
However, we can consider the probability that a random variable is *close to* a particular value.

```{example meeting-nonuniform-pdf1}
Continuing Example \@ref(exm:meeting-pdf1), we will now we assume Regina's arrival time in $[0, 1]$ has pdf

\[
f_X(x) =
        \begin{cases}
      cx, & 0\le x \le 1,\\
      0, & \text{otherwise.}
      \end{cases}
\]

where $c$ is an appropriate constant.

Note that now we're measuring arrival time in hours (i.e., fraction of the hour after noon) instead of minutes.

```

1.  Sketch a plot of the pdf. What does this say about Regina's arival time?
2.  Find the value of $c$ and specify the pdf of $X$.
3.  Find the probability that Regina arrives before 12:15.
4.  Find the probability that Regina arrives after 12:45. How does this compare to the previous part? What does that say about Regina's arrival time?
5.  Find the probability that Regina arrives between 12:15 and 12:45.
6.  Find the probability that Regina arrives between 12:15 and 12:16.
7.  Find the probability that Regina arrives between 12:15:00 and 12:15:01.
8.  Find the probability that Regina arrives at the exact time 12:15:00 (with infinite precision).
9.  Find the probability that Regina arrives between 12:59:00 and 1:00:00. How does this compare to the probability for 12:15:00 to 12:16:00? What does that say about Regina's arrival time?
10. Find the probability that Regina arrives between 12:59:59 and 1:00:00. How does this compare to the probability for 12:15:00 to 12:15:01? What does that say about Regina's arrival time?
11. Find the probability that Regina arrives at the exact time 1:00:00 (with infinite precision).
12. Compare this example and your answers to Example \@ref(exm:meeting-nonuniform-probspace1).

```{solution meeting-nonuniform-pdf1-sol}

to Example \@ref(exm:meeting-nonuniform-pdf1)

```

```{asis, fold.chunk = TRUE}

You should always start problems like this by drawing the pdf and shading regions corresponding to probabilities.
In some cases, areas can be determine by simple geometry without needing calculus.
We have included the integrals below for completeness, but make sure you also see how the probabilities can be determined using geometry.

1.  The density increases linearly with $x$.
Regina is most likely to arrive closer to 1, and least likely to arrive close to noon (0).
1. $c=2$. The total area under the pdf must be 1. The region under the pdf is a triangle with area $(1/2)(1-0)(c)$, so $c$ must be 2 for the area to be 1.  Via integration
    \[
    1 = \int_0^1 cx dx = (c/2)x^2  \Big|_{x=0}^{x=1} = c / 2
    \]
1. Integrate the pdf over the region $[0, 0.25]$.  Since the pdf is linear, regions under the curve are triangles or trapezoids. The area corresponding to [0, 0.25] is a triangle with base (0.25 - 0), height $c(0.25) = 2(0.25)$, and area $0.5(0.25-0)(2(0.25)) = 0.25^2 = 0.0625$.
    \[
    \int_0^{0.25} 2x dx = x^2 \Bigg|_{x=0}^{x=0.25} = 0.25^2 = (1/2)(0.25 - 0)(2(0.25)) = 0.0625
    \]
1. Integrate the pdf over the region $[0.75, 1]$.
    \[
    \int_{0.75}^1 2x dx = x^2 \Bigg|_{x=0.75}^{x=1} = 1 - 0.75^2 = 0.4375
    \]
    So Regina is 7 times more likely to arrive within 15 minutes of 1 than within 15 minutes of noon.
1. 0.5. We could integrate the pdf from 0.25 to 0.75, or just use the previous results and properties of probabilities.
1. Similar to the previous parts, the probability  is $(0.25 + 1/60)^2 - 0.25^2 = 0.0086$.
(This probability is less than what it was in the uniform case.)
1. Similar to the previous part, $(0.25 + 1/3600)^2 - 0.25^2 = 0.00014$. (This probability is less than what it was in the uniform case.)
1. The exact time 12:15:00 represents a single point the sample space, an interval of length 0. The probability that Regina arrives at the exact time 12:15:00 (with infinite precision) is 0.
1. Similar to previous parts, $1^2 - (1-1/60)^2 = 0.0331$. Notice that this one minute interval around 1:00 has a probability that is about 3.85 times larger than a one minute interval around 12:15.
1. $1^2 - (1-1/3600)^2 = 0.00056$. Notice that this one second interval around 1:00 has a probability that is about 4 times higher than a one second interval around 12:15, though both probabilities are small.
1. The exact time 1:00:00 represents a single point the sample space, an interval of length 0.  The probability that Regina arrives at the exact time 1:00:00 (with infinite precision) is 0.
1. The results are the same as those in Example \@ref(exm:meeting-nonuniform-probspace1).
In that example, the probability that Regina arrives in the interval $[0, x]$ was $x^2$, which can be obtained by integrating the pdf in this example from 0 to $x$.
Careful when you integrate; since $x$ is in the bounds of the integral you need a different dummy variable to use in $f_X$.
\[
  \IP(X \le x) = \int_0^x f_X(u) du = \int_0^x 2u du = u^2 \Bigg|_{u=x}^{u=0} = x^2
\]
We will see soon that such a function is called a *cumulative distribution function (cdf)*.
```

(ref:cap-meeting-nonuniform-pdf) The probability density function from Example \@ref(exm:meeting-nonuniform-pdf1).

```{python, meeting-nonuniform-pdf-plot, echo=FALSE, fig.cap="(ref:cap-meeting-nonuniform-pdf)"}

Beta(2, 1).plot()
plt.show()

```

In the previous example, we specified the general shape of the pdf, then found the constant that made the total area under the curve equal to 1.
In general, a pdf is often defined only up to some multiplicative constant $c$, for example $f_X(x) = c\times(\text{some function of }x)$, or $f_X(x) \propto \text{some function of }x$.
The constant $c$ does not affect the shape of the distribution as a function of $x$, only the scale on the density axis.
The absolute scaling on the density axis is somewhat irrelevant; it is whatever it needs to be to provide the proper *area*.
In particular, the total area under the pdf must be 1.
The scaling constant is determined by the requirement that $\int_{-\infty}^\infty f_X(x) dx = 1$.

What's more important about the pdf is *relative* heights.
In the previous example the density at 1, $f_X(1) = c$, was 4 times greater than than density at 0.25, $f_X(0.25) = 0.25c$.
This was the reason why the probability of arriving close to 1 was about 4 times greater than the probability of arriving close to 12:15 (time 0.25).
The ratio of the densities at these two points could be computed without knowing the value of $c$.

Compare the pdf in Example \@ref(fig:meeting-nonuniform-pdf-plot) with the probabilities under the non-uniform measure in Figure \@ref(fig:arrival-time-probmeasure).
The pdf at a particular possible value $x$ is related to the probability that the random value takes a value close to that value $x$.

```{example exponential-pdf-closeto}

Let $X$ be a random variable with the "Exponential(1)" distribution, illustrated by the smooth curve in Figure \@ref(fig:exponential-smooth-histogram).  Then the pdf of $f_X$ is

\[
f_X(x) =
\begin{cases}
e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]

```

1.  Compute $\IP(X = 1)$.
2.  Without integrating, approximate the probability that $X$ rounded to two decimal places is 1.
3.  Without integrating, approximate the probability that $X$ rounded to two decimal places is 1.7.
4.  Find and interpret the ratio of the probabilities from the two previous parts. How could we have obtained this ratio from the pdf?

```{solution exponential-pdf-closeto-sol}

to Example \@ref(exm:exponential-pdf-closeto)

```

```{asis, fold.chunk = TRUE}


1. $\IP(X = 1)=0$, since $X$ is continuous.
1. Over a short region around 1, the area under the curve can be approximated by the area of a rectangle with height $f_X(1)$:
    \[
      \IP(0.995<X<1.005)\approx f_X(1)(1.005 - 0.995)=e^{-1}(0.01)\approx 0.00367879.
      \]
    See the illustration below. This provides a pretty good approximation of the true integral^[Reporting so many decimal places is unnecessary, and provides a false sense of precision.  All of these idealized mathematical models are at best approximately true in practice.  However, we provide the extra decimal places here to compare the approximation with the "exact" calculation.] $\int_{0.995}^{1.005} e^{-x}dx = e^{-0.995}-e^{-1.005}\approx 0.00367881$.
1. Over a short region around 1.7, the area under the curve can be approximated by the area of a rectangle with height $f_X(1.7)$:
    \[
    \IP(1.695<X<1.705)\approx f_X(1.7)(1.705 - 1.695)=e^{-1.7}(0.01)\approx 0.001826835.
    \]
    This provides a pretty good approximation of the integral  $\int_{1.695}^{1.705} e^{-x}dx = e^{-1.695}-e^{-1.705}\approx 0.001826843$.
1. Compare the rectangle-based approximations 
    \[
    \frac{\IP(1 - 0.005 <X < 1 + 0.005)}{\IP(1.7 - 0.005 <X < 1.7 + 0.005)} \approx 2.01 \approx \frac{e^{-1}(0.01)}{e^{-1.7}(0.01)} = \frac{e^{-1}}{e^{-1.7}} = \frac{f_X(1)}{f_X(1.7)}  
    \]
    The probability that $X$ is "close to" 1 is about 2 times greater than the probability that $X$ is "close to" 1.7.  This ratio is determined by the ratio of the densities at 1 and 1.7.

```

(ref:cap-exponential-pdf-area2) Illustration of $\IP(0.995<X<1.005)$ (orange) and $\IP(1.695<X<1.705)$ (blue) for $X$ with an Exponential(1) distribution. The plot illustrates how the probability that $X$ is "close to" $x$ can be approximated by the area of a rectangle with height equal to the density at $x$, $f_X(x)$. The density height at $x = 1$ is roughly twice as large than the density height at $x = 1.7$ so the probability that $X$ is "close to" 1 is (roughly) twice as large as the probability that $X$ is "close to" 1.7.

```{r exponential-pdf-area2, echo=FALSE, fig.cap="(ref:cap-exponential-pdf-area2)"}

x<-seq(0, 5, 0.001)
y<-dexp(x,1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, 1), expand=c(0, 0)) +
  theme_classic() +
  xlab(expression(x)) +
  ylab(expression(f[X](x))) +
#  ggtitle(paste("P(1<X<2.5) for X ~ Exponential(1)")) +
  geom_rect(data=xddf, mapping=aes(xmin=1-0.1/2, xmax=1+0.1/2,
                                   ymin=0, ymax=dexp(1, 1)),
            color="orange", fill = "orange") +
    geom_rect(data=xddf, mapping=aes(xmin=1.7-0.1/2, xmax=1.7+0.1/2,
                                   ymin=0, ymax=dexp(1.7, 1)),
            color="skyblue", fill = "skyblue") 

```

A density is an idealized mathematical model.
In practical applications, there is some acceptable degree of precision, and events like "$X$, rounded to 4 decimal places, equals 0.5" correspond to intervals that do have positive probability.
For continuous random variables, it doesn't really make sense to talk about the probability that the random value equals a particular value.
However, we can consider the probability that a random variable is close to a particular value.

To emphasize: The density $f_X(x)$ at value $x$ is *not* a probability.
Rather, the density $f_X(x)$ at value $x$ is related to the probability that the RV $X$ takes a value "close to $x$" in the following sense[^distribution-5].
$$
\IP\left(x-\frac{\epsilon}{2} \le X \le x+\frac{\epsilon}{2}\right) \approx f_X(x)\epsilon, \qquad \text{for small $\epsilon$}
$$ The quantity $\epsilon$ is a small number that represents the desired degree of precision.
For example, rounding to two decimal places corresponds to $\epsilon=0.01$.

[^distribution-5]: This is true because an integral can be approximated by a sum of the areas of many rectangles with narrow bases.
    Over a small interval of values surrounding $x$, the density shouldn't change that much, so we can estimate the area under the curve by the area of the rectangle with height $f_X(x)$ and base each equal to the length of the small interval of interest.

Technically, any particular $x$ occurs with probability 0, so it doesn't really make sense to say that some values are more likely than others.
However, a RV $X$ is more likely to take values *close to* those values that have greater density.
As we said previously, what's important about a pdf is *relative* heights.
For example, if $f_X(x_2)= 2f_X(x_1)$ then $X$ is roughly "twice as likely to be near $x_2$ than to be near $x_1$" in the above sense.
$$
\frac{f_X(x_2)}{f_X(x_1)} = \frac{f_X(x_2)\epsilon}{f_X(x_1)\epsilon} \approx  \frac{\IP\left(x_2-\frac{\epsilon}{2} \le X \le x_2+\frac{\epsilon}{2}\right)}{\IP\left(x_1-\frac{\epsilon}{2} \le X \le x_1+\frac{\epsilon}{2}\right)}
$$

### Exponential distributions

Exponential distributions are often used to model the *waiting times* between events in a random process that occurs continuously over time.

```{example, exponential-quakes}
Suppose that we model the waiting time, measured continuously in hours, from now until the next earthquake (of any magnitude) occurs in southern CA  as a continuous random variable $X$ with pdf
\[
f_X(x) = 2 e^{-2x}, \; x \ge0
\]
This is the pdf of the "Exponential(2)" distribution.
```

1.  Sketch the pdf of $X$. What does this tell you about waiting times?
2.  Without doing any integration, approximate the probability that $X$ rounded to the nearest minute is 0.5 hours.
3.  Without doing any integration determine how much more likely that $X$ rounded to the nearest minute is to be 0.5 than 1.5.
4.  Compute and interpret $\IP(X > 0.25)$.
5.  Compute and interpret $\IP(X \le 3)$.
6.  Compute and interpret the 25th percentile of $X$.
7.  Compute and interpret the 50th percentile of $X$.
8.  Compute and interpret the 75th percentile of $X$.
9.  How do the values from the three previous parts compare to the percentiles from the Exponential(1) distribution depicted in \@ref(fig:exponential1-spinner)? Suggest a method for simulating values of $X$ using the Exponential(1) spinner.
10. Use simulation to approximate the long run average value of $X$. Interpret this value. At what *rate* do earthquakes tend to occur?
11. Use simulation to approximate the standard deviation of $X$. What do you notice?

```{solution, exponential-quakes-sol}
to Example \@ref(exm:exponential-quakes)
```

```{asis, fold.chunk = TRUE}

1. See simulation below for plots.  Waiting times near 0 are most likely, and density decreases exponentially as waiting time increases.
1. Remember, the density at $x=0.5$ is not a probability, but it is related to the probability that $X$ takes a value close to $x=0.5$. The approximate probability that $X$ rounded to the nearest minute is 0.5 hours is 
\[
f_X(0.5)(1/60) = 2e^{-2(0.5)}(1/60) =  0.0123 
\]
1. Find the ratio of the densities at 0.5 and 1.5:
\[
\frac{f_X(0.5)}{f_X(1.5)} = \frac{2e^{-2(0.5)}}{2e^{-2(1.5)}} = \frac{e^{-2(0.5)}}{e^{-2(1.5)}} \approx 7.4.
\]
$X$ rounded to the nearest minute is about 7.4 times more likely to be 0.5 than 1.5.
A waiting time close to half an hour is about 7.4 times more likely than a waiting time close to 1.5 hours.
1. $\IP(X > 0.25) = \int_{0.25}^\infty 2e^{-2x}dx = e^{-2(0.25)}=0.606$.
Careful: this is *not* $2e^{-2(0.25)}$.
According to this model, the waiting time between earthquakes is more than 15 minutes for about 60% of earthquakes.
1. $\IP(X \le 3) = \int_0^3 2e^{-2x}dx = 1-e^{-2(3)}=0.9975$.  While any value greater than 0 is possible in principle, the probability that $X$ takes a really large value is small.  About 99.75% of earthquakes happen within 3 hours of the previous earthquake.
1. We want to find $x>0$ such that $\IP(X \le x) =0.25$:
$$
0.25 = \IP(X \le x) = \int_0^{x} 2e^{-2u}du = 1 - e^{-2x}
$$
Solve to get $x = -0.5\log(1-0.25) = 0.5(0.288)=0.144$. For 25% of earthquakes, the next earthquake happens within 0.144 hours (8.6 minutes).
Constructing a spinner, the value $0.5(0.288)=0.144$ goes at 3 o'clock on the spinner axis.
1. We want to find $x>0$ such that $\IP(X \le x) =0.5$:
$$
0.5 = \IP(X \le x) = \int_0^{x} 2e^{-2u}du = 1 - e^{-2x}
$$
Solve to get $x = -0.5\log(1-0.5) = 0.5(0.693)=0.347$. For 50% of earthquakes, the next earthquake happens within 0.347 hours (20.8 minutes).
Constructing a spinner, the value $0.5(0.693)=0.347$ goes at 6 o'clock on the spinner axis.
1. We want to find $x>0$ such that $\IP(X \le x) =0.75$:
$$
0.75 = \IP(X \le x) = \int_0^{x} 2e^{-2u}du = 1 - e^{-2x}
$$
Solve to get $x = -0.5\log(1-0.75) = 0.5(1.386)=0.693$. For 75% of earthquakes, the next earthquake happens within 0.693 hours (41.6 minutes).
Constructing a spinner, the value $0.5(1.386)=0.693$ goes at 9 o'clock on the spinner axis.
1. We could construct a spinner corresponding to an Exponential(2) distribution.
But the previous parts suggest that the values on the axis of the Exponential(2) are the values on the Exponential(1) spinner multiplied by 1/2.
For example, for an Exponential(1) distribution 25% of values are less than 0.288, while for an Exponential(2) distribution 25% of values are less than $0.5(0.288) = 0.144$.
Therefore, to simulate a value from an Exponential(2) distribution we can simulate a value from an Exponential(1) distribution and multiply the result by 1/2.
1. See simulation results below.
The simulated average is about 0.5 hours.
According to this model, the average waiting time between earthquakes is 0.5 hours.
That is, earthquakes tend to occur at rate 2 earthquakes per hour on average; this is what the parameter 2 in the pdf represents.
1. The simulated standard deviation is also about 0.5, the same as the long run average value.


```

Below we simulate values from an Exponential distribution with rate parameter 2 and compare the simulated values to the theoretical results.
In Symbulate `.cdf()` returns $\le$ probabilities; for example `Exponential(2).cdf(3)` returns the probability that a random variable with an Exponential(2) distribution takes a value $\le 3$.
(We will discuss cdfs in more detail in the next section.)

```{python}

X = RV(Exponential(rate = 2))

x = X.sim(10000)

x

```

```{python, eval = FALSE}

x.plot() # plot the simulated values

Exponential(rate = 2).plot() # plot the theoretical Exponential(2) pdf

```

```{python, echo = FALSE}
plt.figure()
x.plot()

Exponential(rate = 2).plot()
plt.show()

```

```{python}

x.count_gt(0.25) / x.count(), 1 - Exponential(rate = 2).cdf(0.25)

```

```{python}

x.count_lt(3) / x.count(), Exponential(rate = 2).cdf(3)

```

```{python}

x.mean(), Exponential(rate = 2).mean()

```

```{python}

x.sd(), Exponential(rate = 2).sd()

```

```{definition, exponential-pdf-def}

A continuous random variable $X$ has an **Exponential distribution** with *rate* parameter^[Exponential distributions are sometimes parametrized directly by their mean $1/\lambda$, instead of the rate parameter $\lambda$.  The mean $1/\lambda$ is sometimes called the scale parameter.] $\lambda>0$ if its pdf is
\[
f_X(x) =
\begin{cases}\lambda e^{-\lambda x}, & x \ge 0,\\
0, & \text{otherwise}
\end{cases}
\]
If $X$ has an Exponential($\lambda$) distribution then
\begin{align*}
\IP(X>x)  & = e^{-\lambda x}, \quad x\ge 0\\
\text{Long run average of $X$} & = \frac{1}{\lambda}\\
\text{Standard deviation of $X$} & = \frac{1}{\lambda}
\end{align*}

```

Exponential distributions are often used to model the *waiting time* in a random process until some event occurs.

-   $\lambda$ is the average *rate* at which events occur over time (e.g., 2 per hour)
-   $1/\lambda$ is the mean time between events (e.g., 1/2 hour)

An Exponential density has a peak at 0 and then decreases exponentially as $x$ increases.
The function $e^{-\lambda x}$ defines the shape of the density and the rate at which the density decreases.
The constant $\lambda$, which defines the density at $x=0$, simply rescales the vertical axis so that the total area under the pdf is 1.

(ref:exponential-densities-caption) Exponential densities with rate parameter $\lambda$.

```{python, exponential-densities, echo = FALSE, fig.cap='(ref:exponential-densities-caption)'}
plt.figure()
rates = [0.5, 1, 2]
for rate in rates:
    Exponential(rate).plot()
    
plt.legend(['$\lambda=$' + str(i) for i in rates]);
plt.xlim(0, 8);
plt.show()
```

The "standard" Exponential distribution is the Exponential(1) distribution, with rate parameter 1 and long run average 1.
If $X$ has an Exponential(1) distribution and $\lambda>0$ is a constant then $X/\lambda$ has an Exponential($\lambda$) distribution.
Recall that an Exponential($\lambda$) distribution has long run average value $1/\lambda$.
Values from an Exponential(1) distribution will have long run average value 1, so if we multiply all the values by $1/\lambda$ then the transformed values will have long run average value $1/\lambda$.
Multiplying by the constant $1/\lambda$ does not change the shape of the distribution; it just relabels the values on the variable axis.

```{python}
U = RV(Exponential(1))

X = (1 / 2) * U
```

```{python, eval = FALSE}
X.sim(10000).plot()  # simulated distribution of X

Exponential(rate = 2).plot() # theoretical distribution

```

```{python, echo = FALSE}
plt.figure()
X.sim(10000).plot()  # simulated distribution of X

Exponential(rate = 2).plot() # theoretical distribution
plt.show()

```

## Cumulative distribution functions

While pmfs and pdfs play analogous roles for discrete and continuous random variables, respectively, they do behave differently; pmfs provide probabilities directly, but pdfs do not.
It is convenient to have one object that describes a distribution in the same way, regardless of the type of variable, and which returns probabilities directly.
This object is called the *cumulative distribution function (cdf)*.
While the definition might seem strange at first, you have probably already had experience with cumulative distribution functions.

```{example height-percentile}

Maggie and Seamus are babies who have just turned one.
At their one-year visits to their pediatrician:

- Maggie is 76cm tall and in the [75th percentile of height for girls](https://www.cdc.gov/growthcharts/data/set1clinical/cj41l018.pdf).
- Seamus is 72cm tall and in the [10th percentile of height for boys](https://www.cdc.gov/growthcharts/data/set1clinical/cj41l017.pdf).

Explain what these percentiles mean.

```

```{solution height-percentile-sol}
to Example \@ref(exm:height-percentile)
```

```{asis, fold.chunk = TRUE}

- 75% of one-year-old girls (in the U.S.) are less than 76cm tall, and 25% are more than 76cm tall. So Maggie is taller than 75% of one-year-old girls.
- 10% of one-year-old boys (in the U.S.) are less than 72cm tall, and 90% are more than 72cm tall. So the wee baby Seamus is taller than 10% of one-year-old boys.

```

<!-- ```{example sat-percentile} -->

<!-- According to [data on students who took the SAT in 2018-2019](https://collegereadiness.collegeboard.org/pdf/understanding-sat-scores.pdf), 1400 was the 94th percentile of SAT scores, while 1000 was the 40th percentile.  How do you interpret these percentiles? -->

<!-- ``` -->

<!-- ```{solution sat-percentile-sol} -->

<!-- to Example @ref(exm:sat-percentile) -->

<!-- ``` -->

<!-- ```{asis, fold.chunk = TRUE} -->

<!-- Interpretation: 94% of SAT takers scores at or below 1400, and 6% of SAT takers score greater than 1400.  Similarly, 40% of SAT takers scores at or below 1000, and 60% of SAT takers score greater than 1000.  -->

<!-- ``` -->

Roughly, the value $x$ is the $p$th percentile of a distribution of a random variable $X$ if $p$ percent of values of the variable are less than or equal to $x$: $\IP(X\le x) = p$.
The *cumulative distribution function (cdf)* of a random variable fills in the blank for any given $x$: $x$ is the (blank) percentile.
That is, for an input $x$, the cdf outputs $\IP(X\le x)$.

```{definition, cdf}

The **cumulative distribution function (cdf)** (of a random variable $X$ defined on a probability space with probability measure $\IP$)
is the function, $F_X: \mathbb{R}\mapsto[0,1]$, defined by
$F_X(x) = \IP(X\le x)$. A cdf is defined for all real numbers $x$
regardless of whether $x$ is a possible value of $X$.

```

```{example sat-percentile2}

According to [data on students who took the SAT in 2018-2019](https://collegereadiness.collegeboard.org/pdf/understanding-sat-scores.pdf), 1400 was the 94th percentile of SAT scores, while 1000 was the 40th percentile.
Let $X$ be the SAT score of a randomly selected
student (from this cohort), and let $F_X$ be the cdf of $X$.
Evaluate the cdf for each of the following.
For the purposes of this exercise, interpret these quantities in terms of actual SAT scores, which take values in 400, 410, 420, $\ldots$, 1590, 1600. 

```

1.  $F_X(1400)$
2.  $F_X(1405)$
3.  $F_X(1000)$
4.  $F_X(1003.7)$
5.  $F_X(-3.1)$
6.  $F_X(390)$
7.  $F_X(399.5)$
8.  $F_X(1600)$
9.  $F_X(1610)$
10. $F_X(2307.4)$
11. $F_X(1400)-F_X(1000)$

```{solution sat-percentile2-sol}
to Example \@ref(exm:sat-percentile2)
```

```{asis, fold.chunk = TRUE}

1. $F_X(1400)=0.94$. We are told that $\IP(X \le 1400) = 0.94$.
1. $F_X(1405) = 0.94$. In terms of reall SAT scores, $\IP(X \le 1405) = \IP(X\le 1400)$.
1. $F_X(1000) = 0.40$. We are told that $\IP(X \le 1000) = 0.40$.
1. $F_X(1003.7) = 0.40$. In terms of reall SAT scores, $\IP(X \le 1003.7) = \IP(X\le 1000)$.
1. $F_X(-3.1)=0$. The smallest possible score is 400.
1. $F_X(390)=0$. The smallest possible score is 400.
1. $F_X(399.5)= 0$. The smallest possible score is 400.
1. $F_X(1600) = 1$. The largest possible score is 1600, so 100% of students score no more than 1600.
1. $F_X(1610) = 1$. The largest possible score is 1600.
1. $F_X(2307.4) = 1$. The largest possible score is 1600.
1. $0.54 = F_X(1400)-F_X(1000)=\IP(X\le 1400) - \IP(X \le 1000) = \IP(1000 < X \le 1400)$.  54% of SAT takers score greater than 1000 but at most 1400.

```

To understand a cdf, imagine a spinner for a particular distribution.
Suppose a "second hand" starts at the smallest possible value ("12:00") and sweeps clockwise around the spinner.
The second hand sweeps out area as it goes; when the second hand is pointing at $x$, the area that it has swept through represents $\IP(X\le x)$.
The cdf records the values of $F_X(x) = \IP(X\le x)$ as the second hand moves along and points to different values of $x$.

While a cdf is defined the same way for both discrete and continuous random variables, it is probably best understood in terms of continuous random variables.
Remember that for a continuous random variable, $\IP(X\le x)$ is the area under the density curve over the interval $(-\infty, x]$ (remember the density might be 0 for some values in this range).
Imagine plotting a density curve and adding a vertical line at $x$; $\IP(X\le x)$ is the area under the curve to left of this line.
The cdf is constructed by moving the vertical line from left to right, from smaller to larger values of $x$, and recording the area under the curve to the left of the line, $F_X(x) = \IP(X\le x)$, as $x$ varies.

<!-- See Figure @ref(fig:cdf-illustration) for an illustration. -->

See the figure below for an illustration.
The shaded area in the plot on the left represents $F_X(x)=\IP(X\le x)$, which is about 0.6 in this example.
This area is represented by the $(x, F_X(x))$ point in the cdf plot in the middle.
The cdf plot in the middle represents the result of recording the area in the plot on the left for all values of $x$.
The plot on the right displays the spinner corresponding to the pdf on the left.

<!-- (ref:cap-cdf-picture) Illustration of a pdf (left) and the corresponding cdf (middle). The cdf at $x$ is highlighted in the plots and shaded spinner (right). -->

```{r cdf-illustration, echo=FALSE, fig.show="hold", out.width="33%"}


x0 = qgamma(0.6, shape=4, rate=1)
x<-seq(0, 15, 0.001)
y<-dgamma(x,shape=4, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>0 & x<x0),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0)) +
  theme_classic() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  scale_x_continuous(breaks = x0, labels = expression(x)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(f[X](x)))
#  ggtitle(paste("Shaded area is ", expression(F(x)),", the cdf of X evaluated at x", sep=""))

y<-pgamma(x,shape=4, rate=1)
y0 = pgamma(x0,shape=4, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0.01)) +
  theme_classic() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_x_continuous(breaks = x0, labels = expression(x)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(F[X](x))) +
#  ggtitle(paste("Shaded area is ", expression(F(x)),", the cdf of X evaluated at x", sep="")) +
  geom_segment(x=x0, xend=x0, y=0, yend=y0, size = 2, linetype=2, colour="orange") +
  geom_segment(x=0, xend=x0, y=y0, yend=y0, size = 2, linetype=2, colour="orange")



p = 0.4

df <- data.frame(
  group = c(">x", "<= x"),
  value = c(1-p, p)
  )

cdfpie <- ggplot(df, aes(x="", y=value, fill=group)) +
  geom_bar(width = 1, stat = "identity", color='black') +
    blank_theme +
  scale_y_continuous(breaks=c(1-p, p), labels=c("x", "")) +
  coord_polar("y") +
  scale_fill_manual(values=c("white", "orange")) +
  geom_text(aes(y = value/2 + c(0, cumsum(value)[-length(value)]), 
            label = c("P(X <= x)", "")), size=5) +
  theme(legend.position = "none")

cdfpie



```

```{example exponential-cdf-calcs}

Let $X$ be a random variable with the Exponential(1) distribution.  Then the pdf of $f_X$ is

\[
f_X(x) =
\begin{cases}
e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]

```

1.  Find the cdf of $X$, and sketch a plot of it.
2.  Evaluate and interpret $F_X(1)$, and draw a picture depicting it.
3.  Evaluate and interpret $F_X(2)-F_X(1)$, and draw a picture depicting it.
4.  Evaluate and interpret $F_X(2)$, and draw a picture depicting it.
5.  Find $\IP(1 < X < 2.5)$ without integrating again.
6.  Suppose we had been given the cdf instead of the pdf. How could we find the pdf?

```{solution exponential-cdf-calcs-sol}
to Example \@ref(exm:exponential-cdf-calcs)
```

```{asis, fold.chunk = TRUE}

1. $F_X(x)=0$ for $x<0$. For $x>0$ we integrate the density^[Here $x$ represents a particular value of interest, so we use a different dummy variable, $u$,  in the integrand.]
\[
F_X(x) = \IP(X \le x) = \int_0^x e^{-u} du = 1 - e^{-x}
\]
So the cdf of $X$ is
\[
F_X(x) =
\begin{cases}
1 - e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]
1. $F_X(1)=\IP(X\le 1) = 1-e^{-1}\approx 0.632$.  This is represented by the area under the Exponential(1) density curve from 0 to 1, 63.2%.
1. This is represented by the area under the Exponential(1) density curve from 1 to 2, 23.3%.
\[
F_X(2)- F_X(1)=\IP(X\le 2) - \IP(X \le 1) =\IP(1<X\le 2)= (1-e^{-2})-(1-e^{-1})=e^{-1}-e^{-2}\approx 0.233
\]
1. $F_X(2)=\IP(X\le 2) = 1-e^{-2}\approx 0.865$.  This is represented by the area under the Exponential(1) density curve from 0 to 1, 63.2%+23.3% = 86.5%.
1.
\[
\IP(1 < X < 2.5) = \IP(X\le 2.5) - \IP(X \le 1) = F_X(2.5) - F_X(1) =  (1-e^{-2.5})-(1-e^{-1})=e^{-1}-e^{-2.5}\approx 0.286
\]
1. Since the cdf is obtained by integrating the pdf, the pdf if obtained by differentiating the cdf.  Differentiate the cdf $F_X(x)=1-e^{-x},\ x>0$ with respect to its argument $x$ to obtain the pdf $f_X(x) = e^{-x},\ x>0$.

```

(ref:cap-exponential-cdf-illustration) Illustration of the pdf (left) and the cdf (right) for the Exponential(1) distribution represented by the spinner in Figure \@ref(fig:exponential1-spinner). The shaded area in the plot on the left represents $F_X(1)=\IP(X\le 1)$, which is $1-e^{-1}\approx0.632$. This area is represented by the $(1, F_X(1))$ point in the cdf plot on the right, and in the region from 0 to 1 in the spinner in Figure \@ref(fig:exponential1-spinner).

```{r exponential-cdf-illustration, echo=FALSE, fig.cap="(ref:cap-exponential-cdf-illustration)", out.width='50%', fig.show='hold'}
x0 = 1
xmax = 6
x<-seq(0, xmax, 0.001)
y<-dgamma(x,shape=1, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>0 & x<x0),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0)) +
  theme_classic() +
  scale_x_continuous(limits=c(0, xmax), breaks=0:xmax, expand=c(0, 0)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(f[X](x)))


y<-pgamma(x,shape=1, rate=1)
y0 = pgamma(x0,shape=1, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0.01)) +
  theme_classic() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_x_continuous(limits=c(0, xmax), breaks=0:xmax, expand=c(0, 0)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(F[X](x))) +
#  ggtitle(paste("Shaded area is ", expression(F(x)),", the cdf of X evaluated at x", sep="")) +
  geom_segment(x=x0, xend=x0, y=0, yend=y0, size = 2, linetype=2, colour="orange") +
  geom_segment(x=0, xend=x0, y=y0, yend=y0, size = 2, linetype=2, colour="orange")

```

For named distributions, we can evaluate the cdf in Symbulate using the `.cdf()` method.

```{python}

Exponential(1).cdf(1)

```

```{python}

Exponential(1).cdf([-1, 2, 2.5])

```

**For continuous random variables, think of the cdf as a "generic integral".** Rather than integrating from scratch to find $\IP(X < 1)$, $\IP(X < 2)$, $\IP(1 < X< 2)$, etc, the integral is computed once for a generic $x$ and then evaluated to find probabilities for specific values of $x$, $F_X(1)$, $F_X(2)$, $F_X(2)-F_X(1)$, etc.

We integrate the pdf to find the cdf, and we differentiate the cdf to find the pdf.
If $X$ is a continuous random variable with cdf $F_X$ then its pdf if $f_X = F'_X$.

```{example, meeting-nonuniform-cdf1}
Recall Example \@ref(exm:meeting-nonuniform-probspace1).  Let $X$ be Regina's arrival time.

```

1.  Find the cdf of $X$.
2.  Find the pdf of $X$.

```{solution meeting-nonuniform-cdf1-sol}

to Example \@ref(exm:meeting-nonuniform-cdf1)

```

```{asis, fold.chunk = TRUE}

1. The cdf is provided by the setup: $F_X(x) = x^2, 0<x<1$.  (And $F_X(x) = 0, x <0$, $F_X(x)=1, x > 1$)
1.  Differentiate the cdf with respect to $x$.  $f_X(x) = F'_X(x) = 2x$, $0<x<1$.  This is the pdf in Example \@ref(exm:meeting-nonuniform-pdf1).

```

For any random variable $X$ with cdf $F_X$ $$
F_X(b) - F_X(a) = \IP(a<X \le b)
$$ Note that whether the inequalities in the above event are strict ($<$) or not ($\le$) matters for discrete random variables, but not for continuous.

```{example binomial-cdf}

Let $X$ be the number of heads in 3 flips of a fair coin.  

```

1.  Find the cdf of $X$ and sketch a plot of it.
2.  Let $Y$ be the number of tails in 3 flips of a fair coin. Find the cdf of $Y$.

```{solution binomial-cdf-sol}
to Example \@ref(exm:binomial-cdf)
```

```{asis, fold.chunk = TRUE}

1. See Figure \@ref(fig:binomial-cdf-illustration). $X$ takes values 0, 1, 2, 3, with respective probabilities 1/8, 3/8, 3/8, 1/8.  We sum these probabilities to find the cdf.  For example, $F_X(0) = \IP(X\le 0) = 1/8$, $F_X(1)=\IP(X\le 1) = \IP(X=0) + \IP(X=1) = 1/8+3/8 = 0.5$.  Remember that $x$ is defined for any value of $x$, for example $F_X(1.5) = \IP(X\le 1.5)= \IP(X=0) + \IP(X=1)=0.5$. The cdf is a step function, which is flat for impossible values of $x$ and jumps at possible values $x$ with the jump size at $x$ equal to the value of the pmf at $x$.
\[
F_X(x) =
\begin{cases}
0, & x<0,\\
1/8, & 0\le x<1,\\
4/8, & 1\le x<2,\\
7/8, & 2\le x<3,\\
1, & x\ge 3.
\end{cases}
\]
1. The cdf describes a distribution.  Since $X$ and $Y$ have the same distribution, they will have the same cdf.  The only difference would be labeling; we would call the cdf of $Y$, $F_Y$, and the argument of this function would typically (but not necessarily) be denoted $y$.

```

(ref:cap-binomial-cdf-illustration) Illustration of the pmf (left) and the cdf (right) of $X$, the number of heads in 3 flips of a fair coin. The possible values of $X$ are 0, 1, 2, 3. The pmf on the left displays the probabilities of these values, $p_X(x) = \IP(X=x)$. The cdf on the right displays $F_X(x)=\IP(X\le x)$. The cdf is flat between possible values, and jumps at the possible values, with the jumps sizes given by the pmf. (The corresponding distribution is the "Binomial(3, 0.5)" distribution.)

```{r binomial-cdf-illustration, echo=FALSE, fig.cap="(ref:cap-binomial-cdf-illustration)", out.width='50%', fig.show='hold'}

n = 3
p = 0.5
x = 0:3
xmin = -0.5
xmax = 3.5
px = dbinom(x, n, p)
Fx = pbinom(x, n, p)

plot(x, px, type='h', ylim=c(0, 1), xlim=c(xmin, xmax), ylab=expression(p[X](x)), yaxt='n', col="orange", lwd=2)
axis(2, (0:8)/8, cex.axis = 0.7)


plot(x, Fx, type='n', ylim=c(0, 1), xlim=c(xmin, xmax), ylab=expression(F[X](x)), yaxt='n')
axis(2, (0:8)/8, cex.axis = 0.7)
points(x, Fx, pch=19)
segments(x0=c(xmin, x), x1=c(x, xmax), y0=c(0, Fx), y1 = c(0, Fx))
segments(x0=x, x1=x, y0=c(0, Fx[-length(Fx)]), y1=Fx, lty=2, lwd = 2, col="orange") 
```

```{python}

Binomial(3, 0.5).cdf(2)

```

```{python}

Binomial(3, 0.5).cdf([-1, 0, 0.5, 0.99, 1, 1.1, 2.4, 2.9, 3, 3.1, 3.9999, 4, 10])

```

A few properties of cdfs

-   A cdf is defined for all values of $x$, regardless if $x$ is a possible value of the RV.
-   A cdf is a non-decreasing function[^distribution-6]: if $x_1 \le x_2$ then $F_X(x_1)\le F_X(x_2)$.
-   A cdf approaches 0 as the input approaches $-\infty$: $\lim_{x\to-\infty}F_X(x) = 0$
-   A cdf approaches 1 as the input approaches $\infty$: $\lim_{x\to\infty}F_X(x) = 1$
-   The cdf of a *discrete* random variable is a step function.
    -   The steps occur at the possible values of the random variable.
    -   The height of a particular step corresponds to the probability of that value, given by the pmf.
-   The cdf of a *continuous* random variable is a continuous function.
    -   The cdf of a *continuous* random variable is obtained by integrating the pdf, so
    -   The pdf of a *continuous* random variable is obtained by differentiating the cdf $$
        F_X' = f_X \qquad \text{if $X$ is continuous}
        $$
-   For any random variable $X$ with cdf $F_X$ $$
    F_X(b) - F_X(a) = \IP(a<X \le b)
    $$ Whether the inequalities in the above event are strict ($<$) or not ($\le$) matters for discrete random variables, but not for continuous.

[^distribution-6]: This follows from the subset rule, since if $x_1\le x_2$ then $\{X\le x_1\}\subseteq\{X\le x_2\}$

One advantage to using cdfs is that they are defined the same way ($F_X(x) = \IP(X\le x)$) for both continuous and discrete random variables.
So results stated in terms of cdfs apply for both discrete and continuous random variables.
This is a little more convenient than having two versions of every definition/result/proof: a statement for discrete RVs in terms of pmfs and a separate statement for continuous RVs in terms of pdfs.
The following definition is an example.

```{definition, samedist-def}

Random variables $X$ and $Y$ **have the same distribution** if their cdfs are the same, that is, if $F_X(u) = F_Y(u)$ for all^[Note that $u$ just represents a dummy variable, the argument of the two functions.  While we generally think of $x$ as the argument of $F_X$, that is just a convenient labeling.  Here we are checking for equality of two *functions*, so we need to use the same input for both.  That is, something like "$F_X(x) = F_Y(y)$" makes no sense because $x$ and $y$ represent different inputs.] $u\in\mathbb{R}$.

```

That is, two random variables have the same distribution if all the percentiles are the same.
While we generally think of two discrete random variables having the same distribution if they have the same pmf, and two continuous random variables having the same distribution if they have the same pdf, the above definition provides a consistent criteria for any two random variables to have the same distribution, regardless of type.

```{example mixed-cdf}
Randomly select a car insurance policy and let $X$ be the amount of claims in a year for the policy, measured in thousands of dollars, which could be 0
    if the policy has no claims. Suppose the cdf of $X$ is
$$
F_X(x) = 1-0.06e^{-x / 4.3}, \quad x\ge 0
$$
```

1.  Compute and interpret $\IP(X \le 2)$.
2.  Compute and interpret $\IP(X > 2)$.
3.  Compute $\IP(X \le -0.001)$
4.  Compute $\IP(X \le 0)$.
5.  Compute and interpret $\IP(X = 0)$. Be careful! (Hint: see the two previous parts. Draw the cdf, starting from $x < 0$ and see what happens at $x = 0$.)
6.  Compute and interpret $\IP(X > 0)$. Be careful!
7.  Compute and interpret $\IP(0 < X \le 2)$. Be careful!
8.  Is $X$ discrete, continuous, or neither? Explain.
9.  Compute and interpret $\IP(X > 2 | X > 0)$.
10. Find $\IP(X > x | X > 0)$ for $x > 0$.
11. Identify by name the conditional distribution of $X$ given $X>0$, including the values of relevant parameters.
12. Describe a "two-stage" process for simulating values of $X$.

```{solution mixed-cdf-sol}
to Example \@ref(exm:mixed-cdf)
```

```{asis, fold.chunk = TRUE}

1.  $\IP(X \le 2) = F_X(2) = 1 - 0.06e^{-2/4.3} = 0.962$. About 96.2% of policies have claims no larger than 2 thousand dollars.
1.  $\IP(X > 2) = 1 - \IP(X \le 2) = 1 - F_X(2) = 0.06e^{-2/4.3} = 0.038$. About 3.8% of policies have claims greater than 2 thousand dollars.
1.  $\IP(X \le -0.001) = F_X(-0.001) = 0$
1.  $\IP(X \le 0) = F_X(0) = 1 - 0.06e^{-0/4.3} = 1 - 0.06 = 0.94$.
1.  $F(x) = 0$ for all $x<0$ so $\IP(X < 0) = 0$. But $\IP(X\le 0) = 0.94.$ So $\IP(X = 0) = \IP(X\le 0) - \IP(X < 0) = 0.94 - 0$. About 94% of policies have no claims.
1.  $\IP(X> 0) = 1 - \IP(X \le 0) = 1 - F_X(0) = 0.06e^{-0/4.3} = 0.06.$ About 6% of policies have non-zero claims.
1.  $\IP(0 < X \le 2) = F_X(2) - F_X(0) = (1-0.06e^{-2/4.3}) - (1-0.06e^{-0/4.3}) = 0.962 - 0.94 = 0.022$.  About 2.2% of policies have non-zero claims less than 2 thousand dollars.
1.  $X$ is neither discrete or continuous; it's a mixture of both. We see that $\IP(X=0) > 0$, so $X$ is not continuous. But also any value $X \ge 0$ is possible, so it's not discrete either. Here $X$ is mixed discrete and continuous. 94% of policies have no claims, so there is a spike at 0. But among the policies that do have claims, the amounts follow a continuous distribution (here it's Exponential). If you plot the cdf, there is a jump at $x = 0$ (from 0 to 0.94) but then it is continuous for $x>0$.
1.  Use the definition of conditional probability
$$
\IP(X > 2 | X > 0) = \frac{\IP(X> 2, X>0)}{\IP(X > 0)} = \frac{\IP(X> 2)}{\IP(X > 0)} = \frac{0.06e^{-2 / 4.3}}{0.06} = e^{-2 / 4.3} = 0.628.
$$
About 62.8% *of policies that have non-zero claims* have claims greater than 2 thousand dollars.
1.  Similar to the previous part for a generic $x>0$
$$
\IP(X > x | X > 0) = \frac{\IP(X> x, X>0)}{\IP(X > 0)} = \frac{\IP(X> x)}{\IP(X > 0)} = \frac{0.06e^{-x / 4.3}}{0.06} = e^{-x / 4.3}.
$$
1.  The key is to recognize that $e^{-x/4.3}$ is the one minus  the cdf of the Exponential(1/4.3) distribution. Therefore, the conditional cdf of $X$ given $X>0$ is the cdf of the Exponential(1/4.3) distribution. That is, the conditional distribution of $X$ given $X>0$ is the Exponential(1/4.3) distribution, with rate parameter 1/4.3 and long run average 4.3 thousand dollars.
*Among policies with non-zero claims*, the distribution of claim amounts follows an Exponential(1/4.3) distribution. *Among policies with non-zero claims*, the average claim amount is 4.3 thousand dollars.
1.  First, simulate whether or not the policy has a non-zero claim: construct a spinner than lands on "no claim" with probability 0.94 and "claim" with probability 0.06. If this spinner lands on "no claim" set $X = 0$. Otherwise, simulate a value from an Exponential(1/4.3) distribution --- for example, by simulating a value from the Exponential(1) spinner and multiplying the result by 4.3 --- and let $X$ be the simulated value.
```

The random variable in the previous example is a mixed discrete and continuous random variable.
The cdf has both discrete (jumps) and continuous features.

(ref:cap-mixed-cdf-plot) Illustration of the cdf of $X$ (left) and the conditional cdf of $X$ given $X>0$ (right) in Example \@ref(exm:mixed-cdf).

```{r mixed-cdf-plot, echo=FALSE, fig.cap="(ref:cap-mixed-cdf-plot)", out.width='50%', fig.show='hold'}

x2 = seq(0.01, 20, 0.01)


plot(x2, 1 - 0.06 * exp(- x2 / 4.3), type='l', ylim=c(0, 1), xlim=c(-2, 20), ylab=expression(F[X](x)), yaxt='n', xlab = "x", main = "cdf of X", lwd = 2)
axis(2, (0:8)/8, cex.axis = 0.7)
points(0, 0, cex = 1.5)
points(0, 0.94, pch = 19, cex = 1.5)
segments(x0 = -5, x1= -0.05, y0 = 0, y1 = 0, lwd = 2)

plot(x2, 1 - exp(- x2 / 4.3), type='l', ylim=c(0, 1), xlim=c(-2, 20), ylab="P(X <= x | X > 0)", yaxt='n', xlab = expression(x), main = "Conditional cdf of X given X > 0", lwd = 2)
axis(2, (0:8)/8, cex.axis = 0.7)
segments(x0 = -5, x1= 0, y0 = 0, y1 = 0, lwd = 2)

```

The code below implements the two-stage method for simulating values of $X$ discussed in the example.
$I$ indicates if there is a claim ($I=1$) or not $(I=0)$.
$Y$ is the output of the Exponential(1/4.3) spinner.
Defining $X = IY$ reflects that if $I= 0$ then $X=0$, otherwise $X = Y$.

```{python}
I, Y = RV(BoxModel([0, 1], probs = [0.94, 0.06]) * Exponential(rate = 1 / 4.3))

X = I * Y

x = X.sim(10000)

x
```

The histogram is obscured by the large proportion of zeros.
For mixed discrete and continuous random variables like this, it is often better to summarize the discrete and continuous parts separately.

```{python, eval = FALSE}
x.plot()

```

```{python, echo = FALSE}
plt.figure()
x.plot()
plt.show()

```

```{python}
x.count_eq(0) / x.count()
```

```{python}
x.count_gt(2) / x.count()
```

```{python}
x.mean()
```

Now we condition on policies that have non-zero claims.
Among the policies with non-zero claims, the claim amounts follow an Exponential(1/4.3) distribution.

```{python}
x_given_not0 = (X | (X > 0) ).sim(10000)

x_given_not0
```

```{python, eval = FALSE}
x_given_not0.plot() # plot the simulated values

Exponential(rate = 1 / 4.3).plot() # plot the theoretical pdf
```

```{python, echo = FALSE}
plt.figure()
x_given_not0.plot()
Exponential(rate = 1 / 4.3).plot()
plt.show()

```

```{python}
x_given_not0.count_gt(2) / x_given_not0.count()
```

```{python}
x_given_not0.mean()
```

## Quantile functions

Recall that the cdf fills in the following blank for any given $x$: $x$ is the (blank) percentile.
The *quantile function* (essentially the inverse cdf[^distribution-7]) fills in the following blank for a given $p\in[0, 1]$: the $100p$th percentile is (blank).
For example, evaluating the quantile function at $p=0.25$ outputs the 25th percentile.

[^distribution-7]: If the cdf is a continuous function, then the quantile function is the inverse cdf.
    But the inverse of a cdf might not exist, if the cdf has jumps or flat spots.
    In particular, the inverse cdf does not exist for discrete random variables.
    So in general, the quantile function corresponding to cdf $F$ is defined as $Q(p) = \inf\{u:F(u)\ge p\}$.

The empirical rule in Section \@ref(sec-empirical-rule) describes the quantile function for Normal distributions.

```{example meeting-normal-percentile2}
In the meeting problem, suppose arrival times (minutes) follow a Normal(30, 10) distribution.
Let $Q$ be the quantile function.

In addition to finding the values below, identify how they are represented in the spinner in Figure \@ref(fig:meeting-normal-spinner).
```

1.  Find $Q(0.16)$.
2.  Find $Q(0.25)$.
3.  Find $Q(0.5)$.
4.  Find $Q(0.25)$.
5.  Find $Q(0.975)$.

```{solution meeting-normal-percentile2-sol}
to Example \@ref(exm:meeting-normal-percentile2)
```

```{asis fold.chunk = TRUE}


1. $Q(0.16)$ is the 16th percentile.
For a Normal distribution the 16th percentile is 1 standard deviation below the mean, so $Q(0.16) = 30 - 10 = 20$ minutes.
$Q(0.16)=20$ is 16% of the way around (at about 1:55) on the Normal(30, 10) spinner.
1. $Q(0.25)$ is the 25th percentile.
For a Normal distribution the 25th percentile is 0.67 standard deviations below the mean, so $Q(0.25) = 30 - 0.67(10) = 23.26$ minutes.
$Q(0.25)=23.26$ is 25% of the way around (at 3 o'clock) on the Normal(30, 10) spinner.
1. $Q(0.5)$ is the 50th percentile.
For a Normal distribution the 50th percentile is the mean the mean, so $Q(0.5) = 30$ minutes.
$Q(0.5)=30$ is 50% of the way around (at 6 o'clock) on the Normal(30, 10) spinner.
1. $Q(0.75)$ is the 75th percentile.
For a Normal distribution the 75th percentile is 0.67 standard deviations above the mean, so $Q(0.75) = 30 + 0.67(10) = 36.74$ minutes.
$Q(0.75)=36.74$ is 75% of the way around (at 9 o'clock) on the Normal(30, 10) spinner.
1. $Q(0.975)$ is the 97.5th percentile.
For a Normal distribution the 97.5th percentile is 2 standard deviations above the mean, so $Q(0.975) = 30 + 2(10) = 50$ minutes.
$Q(0.975)=50$ is 97.5% of the way around (at about 11:42) on the Normal(30, 10) spinner.

```

For named distributions, we can evaluate the theoretical quantile function in Symbulate using the `.quantile()` method.

```{python}

Normal(30, 10).quantile(0.25)

```

```{python}

Normal(30, 10).quantile([0.16, 0.5, 0.75, 0.975])

```

For a *continuous* random variable with cdf $F$, the **quantile function** $Q:[0,1]\mapsto\mathbb{R}$ is the inverse of the cdf, $Q(p) = F^{-1}(p)$.

```{example exponential-quantile}

Let $X$ have an Exponential(1) distribution.  Recall that the cdf is $F_X(x) = 1-e^{-x}, x>0$.

```

1.  Find the 25th percentile.
2.  Find the quantile function $Q_X$.
3.  Remember that we first encountered the Exponential(1) distribution at the start of Section \@ref(pdf). We saw that if $U$ has a Uniform(0, 1) distribution then $X = -\log(1-U)$ has an Exponential(1) distribution. How does the transformation of $U$ relate to the quantile function? What insight does this give you into constructing spinners?

```{solution exponential-quantile-sol}
to Example \@ref(exm:exponential-quantile)
```

```{asis, fold.chunk = TRUE}


1. Set $0.25 = \IP(X\le x) = 1-e^{-x}$ and solve to find $x=-\log(1-0.25)\approx 0.288$. Therefore $Q(0.25) = -\log(1-0.25) = 0.288$.
1. Set $p = \IP(X\le x) = 1-e^{-x}$ and solve for $x$ to find $x=-\log(1-p)$. Therefore, the quantile function is $Q_X(p) = -\log(1-p)$ for $0<p<1$.
1. $X = Q_X(U)$. The Uniform(0, 1) spinner lands uniformly on values between 0 and 1. For a given $p$ --- the area of any region starting from 0 in the spinner in Figure \@ref(fig:exponential1-spinner) --- $-\log(1-p)$ is the corresponding value on the axis of the spinner. See below for further discussion.

```

```{python}

Exponential(1).quantile(0.25)

```

```{python}

Exponential(1).quantile([0.632, 0.75, 0.865])

```

The quantile function can be used to create a spinner for a distribution.
Basically, the values on the outside boundary of the spinner are scaled based on the quantile function (which is determined by the cdf).
Intervals corresponding to regions of higher density ("more likely") values are stretched out on the spinner boundary; intervals corresponding regions of lower density ("less likely" values) are shrunk.

### One ~~ring~~ spinner to rule them all? {#univeral-spinner}

Throughout the book we have constructed spinners to represent a variety of distributions.
However, all of the examples assumed the same generic spinner: the needle was infinitely precise and "equally likely" to land on any value on the axis around the spinner.
We modeled different distributions simply by changing the values on the axis.

**The foundation of all spinners is the Uniform(0, 1) spinner** reproduced below.

```{r uniform-clock-spinner, echo = FALSE}

n = 12

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("1|0", round(xp$x[-1], 4))) +
  theme(axis.text.x=element_text(size=12, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
  ggtitle(paste("Uniform(0, 1) spinner", sep=""))

spinner
```

By suitably relabeling the axes on the Uniform(0, 1) spinner, we could have constructed the spinners for any of the other examples.

For example, to obtain the discrete spinner in the middle of Figure \@ref(fig:die-three-spinners), corresponding to a weighted four-sided die, start with the Uniform(0, 1) spinner and map

-   The range (0, 0.1] to 1,
-   The range (0.1, 0.3] to 2,
-   The range (0.3, 0.6] to 3,
-   The range (0.6, 1] to 4

Then the probability that the Uniform(0, 1) spinner lands in the range (0.3, 0.6] is 0.3, so the spinner resulting from this mapping would return a value of 3 with probability 0.3. (The probability of the infinitely precise needle landing on a specific value like 0.3 (that is, $0.300000000\ldots$) is 0, so it doesn't really matter what we do with the endpoints of the intervals.)

For non-uniform values on a continuous scale, we could construct a spinner according to the distribution of interest by rescaling and stretching/shrinking the axis of the Uniform(0, 1) spinner to correspond to intervals of larger/smaller probability.
For example, if we want to simulate values according to the Exponential(1) distribution we could start with the Uniform(0, 1) spinner and then transform the axis values $u \mapsto -\log(1-u)$ to obtain the spinner in Figure \@ref(fig:exponential1-spinner).

In Section \@ref(pdf) we started with the transformation $u\mapsto -\log(1-u)$ of the Uniform(0, 1) spinner and saw what distribution the transformed values followed via simulation (i.e., Exponential(1)).
But what about the reverse question: given a particular distribution, how do we find the transformation of Uniform(0, 1) that will generate values according to the specified distribution?

Recall how the spinner in Figure \@ref(fig:exponential1-spinner) was constructed.
We started with the Uniform(0, 1) spinner with equally spaced increments, and applied the transformation $-\log(1-u)$, which "stretched" the intervals corresponding to higher probability and "shrunk" the intervals corresponding to lower probability.
The distribution that we ended up with was the Exponential(1) distribution with cdf $1-e^{-x}, x>0$.
Notice now that the transformation $-\log(1-u)$ corresponds to the quantile function of an Exponential(1) distribution.
For example, we want to label the 75th percentile of the Exponential(1) distribution on the axis 75% of the way around the spinner; the quantile function tells us what the 75th percentile is so we know what value to put on the axis.

Example \@ref(exm:exponential-quantile) provides an example of how to go backwards.
Starting from a cdf, we can construct the corresponding spinner by finding the quantile function, essentially the inverse cdf, and applying it to the equally spaced values on the Uniform(0, 1) spinner.
The quantile function will stretch/shrink the intervals just right to correspond to the probabilities given by the cdf.

```{example meeting-nonuniform-pdf1-spinner}
Recall Example \@ref(exm:meeting-nonuniform-pdf1) where Regina's arrival time $X$ (in hours) had pdf
$$
f_X(x) = 2x, \qquad 0<x<1.
$$
```

1.  Find the cdf of $X$.
2.  Find the quantile function of $X$.
3.  Construct a spinner for simulating values of $X$ (hours) according to its distribution.
4.  If we measure $X$ in minutes, how would the spinner change?

```{solution meeting-nonuniform-pdf1-spinner-sol}
to Example \@ref(exm:meeting-nonuniform-pdf1-spinner)
```

```{asis, fold.chunk = TRUE}
1. Integrate the pdf: $F_X(x) = x^2, 0 < x < 1$ (and $F_X(x) = 0$ if $x<0$ and $F_X(x) = 1$ if $x>1$).
1. Invert the cdf: $p = F(x) = x^2$ implies $x = \sqrt{p}$, so $Q(p) = \sqrt{p}$ for $0<p<1$.
1. Transform the axis of the Uniform(0, 1) spinner using $u\mapsto \sqrt{u}$. For example, the 25th percentile is $Q(0.25) = \sqrt{0.25} = 0.5$ (hours), the 50th percentile is $Q(0.5) = \sqrt{0.5} = 0.707$ (hours), and the 75th percentile is $Q(0.75) = \sqrt{0.75} = 0.866$ (hours).
Notice that the transformation $u\mapsto \sqrt{u}$ stretches out intervals near 1, the intervals with higher density, and shrinks intervals near 0, the intervals with lower density.
1. Changing from hours to minutes is a linear rescaling, so it will just relabel the axis without any differential stretching/shrinking. Simply multiple all the values on the axis by 60.
```

(ref:cap-meeting-nonuniform-pdf1-spinner) Spinner for $X$ in Example \@ref(exm:meeting-nonuniform-pdf1-spinner). The axis is transformed according to the quantile function $Q(p) = \sqrt{p}$.

```{r meeting-nonuniform-pdf1-spinner-plot, echo = FALSE, fig.cap = "(ref:cap-meeting-nonuniform-pdf1-spinner)"}

n = 12

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("1|0", round(sqrt(xp$x[-1]), 3))) +
  theme(axis.text.x=element_text(size=12, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
  ggtitle(paste("Spinner for pdf f(x) = 2x, 0<x<1;\n sqrt(U) for U ~ Uniform(0, 1)", sep=""))

spinner
```

These examples illustrate "universality of the uniform".
Basically, we can always start with a spinner that lands uniformly in (0, 1) and suitably stretch/scale the axis around the spinner to construct a spinner corresponding to any distribution of interest.
The Uniform(0, 1) spinner returns a value in (0, 1); we obtain the value of the corresponding percentile from the quantile function.

**Universality of the Uniform (or "one spinner to rule them all").** Let $F$ be a cdf and $Q$ its corresponding quantile function.
Let $U$ have a Uniform(0, 1) distribution and define the random variable $X=Q(U)$.
Then[^distribution-8] the cdf of $X$ is $F$.

[^distribution-8]: We'll only prove the result assuming $F$ is a continuous, strictly increasing function, so that the quantile function is just the inverse of $F$, $Q(p) = F^{-1}(p)$.
    First note that $\{F^{-1}(U)\le x\} = \{U\le F(x)\}$; to see why draw a picture like Figure ???.
    Then $$
    \IP(X \le x) = \IP(Q(U) \le x) = \IP(F^{-1}(U)\le x) = \IP(U\le F(x)) = F(x)
    $$ The last step follows since $F(x)$ is just a number in [0, 1] and $\IP(U\le u) = u$ for $0\le u\le 1$ since $U$ has a Uniform(0, 1) distribution.

In the above, $U$ represents the result of the spin on the [0, 1] scale, and $Q(U)$ is the corresponding value on the stretched/shrunk scale.
$U$ also represents the area inside the spinner, while $Q(U)$ represents the value on the circular axis with that area to the left of it.

Universality of the uniform might look complicated but all it basically says is that you can construct a spinner by putting the 25th percentile 25% of the way around, the 75th percentile 75% of the way around, etc.

<!-- Consider $u\in[0, 1]$ and let $F^{-1}(u)=x$, so $F(x)=u$. -->

<!-- Since $F$ is increasing if $\tilde{x}<x$ then $F(\tilde{x})<F(x)$ -->

Actually, universality of the uniform says we don't have to create a new spinner.
We can just spin the Uniform(0, 1) spinner and transform each resulting value by plugging it into the quantile function.

Here is such a simulation for Example \@ref(exm:meeting-nonuniform-pdf1-spinner).
Notice that the histogram of simulated $X$ values has the desired shape of the pdf.

```{python}
U = RV(Uniform(0, 1))

X = sqrt(U)
```

```{python, eval = FALSE}
X.sim(10000).plot()

```

```{python, echo = FALSE}
plt.figure()
X.sim(10000).plot()
plt.show()
```

The only examples we have seen where a single spin of a rescaled Uniform(0, 1) distribution would not work were Bivariate Normal distributions, where we described a "globe" for simulating values.
However, Example \@ref(exm:meeting-conditional-then-marginal) introduced a method for simulating from a Bivariate Normal distribution using *two spins* of the standard Normal spinner.
Since the standard Normal spinner can be derived from a Uniform(0, 1) spinner, we can in principle use a Uniform(0, 1) distribution to simulate values from a Bivariate Normal distribution, but we need to spin it *twice* to generate a single pair.

## Distributions of transformations of random variables {#cdf-method}

It is common in many scientific, mathematical, and statistical context to transform variables.
A function of a random variable is a random variable: if $X$ is a random variable and $g$ is a function then $Y=g(X)$ is a random variable.
Since $g(X)$ is a random variable it has a distribution.
In general, the distribution of $g(X)$ will have a different shape than the distribution of $X$.
This section discusses some techniques for determining how a transformation changes the shape of a distribution.

### Linear rescaling

In general, the distribution of $g(X)$ will have a different shape than the distribution of $X$.
The exception is when $g$ is a linear rescaling.

A **linear rescaling** is a transformation of the form $g(u) = a +bu$, where $a$ (intercept) and $b$ (slope[^distribution-9]) are constants.
For example, converting temperature from Celsius to Fahrenheit using $g(u) = 32 + 1.8u$ is a linear rescaling.

[^distribution-9]: You might be familiar with "$mx+b$" where $b$ denotes the intercept.
    In Statistics, $b$ is often used to denote slope.
    For example, in R `abline(a = 32, b = 1.8)` draws a line with intercept 32 and slope 1.8.

A linear rescaling "preserves relative interval length" in the following sense.

-   If interval A and interval B have the same length in the original measurement units, then the rescaled intervals A and B will have the same length in the rescaled units. For example, [0, 10] and [10, 20] Celsius, both length 10 degrees Celsius, correspond to [32, 50] and [50, 68] Fahrenheit, both length 18 degrees Fahrenheit.
-   If the ratio of the lengths of interval A and B is $r$ in the original measurement units, then the ratio of the lengths in the rescaled units is also $r$. For example, [10, 30] is twice as long as [0, 10] in Celsius; for the corresponding Fahrenheit intervals, [50, 86] is twice as long as [32, 50].

Think of a linear rescaling as just a consistent relabeling of the variable axis; every 1 unit increment in the original scale corresponds to a $b$ unit increment in the linear rescaling.

```{r, echo = FALSE}

knitr::include_graphics('_graphics/celsius-fahrenheit.png')

```

Suppose that SAT Math score $X$ follows a Uniform(200, 800) distribution.
(It doesn't but go with in for now).
One way to simulate values of $X$ is to simulate values of $U$ from a Uniform(0, 1) distribution and let $X = 200 + (800 - 200)U= 200 + 600U$.
Then $X$ is a linear rescaling of $U$, and $X$ takes values in the interval [200, 800].
We can define and simulate values of $X$ in Symbulate.
Before looking at the results, sketch a plot of the distribution of $X$ and make an educated guess for its mean and standard deviation.

```{python}

U = RV(Uniform(0, 1))

X = 200 + 600 * U

(U & X).sim(10)

```

```{python, eval = FALSE}

X.sim(10000).plot()

```

```{python, echo = FALSE}
plt.figure()
X.sim(10000).plot()
plt.show()
```

We see that $X$ has a Uniform(200, 800) distribution.
The linear rescaling changes the range of possible values, but the general shape of the distribution is still Uniform.
We can see why by inspecting a few intervals on both the original and revised scale.

| Interval of $U$ values | Probability that $U$ lies in the interval | Interval of $X$ values | Probability that $X$ lies in the interval |
|---------------:|--------------------:|---------------:|--------------------:|
|             (0.0, 0.1) |                                       0.1 |             (200, 260) |                          $\frac{60}{600}$ |
|             (0.9, 1.0) |                                       0.1 |             (740, 800) |                          $\frac{60}{600}$ |
|             (0.0, 0.2) |                                       0.2 |             (200, 320) |                         $\frac{120}{600}$ |

For a Uniform distribution the long run average is the midpoint of possible values.
The long run average value of $U$ is 0.5, and of $X$ is 500.
These two values are related through the same formula mapping $U$ to $X$ values: $500 = 200 + 600\times 0.5$.

```{python}
U.sim(10000).mean(), X.sim(10000).mean()
```

For a Uniform distribution, the standard deviation the about 0.289 times the length of the interval: $|b-a|/\sqrt{12}$.
The standard deviation of $U$ is about 0.289, and of $X$ is about 173.

```{python}

U.sim(10000).sd(), X.sim(10000).sd()

```

The standard deviation of $X$ is 600 times the standard deviation of $U$.
Multiplying the $U$ values by 600 rescales the distance between the values.
Two values of $U$ that are 0.1 units apart correspond to two values of $X$ that are 60 units apart.
A $U$ value of 0.6 is 0.1 units above the mean of $U$, and the corresponding $X$ value 560 is 60 units about the mean of $X$.
However, adding the constant 200 to all values just shifts the distribution and does affect degree of variability.

```{example uniform-linear}

Let $\IP$ be the probabilty space corresponding to the Uniform(0, 1) spinner and let $U$ represent the result of a single spin.  Define $V=1-U$.

```

1.  Does $V$ result from a linear rescaling of $U$?
2.  What are the possible values of $V$?
3.  Is $V$ the same random variable as $U$?
4.  Find $\IP(U \le 0.1)$ and $\IP(V \le 0.1)$.
5.  Sketch a plot of what the histogram of many simulated values of $V$ would look like.
6.  Does $V$ have the same distribution as $U$?

```{solution uniform-linear-sol}
to Example \@ref(exm:uniform-linear)
```

```{asis, fold.chunk = TRUE}



1. Yes, $V$ result from the linear rescaling $u\mapsto 1-u$ (intercept of 1 and slope of $-1$.)
1. $V$ takes values in the interval [0,1].
Basically, this transformation just changes the direction of the spinner from clockwise to counterclockwise.
The axis on the usual spinner has values $u$ increasing clockwise from 0 to 1.  Applying the transformation $1-u$,  the values would decrease clockwise from 1 to 0.
1. No. $V$ and $U$ are different random variables.  If the spin lands on $\omega=0.1$, then $U(\omega)=0.1$ but $V(\omega)=0.9$.  $V$ and $U$ return different values for the same outcome; they are measuring different things.
1. $\IP(U \le 0.1) = 0.1$ and $\IP(V \le 0.1)=\IP(1-U \le 0.1) = \IP(U\ge 0.9) = 0.1$.  Note, however, that these are different events: $\{U \le 0.1\}=\{0 \le \omega \le 0.1\}$ while $\{V \le 0.1\}=\{0.9 \le \omega \le 1\}$.  But each is an interval of length 0.1 so they have the same probability according to the uniform probability measure.
1. Since $V$ is a linear rescaling of $U$, the shape of the histogram of simulated values of $V$ should be the same as that for $U$.  Also, the possible values of $V$ are the same as those for $U$.  So the histograms should look identical (aside from natural simulation variability).
1. Yes, $V$ has the same distribution as $U$.  While for any single outcome (spin), the values of $V$ and $U$ will be different, over many repetitions (spins) the pattern of variation of the $V$ values, as depicted in a histogram, will be identical to that of $U$.
```

```{python}

P = Uniform(0, 1)
U = RV(P)

V = 1 - U

V.sim(10000).plot()
plt.show()

```

Let's consider a non-uniform example.
Now let's suppose that SAT Math score $X$ follows a Normal(500, 100) distribution.
We can simulate values of $X$ by simulating $Z$ from the standard Normal(0, 1) distribution and setting $X = 500 + 100Z$.
(Remember that the standard Normal spinner returns standardized values, so $Z = 1$ corresponds to 1 standard deviation above the mean, that is, $X= 600$.) The reason this works is because the linear rescaling doesn't change the Normal shape.

```{python}
Z = RV(Normal(0, 1))

X = 500 + 100 * Z

(Z & X).sim(10)

```

```{python}
x = X.sim(10000)
```

```{python, eval = FALSE}
x.plot() # plot the simulated values
Normal(500, 100).plot() # plot the theoretical density
```

```{python, echo = FALSE}
plt.figure()
x.plot()
Normal(500, 100).plot() # plot the density
plt.show()
```

```{python}
x.mean(), x.sd()

```

The linear rescaling changes the range of observed values; almost all of the values of $Z$ lie in the interval $(-3, 3)$ while almost all of the values of $X$ lie in the interval $(200, 800)$.
However, the distribution of $X$ still has the general Normal shape.
The means are related by the conversion formula: $500 = 500 + 100 \times 0$.
Multiplying the values of $Z$ by 100 rescales the distance between values; two values of $Z$ that are 1 unit apart correspond to two values of $X$ that are 100 units apart.
However, adding the constant 500 to all the values just shifts the center of the distribution and does not affect variability.
Therefore, the standard deviation of $X$ is 100 times the standard deviation of $Z$.

In general, if $Z$ has a Normal(0, 1) distribution then $X = \mu + \sigma Z$ has a Normal($\mu$, $\sigma$) distribution.

```{example normal-sat-linear}

Suppose that $X$, the SAT Math score of a randomly selected student, follows a Normal(500, 100) distribution.  Randomly select a student and let $X$ be the student's SAT Math score.  Now have the selected student spin the Normal(0, 1) spinner.  Let $Z$ be the result of the spin and let $Y=500 + 100 Z$.

```

1.  Is $Y$ the same random variable as $X$?
2.  Does $Y$ have the same distribution as $X$?

```{solution normal-sat-linear-sol}
to Example \@ref(exm:normal-sat-linear)
```

```{asis, fold.chunk = TRUE}
1. No, these two random variables are measuring different things.  One is measuring SAT Math score; one is measuring what comes out of a spinner.  Taking the SAT and spinning a spinner are not the same thing.
1. Yes, they do have the same distribution. Repeating the process of randomly selecting a student and measuring SAT Math score will yield values that follow a Normal(500, 100) distribution.  Repeating the process of spinning the Normal(0, 1) spinner to get $Z$ and then setting $Y=500+100Z$ will also yield values that follow a Normal(500, 100) distribution.  Even though $X$ and $Y$ are different random variables they follow the same long run pattern of variability.
```

```{example dd-normal-negative}

Let $Z$ be a random variable with a Normal(0, 1) distribution. Consider $-Z$.

```

1.  Donny Don't says that the distribution of $-Z$ will look like an "upside-down bell". Is Donny correct? If not, explain why not and describe the distribution of $-Z$.
2.  Donny Don't says that the standard deviation of $-Z$ is -1. Is Donny correct? If not, explain why not and determine the standard deviation of $-Z$.

```{solution dd-normal-negative-sol}
to Example \@ref(exm:dd-normal-negative)
```

```{asis, fold.chunk = TRUE}

1. Donny is confusing a random variable with its distribution.
The values of the random variable are multiplied by -1, not the heights of the histogram.
Multiplying values of $Z$ by -1 rotates the "bell" about 0 horizontally (not vertically).
Since the Normal(0, 1) distribution is symmetric about 0, the distribution of $-Z$ is the same as the distribution of $Z$, Normal(0, 1).
1. Standard deviation cannot be negative.
Standard deviation measures average distance of values from the mean.
A $Z$ value of 1 yields a $-Z$ value of -1; in either case the value is 1 unit away from the mean.
Multiplying a random variable by $-1$ simply reflects the values horizontally about 0, but does not change distance from the mean^[If the mean were not 0, multiplying a random variable by $-1$  reflects the mean about 0 too, and distances from the mean are unaffected. For example, suppose $X$ has mean 1 so a value of 5 is 4 units away from the mean of 1. Then $-X$ has mean -1; a value of $X$ of 5 yields a value of $-X$ of -5, which is 4 units away from the mean of -1.].
In general, $X$ and $-X$ have the same standard deviation.
In this example we can say more: $Z$ and $-Z$ have the same distribution so they must have the same standard deviation, 1.


```

#### Summary

-   A linear rescaling is a transformation of the form $g(u) = a + bu$.
-   A linear rescaling of a random variable does not change the basic shape of its distribution, just the range of possible values.
    -   However, remember that the possible values are part of the distribution. So a linear rescaling does technically change the distribution, even if the basic shape is the same. (For example, Normal(500, 100) and Normal(0, 1) are two different distributions.)
-   A linear rescaling transforms the mean in the same way the individual values are transformed.
-   Adding a constant to a random variable does not affect its standard deviation.
-   Multiplying a random variable by a constant multiplies its standard deviation by the *absolute value* of the constant.
-   Whether in the short run or the long run, \begin{align*}
    \text{Average of $a+bX$} & = a+b(\text{Average of $X$})\\
    \text{SD of $a+bX$} & = |b|(\text{SD of $X$})\\
    \text{Variance of $a+bX$} & = b^2(\text{Variance of $X$})
    \end{align*}
-   If $U$ has a Uniform(0, 1) distribution then $X = a + (b-a)U$ has a Uniform($a$, $b$) distribution.
-   If $Z$ has a Normal(0, 1) distribution then $X = \mu + \sigma Z$ has a Normal($\mu$, $\sigma$) distribution.
-   Remember, do NOT confuse a random variable with its distribution.
    -   The random variable is the numerical quantity being measured
    -   The distribution is the long run pattern of variation of many observed values of the random variable

### Nonlinear transformations of random variables {#sim-nonlinear}

A linear rescaling does not change the shape of a distribution, only the range of possible values.
But what about a nonlinear transformation, like a logarithmic or square root transformation?
In contrast to a linear rescaling, a nonlinear rescaling does *not* preserve relative interval length, so we might expect that a nonlinear rescaling can change the shape of a distribution.
We'll investigate by considering the Uniform(0, 1) spinner and a logarithmic[^distribution-10] transformation.

[^distribution-10]: As in many other contexts and programming languages, in this text any reference to logarithms or $\log$ refers to natural (base $e$) logarithms.
    In the instances we need to consider another base, we'll make that explicit.

Let $U$ represent the result of a single spin of the Uniform(0, 1) spinner.
We'll basically consider $\log(U)$, but this leads to to two minor technicalities.

-   Since $U\in[0, 1]$, $\log(U)\le 0$. To obtain positive values we consider $-\log(U)$, which takes values in $[0,\infty)$.
-   Technically, applying $-\log(u)$ to the values on the axis of the Uniform(0, 1) spinner, the resulting values would decrease from $\infty$ to 0 clockwise. To make the values start at 0 and increase to $\infty$ clockwise, we consider $-\log(1-U)$. (We saw in the previous section the transformation $u \to 1-u$ basically just changes direction from clockwise to counterclockwise.)

Therefore, it's a little more convenient to consider the random variable $X=-\log(1-U)$ which takes values in $[0,\infty)$.
It also turns out, as we saw in earlier, that $-\log(1-u)$ is the quantile function of the Exponential(1) distribution.
We have already seen that $X$ has an Exponential(1) distribution.
Now we'll take a closer look why.

The following code defines $X$ and plots a few simulated values.

```{python}
P = Uniform(0, 1)
U = RV(P)

X = -log(1 - U)

x = X.sim(100)
```

```{python, eval = FALSE}
x.plot('rug')
```

```{python, echo = FALSE}
plt.figure()
x.plot('rug')
plt.show()
```

Notice that values near 0 occur with higher frequency than larger values.
For example, there are many more simulated values of $X$ that lie in the interval $[0, 1]$ than in the interval $[3, 4]$, even though these intervals both have length 1.
Let's see why this is happening.

```{example uniform-log-transform-calcs}

For each of the intervals in the table below find the probability that $U$ lies in the interval, and identify the corresponding values of $X$.  (You should at least compute a few by hand to see what's happening, but you can use software to fill in the rest.)
After completing the table, sketch a histogram representing the distribution of $X$.

```

```{r, echo = FALSE}

u1 = seq(0, 0.9, 0.1)
u2 = seq(0.1, 1, 0.1)

x1 = -log(1-u1)
x2 = -log(1-u2)

knitr::kable(
  data.frame(paste("(", u1, ", ", u2, ")", sep=""), rep("", 10), rep("", 10), rep("", 10), rep("", 10)),
  col.names = c('Interval of U', 'Length of U interval', 'Probability', 'Interval of X', 'Length of X interval'),
  digits = c(1,  1, 1, 3, 3)
)



```

```{solution uniform-log-transform-calcs-sol}
to Example \@ref(exm:uniform-log-transform-calcs)
```

Plug the endpoints into the conversion formula $u\mapsto -\log(1-u)$ to find the corresponding $X$ interval.
For example, the $U$ interval $(0.1, 0.2)$ corresponds to the $X$ interval $(-\log(1-0.1), -\log(1-0.2)) = (0.105, 0.223)$.
Since $U$ has a Uniform(0, 1) distribution the probability is just the length of the $U$ interval.

```{r, echo = FALSE}

u1 = seq(0, 0.9, 0.1)
u2 = seq(0.1, 1, 0.1)

x1 = -log(1-u1)
x2 = -log(1-u2)

knitr::kable(
  data.frame(paste("(", u1, ", ", u2, ")", sep=""), u2 - u1, u2-u1, paste("(", round(x1, 3), ", ", round(x2, 3), ")", sep=""), x2-x1),
  col.names = c('Interval of U', 'Length of U interval', 'Probability', 'Interval of X', 'Length of X interval'),
  digits = c(1,  1, 1, 3, 3)
)



```

We see that the logarithmic transformation does not preserve relative interval length.
Each of the original intervals of $U$ values has the same length, but the nonlinear logarithmic transformation "stretches out" these intervals in different ways.
The probability that $U$ lies in each of these intervals is 0.1.
As the transformation stretches the intervals, the 0.1 probability gets "spread" over intervals of different length.
Since probability/relative frequency is represented by area in a histogram, if two regions of differing length have the same area, then they must have different heights.
Thus the shape of the distribution of $X$ will not be Uniform.

The following plot illustrates the results of Example \@ref(exm:uniform-log-transform-calcs).
Each bar in the top histogram corresponds to the same color bar in the bottom histogram.
All bars have area 0.1.
In the top histogram, the bins have equal width so the heights are the same.
However, in the bottom histogram the bars have different widths but the same area, so they must have different heights, and we start to see where the Exponential(1) shape comes from.

```{r, echo = FALSE}

set.seed(1)

u = runif(100000)

ubreaks = seq(0, 1, 0.1)

bar_colors = c("#88CCEE", "#CC6677", "#DDCC77", "#117733", "#332288", "#AA4499", "#44AA99", "#999933", "#882255", "#661100", "#6699CC", "#888888")

par(mfrow=c(2, 1))
hist(u, breaks = ubreaks, xlab = "u", xaxt='n',
     freq = FALSE, main = "U ~ Uniform(0, 1)",
     col = bar_colors)
axis(1, ubreaks)

x = -log(1 - u)

xbreaks = -log(1 - pmin(ubreaks, 0.99999))
hist(x, breaks = xbreaks, xlab = "x", xaxt='n',
     freq = FALSE, main = "X = -log(1 - U) where U ~ Uniform(0, 1)",
     col = bar_colors)
axis(1, round(xbreaks, 2))


```

The following example provides a similar illustration, but from the reverse perspective.

```{example uniform-log-transform-calcs2}

For each of the intervals of $X$ values in the table below identify the corresponding values of $U$, and then find the probability that $X$ lies in the interval.   (You should at least compute a few by hand to see what's happening, but you can use software to fill in the rest.)
After completing the table, sketch a histogram representing the distribution of $X$.
Hint: if $X = -\log(1-U)$ then $U = 1-e^{-X}$.

```

```{r, echo = FALSE}

x1 = seq(0, 4.5, 0.5)
x2 = seq(0.5, 5, 0.5)

u1 = 1 - exp(-x1)
u2 = 1 - exp(-x2)

knitr::kable(
  data.frame(paste("(", x1, ", ", x2, ")", sep=""), rep("", 10), rep("", 10), rep("", 10), rep("", 10)),
  col.names = c('Interval of X', 'Length of X interval', 'Probability', 'Interval of U', 'Length of U interval'),
  digits = c(1,  1, 3, 3, 3)
)



```

```{solution uniform-log-transform-calcs2-sol}
to Example \@ref(exm:uniform-log-transform-calcs2)
```

The corresponding $U$ intervals are obtained by applying the inverse transformation $v\mapsto 1-e^{-v}$.
For example, the $X$ interval $(0.5, 1)$ corresponds to the $U$ interval $(1-e^{-0.5}, 1-e^{-1}) = (0.393, 0.632)$.

```{r, echo = FALSE}

x1 = seq(0, 4.5, 0.5)
x2 = seq(0.5, 5, 0.5)

u1 = 1 - exp(-x1)
u2 = 1 - exp(-x2)

knitr::kable(
  data.frame(paste("(", x1, ", ", x2, ")", sep=""), x2 - x1, u2-u1, paste("(", round(u1, 3), ", ", round(u2, 3), ")", sep=""), u2-u1),
  col.names = c('Interval of X', 'Length of X interval', 'Probability', 'Interval of U', 'Length of U interval'),
  digits = c(1,  1, 3, 3, 3)
)



```

Since $U$ has a Uniform(0, 1) distribution the probability is just the length of the $U$ interval.
Each of the $X$ intervals has the same length but they correspond to intervals of differing length in the original $U$ scale, and hence intervals of different probability.

The following plot illustrates the results of Example \@ref(exm:uniform-log-transform-calcs2) (plots on the right).
These two examples give some insight into how the transformed random variable $X = -\log(1-U)$ has an Exponential(1) distribution.

```{r, echo = FALSE}

set.seed(1)

# Version 2

par(mfrow=c(2, 1))

xbreaks = c(seq(0, 5, 0.5), 15)

hist(x, breaks = xbreaks, xlab = "x", xaxt='n',
     freq = FALSE, main = "X = -log(1 - U) where U ~ Uniform(0, 1)",
     col = bar_colors)
axis(1, round(xbreaks, 2))


ubreaks = 1 - exp(-xbreaks)

hist(u, breaks = ubreaks, xlab = "u", xaxt='n',
     freq = FALSE, main = "U ~ Uniform(0, 1)",
     col = bar_colors)
axis(1, round(ubreaks, 3))

```

"Spreadsheet" calculations like those in the previous two examples can help when sketching the distribution of a transformed random variable.

<!-- It should be clear that the simulated values of $X$ do not follow a Uniform distribution.  Values near 0 occur with greater frequency than larger values.  The non-linear log transformation changes the shape of the distribution. -->

<!-- To get some intuition behind why the shape changes, consider the following illustration. Consider intervals in increments of 0.1, starting from 0, on the original [0, 1] scale.  These intervals each have length 0.1 and so each have probability 0.1 according to the uniform probability measure.  Now consider the corresponding transformed intervals. -->

<!-- - [0, 0.1] maps to^[Each of these values is obtained from the transformation $u\mapsto-\log(1-u)$, e.g. $-\log(1-0.1)\approx 0.105$.] [0, 0.105], an interval of length 0.105. -->

<!-- - [0.1, 0.2] maps to [0.105, 0.223], an interval of length 0.118. -->

<!-- - [0.2, 0.3] maps to [0.223, 0.357], an interval of length 0.134. -->

<!-- - [0.3, 0.4] maps to [0.357, 0.511], an interval of length 0.154. -->

<!-- - [0.4, 0.5] maps to [0.511, 0.693], an interval of length 0.182, and so on. -->

<!-- - [0, 1] corresponds to^[Each of these values is obtained by applying the inverse transformation $u\mapsto 1-e^{-u}$, e.g. $1-e^{-1}\approx 0.632$] [0, 0.632], and interval with probability 0.632. -->

<!-- - [1, 2] corresponds to [0.632, 0.865], and interval with probability 0.233. -->

<!-- - [2, 3] corresponds to [0.865, 0.950], and interval with probability 0.086. -->

<!-- Notice that the shape of the histogram depicting the simulated values of $X$ appears that it can be approximated by a smooth curve.  This smooth curve is an idealized model of what would happen in the long run if -->

<!--   - we kept simulating more and more values, and -->

<!--   - made the histogram bin widths smaller and smaller. -->

<!-- The following plot illustrates the results of 100,000 simulated values of $X$ summarized in a histogram with 1000 bins.  -->

For a linear rescaling, we could just plug the mean of the original variable into the conversion formula to find the mean of the transformed variable.
However, this will not work for nonlinear transformations.

```{python}

(U & X).sim(10000).mean()

```

We know that since $U$ has a Uniform(0, 1) distribution its long run average value is 0.5, and since $X$ has an Exponential(1) distribution its long run average value is 1, but $-\log(1 - 0.5) \neq 1$.
The nonlinear "stretching" of the axis makes some value relatively larger and others relatively smaller than they were on the original scale, which influences the average.
Remember, in general: whether in the short run or the long run $$
\text{Average of } g(X) \neq g(\text{Average of }X).
$$

Recall that a function of a random variable is also a random variable.
If $X$ is a random variable, then $Y=g(X)$ is also a random variable and so it has a probability distribution.
Unless $g$ represents a linear rescaling, a transformation will change the shape of the distribution.
So the question is: what is the distribution of $g(X)$?
We'll focus on transformations of continuous random variables, in which case the key to answering the question is to work with cdfs.

```{example log-uniform-cdf-method}

We have now seen a few reasons why if $U$ has a Uniform(0, 1) distribution then $X=-\log(1-U)$ has an Exponential(1) distribution.
The purpose of the example is derive the pdf of $X$, starting from just the setup in the previous sentence.

```

1.  Identify the possible values of $X$. (We have done this already, but this should always be your first step.)
2.  Let $F_X$ denote the cdf of $X$. Find $F_X(1)$.
3.  Find $F_X(2)$.
4.  Find the cdf $F_X(x)$.
5.  Find the pdf $f_X(x)$.
6.  Why should we not be surprised that $X=-\log(1-U)$ has cdf $F_X(x) = 1 - e^{-x}$? Hint: what is the function $u\mapsto -\log(1-u)$ in this case?

```{solution log-uniform-cdf-method-sol}
to Example \@ref(exm:log-uniform-cdf-method)
```

```{asis, fold.chunk = TRUE}


1. As always, first determine the range of possible values.  When $u=0$, $-\log(1-u)=0$, and as $u$ approaches 1, $-\log(1-u)$ approaches $\infty$; see the picture of the function below.  So $X$ takes values in $[0, \infty)$.
1. $F_X(1) = \IP(X \le 1)$, a probability statement involving $X$.  Since we know the distribution of $U$, we express the event $\{X \le 1\}$ as an equivalent event involving $U$.  
\[
\{X \le 1\} = \{-\log(1-U) \le 1\} = \{U \le 1 - e^{-1}\}
\]
The above follows since $-\log(1-u)\le 1$ if and only if $u\le 1-e^{-1}$; see Figure \@ref(fig:log-function-plot) below.  Therefore
\[
F_X(1) = \IP(X \le 1) = \IP(-\log(1-U)\le 1) = \IP(U\le 1-e^{-1})
\]
Now since $U$ has a Uniform(0, 1) distribution, $\IP(U\le u)=u$ for $0<u<1$.  The value $1-e^{-1}\approx 0.632$ is just a number between 0 and 1, so $\IP(U\le 1-e^{-1}) = 1-e^{-1}\approx 0.632$. Therefore $F_X(1)=1-e^{-1}\approx 0.632$.
1. Similar to the previous part
\[
F_X(2) = \IP(X \le 2) = \IP(-\log(1-U)\le 2) = \IP(U\le 1-e^{-2}) = 1-e^{-2}\approx 0.865
\]
<!-- The above follows since $-\log(1-u)\le 2$ if and only if $u\le 1-e^{-2}$; see the picture below.  Now since $U$ has a Uniform(0, 1) distribution, $\IP(U\le u)=u$ for $0<u<1$.  The value $1-e^{-2}\approx 0.865$ is just a number between 0 and 1, so $\IP(U\le 1-e^{-2}) = 1-e^{-2}\approx 0.865$. Therefore $F_X(1)=1-e^{-2}\approx 0.865$.  (This is represented in Figure \@ref(fig:log-uniform-density) by the area of the region from 0 to 2, 86.5%.) -->
1. As suggested in the paragraph before the example, the key to finding the pdf is to work with cdfs.  We basically repeat the calculation in the previous steps, but for a generic $x$ instead of 1 or 2. Consider $0\le x<\infty$; we wish to find the cdf evaluated at $x$.
\[
F_X(x) = \IP(X \le x) = \IP(-\log(1-U)\le x) = \IP(U \le 1-e^{-x}) = 1-e^{-x}
\]
The above follows since, for $0<x<\infty$, $-\log(1-u)\le x$ if and only if $u\le 1-e^{-x}$; see Figure \@ref(fig:log-function-plot) below (illustrated for $x=1$).  Now since $U$ has a Uniform(0, 1) distribution, $\IP(U\le u)=u$ for $0<u<1$.  For a fixed $0<x<\infty$, the value $1-e^{-x}$ is just a number between 0 and 1, so $\IP(U\le 1-e^{-x}) = 1-e^{-x}$. Therefore $F_X(x)=1-e^{-x}, 0<x<\infty$.
1. From the previous part, we see that the cdf of $X$ is the cdf of the Exponential(1) distribution, so that is enough to show that $X$ has a Exponential(1) distribution.
But we can also directly differentiate the cdf with respective to $x$ to find the pdf.
\[
f_X(x) = F'(x) =\frac{d}{dx}(1-e^{-x}) = e^{-x}, \qquad 0<x<\infty
\]
Thus we see that $X$ has the Exponential(1) pdf.
1. The function $Q_X(u) = -\log(1-u)$ is the quantile function (inverse cdf) corresponding to the cdf $F_X(x) = 1-e^{-x}$.  Therefore, since $U$ has a Uniform(0, 1) distribution, the random variable $Q_X(U)$ will have cdf $F_X$ by universality of the Uniform.

```

(ref:cap-log-function-plot) A plot of the function $u\mapsto -\log(1-u)$. The dotted lines illustrate that $-\log(1-u)\le 1$ if and only if $u\le 1-e^{-1}\approx 0.632$.

```{r, log-function-plot, echo = FALSE, fig.cap="(ref:cap-log-function-plot)"}

g0 = 1
x0 = 1-exp(-g0)
x = seq(0, 0.99999, 0.0001)
gx = -log(1-x)
plot(x, gx, type="l", xlim = c(0, 1), ylim = c(0, 5), xlab="u", ylab="-log(1-u)",xaxs="i",yaxs="i")
segments(x0=x0, y0=0, x1=x0, y1=g0, col="orange", lty=2, lwd=2)
segments(x0=0, y0=0, x1=0, y1=g0, col="skyblue", lty=1, lwd=10)
segments(x0=0, y0=g0, x1=x0, y1=g0, col="skyblue", lty=2, lwd=2)
segments(x0=0, y0=0, x1=x0, y1=0, col="orange", lty=1, lwd=10)

```

If $X$ is a continuous random variable whose distribution is known, the **cdf method** can be used to find the pdf of $Y=g(X)$

-   Determine the possible values of $Y$. Let $y$ represent a generic possible value of $Y$.
-   The cdf of $Y$ is $F_Y(y) = \IP(Y\le y) = \IP(g(X) \le y)$.
-   Rearrange $\{g(X) \le y\}$ to get an event involving $X$. Warning: it is not always $\{X \le g^{-1}(y)\}$. Sketching a picture of the function $g$ helps.
-   Obtain an expression for the cdf of $Y$ which involves $F_X$ and some transformation of the value $y$.
-   Differentiate the expression for $F_Y(y)$ with respect to $y$, and use what is known about $F'_X = f_X$, to obtain the pdf of $Y$. You will typically need to apply the chain rule when differentiating.

You will need to use information about $X$ at some point in the last step above.
You can either:

-   Plug in the *cdf* of $X$ and then differentiate with respect to $y$.
-   Differentiate with respect to $y$ and then plug in the *pdf* of $X$.

Either way gets you to the correct answer, but depending on the problem one way might be easier than the other.
We'll illustrate both methods in the next example.

```{example uniform-square-cdf-method}

Let $X$ be a random variable with a Uniform(-1, 1) distribution and Let $Y=X^2$. 

```

1.  Identify the possible values of $Y$.
2.  Sketch the pdf of $Y$. Hint: consider a few equally spaced intervals of $Y$ values and see what $X$ values they correspond to.
3.  Run a simulation to approximate the pdf of $Y$.
4.  Find $F_Y(0.49)$.
5.  Use the cdf method to find the pdf of $Y$. Is the pdf consistent with your simulation results?

```{solution uniform-square-cdf-method-sol}
to Example \@ref(exm:uniform-square-cdf-method)
```

```{asis, fold.chunk = TRUE}



1. Always start with the possible values: since $-1< X<1$ we have $0<Y<1$.
1. The idea to the sketch is that squaring a number less than 1 in absolute value returns a smaller number, e.g. $0.1^2 = 0.01$.  So the transformation "pushes values towards 0" making the density higher near 0.  Consider the intervals $[0, 0.1]$ and $[0.9, 1]$ on the original scale; both intervals have probability 0.05 under the Uniform($-1, 1$) distribution.  On the squared scale, these intervals correspond to $[0, 0.01]$ and $[0.81, 1]$ respectively.  So the 0.05 probability is "squished" into $[0, 0.01]$, resulting in a greater height, while it is "spread out" over $[0.81, 1]$ resulting in a smaller height.  Remember: probability is represented by area.
This gives you an idea that the pdf should be highest at 0 and lowest at 1.
    To get a better sketch, you can do a "spreadsheet" type calculation; see below. We start with equally spaced intervals on the $Y$ scale and then find the corresponding intervals of $X = \pm\sqrt{Y}$. Since $X$ has a Uniform distribution over an interval of length 2, the probability that $X$ lies in a subinterval of $(-1, 1)$ is the length of the subinterval divided by 2. The evenly spaced $Y$ intervals correspond to differentially spaced $X$ intervals, hence different probabilities/areas, and so the density height of the $Y$ intervals changes accordingly.
1. See the simulation results below.  We see that the density is highest near 0 and lowest near 1.
1. Since $X$ can take negative values, we have to be careful; see Figure \@ref(fig:square-function-plot) below.
\[
\{Y \le 0.49\} = \{X^2 \le 0.49\} = \{-\sqrt{0.49} \le X \le \sqrt{0.49}\}
\]
Therefore, since $X$ has a Uniform($-1,1$) distribution,
\[
F_Y(0.49) = \IP(Y \le 0.49) = \IP(-0.7 \le X \le 0.7) = \frac{1.4}{2} = 0.7
\]
1. Fix $0<y<1$.  We now do the same calculation in the previous part in terms of a generic $y$, but it often helps to think of $y$ as a particular number first. 
    \begin{align*}
    F_Y(y) & = \IP(Y\le y)\\
    & = \IP(X^2\le y)\\
    & = \IP(-\sqrt{y}\le X\le \sqrt{y})\\
    & = F_X(\sqrt{y}) - F_X(-\sqrt{y})
    \end{align*}
    Note that the event of interest is *not* just $\{X\le \sqrt{y}\}$; see Figure \@ref(fig:square-function-plot) below. From here we can either

    use the cdf of $X$ and then differentiate, or differentiate and then use the pdf of $X$.  We'll illustrate both.
    (a) Using the Uniform(-1, 1) cdf, the interval $[-\sqrt{y}, \sqrt{y}]$ has length $2\sqrt{y}$, and the total length of $[-1, 1]$ is 2, so we have
    \[
    F_Y(y) = F_X(\sqrt{y}) - F_X(-\sqrt{y}) = \frac{2\sqrt{y}}{2} = \sqrt{y}
    \]
    Now differentiate with respect to the argument $y$ to obtain $f_Y(y) = \frac{1}{2\sqrt{y}}, 0<y<1$.
    (b) Differentiate both sides of $F_Y(y) = F_X(\sqrt{y}) - F_X(-\sqrt{y})$, with respect to $y$.  Differentiating the cdf $F_Y$ yields its pdf $f_Y$, and differentiating the cdf $F_X$ yields its pdf $f_X$.  But don't forget to use the chain rule when differentiating $F_X(\sqrt{y})$.
    \begin{align*}
    F_Y(y) & = F_X(\sqrt{y}) - F_X(-\sqrt{y})\\
        \Rightarrow \frac{d}{dy} F_Y(y) & = \frac{d}{dy}\left(F_X(\sqrt{y}) - F_X(-\sqrt{y})\right)\\
    \qquad f_Y(y) & = f_X(\sqrt{y})\frac{1}{2\sqrt{y}} - f_X(-\sqrt{y})\left(-\frac{1}{2\sqrt{y}}\right)\\
    &= \frac{1}{2\sqrt{y}}\left(f_X(\sqrt{y})+f_X(-\sqrt{y})\right)
    \end{align*}
    Since $X$ has a Uniform(-1, 1) distribution, its pdf is $f_X(x) = 1/2, -1<x<1$.  But for $0<y<1$, $\sqrt{y}$ and $-\sqrt{y}$ are just numbers in $[-1, 1]$, so $f_X(\sqrt{y})=1/2$ and $f_X(-\sqrt{y})=1/2$. Therefore,  $f_Y(y) = \frac{1}{2\sqrt{y}}, 0<y<1$.  The histogram of simulated values seems consistent with this shape.  (The density blows up at 0.)

```

(ref:cap-square-function-plot) A plot of the function $x\mapsto x^2$ for $-1<x<1$. The dotted lines illustrate that $x^2\le 0.49$ if and only if $-\sqrt{0.49}\le x\le \sqrt{0.49}$.

```{r, square-function-plot, echo = FALSE, fig.cap="(ref:cap-square-function-plot)"}

g0 = 0.49
x0 = sqrt(g0)
x = seq(-1, 1, 0.0001)
gx = x^2
plot(x, gx, type="l", xlab="x", ylab=expression(x^2),xaxs="i",yaxs="i")
segments(x0=x0, y0=0, x1=x0, y1=g0, col="orange", lty=2, lwd=2)
segments(x0=-x0, y0=0, x1=-x0, y1=g0, col="orange", lty=2, lwd=2)
segments(x0=-1, y0=0, x1=-1, y1=g0, col="skyblue", lty=1, lwd=10)
segments(x0=-1, y0=g0, x1=x0, y1=g0, col="skyblue", lty=2, lwd=2)
segments(x0=-x0, y0=0, x1=x0, y1=0, col="orange", lty=1, lwd=10)

```

The table below helps us see how the transformation $Y = X^2$ "pushes" density towards 0 if $X$ has a Uniform(-1, 1) distribution.

```{r, echo = FALSE}


x = seq(0, 0.9, 0.1)
x1 = x + 0.1

x_intervals = apply(cbind("(", x, ", ", x1, ")"), 1 , paste , collapse = "" )
  
u = round(sqrt(x), 4)
u1 = round(sqrt(x1), 4)

u_intervals = apply(cbind("(", -u1, ", ", -u, ") U (", u, ",", u1, ")"), 1 , paste , collapse = "" )

pu = (u1 - u)
du = pu / (x1 - x)

df = data.frame(x_intervals, u_intervals, pu * 2, pu, x1 - x, du)

df %>% kbl(col.names = c("Y interval",
                         "X interval",
                         "Length of X interval",
                         "Probability",
                         "Length of Y interval",
                         "Height of Y interval"),
           digits = 4) %>%
  kable_styling(fixed_thead = TRUE)




```

We can use the table to sketch a histogram.

```{r, echo = FALSE}
plot(x1 - 0.05, du, type = "h", lwd = 52, lend = 1, col = "skyblue",
     xlab = "Y",
     ylab = "Density",
     xlim = c(0, 1))
axis(1, 0.1 * (0:10))
```

If we continued the above process with narrower and narrower $Y$ intervals we would arrive at the smooth pdf given by $f_Y(y) = \frac{1}{2\sqrt{y}}, 0<y<1$; see the black curve in the plot below.

Now we'll approximate the pdf via simulation.
The density blows up at 0 so it's hard for the chunky histogram to capture that, but we see the simulated values follow a distribution described by the smooth $f_Y(y) = \frac{1}{2\sqrt{y}}, 0<y<1$ in the black curve.

```{python}

X = RV(Uniform(-1, 1))
Y = X ** 2

plt.figure()

Y.sim(100000).plot()

# plot the density
from numpy import *
y = linspace(0.001, 1, 1000)
plt.plot(y, 0.5 / sqrt(y), 'k-');
plt.ylim(0, 10);

plt.show()

```

## Joint distributions

Most interesting problems involve two or more[^distribution-11] random variables defined on the same probability space.
In these situations, we can consider how the variables vary together, or jointly, and study their relationship.
The *joint distribution* of random variables $X$ and $Y$ (defined on the same probability space) is a probability distribution on $(x, y)$ *pairs*. In this context, the distribution of one of the variables alone is called a *marginal distribution*.

[^distribution-11]: We mostly focus on the case of two random variables, but analogous definitions and concepts apply for more than two (though the notation can get a bit messier).

Think of a joint distribution of two random variables as being represented by a spinner or "globe" that returns pairs of values.

### Joint probability mass functions

The joint distribution of table discrete random variables can be summarized in a table of possible pairs and their probabilities.

```{example mscoin-joint}
Flip a fair coin four times and record the results in order, e.g. HHTT means two heads followed by two tails.
Recall that in Section \@ref(sim) we considered the *proportion of the flips which immediately follow a H that result in H*.
We cannot measure this proportion if no flips follow a H, i.e. the outcome is either TTTT or TTTH; in these cases, we would discard the outcome and try again.

Let:
  
- $Z$  be the number of flips immediately following H.
- $Y$  be the number of flips immediately following H that result in H.
- $X = Y/Z$  be the proportion of flips immediately following H that result in H.

```

1.  Make a table of all possible outcomes and the corresponding values of $Z, Y, X$.
2.  Make a two-way table representing the joint probability mass function of $Y$ and $Z$.
3.  Construct a spinner for simulating $(Y, Z)$ pairs.
4.  Make a table specifying the pmf of $X$.

```{solution mscoin-joint-sol}
to Example \@ref(exm:mscoin-joint)
```

```{asis, fold.chunk = TRUE}
1. See Table \@ref(tab:mscoin). The **flips which follow H are in bold**.
1. See Table \@ref(tab:mscoin-joint).
There are 16 possible four flip sequences but two of these --- TTTH and TTTT --- are discarded.
So there are only 14 possible outcomes of interest, all equally likely.
We can find the possible $(Y, Z)$ pairs and their probabilities from Table \@ref(tab:mscoin).
For example, $p_{Y,Z}(1,2) = \IP(Y=1, Z=2) = \IP(\{HHTH, HTHH, HHTT, THHT\})=4/14$; $p_{Y,Z}(2,3) = \IP(Y=2, Z=3) = \IP(\{HHHT\})=1/14$.
1. See Figure \@ref(fig:mscoin-joint-spinner).
The spinner returns possible $(Y, Z)$ pairs according to their joint probabilities.
1. See Table \@ref(tab:mscoin-marginal).
We started with the table of possible values, but if we hadn't we still could have obtained the marginal distribution of $X = Y / Z$ from from the joint distribution of $(Y, Z)$.
For example $p_X(1) = \IP(X = 1) = \IP(Y = 1, Z = 1) + \IP(Y = 2, Z = 2) + \IP(Y = 3, Z = 3) = 3/14$.


  
```

| Outcome      | $Z$ |         $Y$ |   $X = Y/Z$ |
|--------------|----:|------------:|------------:|
| H**HHH**     |   3 |           3 |           1 |
| H**HHT**     |   3 |           2 |         2/3 |
| H**HT**H     |   2 |           1 |         1/2 |
| H**T**H**H** |   2 |           1 |         1/2 |
| TH**HH**     |   2 |           2 |           1 |
| H**HT**T     |   2 |           1 |         1/2 |
| H**T**H**T** |   2 |           0 |           0 |
| H**T**TH     |   1 |           0 |           0 |
| TH**HT**     |   2 |           1 |         1/2 |
| TH**T**H     |   1 |           0 |           0 |
| TTH**H**     |   1 |           1 |           1 |
| H**T**TT     |   1 |           0 |           0 |
| TH**T**T     |   1 |           0 |           0 |
| TTH**T**     |   1 |           0 |           0 |
| TTTH         |   0 | not defined | not defined |
| TTTT         |   0 | not defined | not defined |

: (#tab:mscoin) Possible values of (1) $Z$, the number of flips immediately following H, (2) $Y$, the number of flips immediately following H that result in H, and (3) $X$, the proportion of flips immediately following H that result in H, for four flips of a fair coin.

| $y, z$ |    1 |    2 |    3 |
|--------|-----:|-----:|-----:|
| 0      | 5/14 | 1/14 |    0 |
| 1      | 1/14 | 4/14 |    0 |
| 2      |    0 | 1/14 | 1/14 |
| 3      |    0 |    0 | 1/14 |

: (#tab:mscoin-joint) Joint probability mass function of (1) $Z$, the number of flips immediately following H, (2) $Y$, the number of flips immediately following H that result in H, for four flips of a fair coin. Interior cells represent $p_{Y, Z}(y, z) = \IP(Y = y, Z = z)$.

(ref:cap-mscoin-joint-spinner) Spinner representing the joint probability mass function of $(Y, Z)$ where $Z$ is the number of flips immediately following H, and $Y$ is the number of flips immediately following H that result in H, for four flips of a fair coin.

```{r mscoin-joint-spinner, echo = FALSE, fig.cap="(ref:cap-mscoin-joint-spinner)"}
x = c("(0,1)", "(0,2)", "(1,1)", "(1,2)", "(2,2)", "(2,3)", "(3,3)") 
p = c(5, 1, 1, 4, 1, 1, 1) / 14

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = plotp, labels=xp$x) +
  theme(axis.text.x=element_text(size=12, face="bold")) +
  # plot the probabilities as percents inside
  geom_text(aes(y = plotp,
                label = format(percent(p),2)), size=3) +
  ggtitle(paste("Joint distribution of (H after H, Flips after H) \n in four flips of a fair-coin", sep=""))

spinner
```

| $x$ | $p_X(x)$ |
|----:|---------:|
|   0 |     6/14 |
| 1/2 |     4/14 |
| 2/3 |     1/14 |
|   1 |     3/14 |

: (#tab:mscoin-marginal) Marginal distribution of $X$, the proportion of flips immediately following H that result in H, for four flips of a fair coin.

```{definition joint-pmf}

The **joint probability mass function (pmf)** of two *discrete* random variables $(X,Y)$ defined on a probability space with probability measure $\IP$ is the function $p_{X,Y}:\mathbb{R}^2\mapsto[0,1]$ defined by
\[
p_{X,Y}(x,y) = \IP(X= x, Y= y) \qquad \text{ for all } x,y
\]

```

The axioms of probability imply that a valid joint pmf must satisfy

```{=tex}
\begin{align*}
p_{X,Y}(x,y) & \ge 0 \quad \text{for all $x, y$}\\
p_{X,Y}(x,y) & >0 \quad \text{for at most countably many $(x,y)$ pairs (the possible values, i.e., support)}\\
\sum_x \sum_y p_{X,Y}(x,y) & = 1
\end{align*}
```
Remember to specify the possible $(x, y)$ pairs when defining a joint pmf.

For example, let $X$ be the sum and $Y$ the larger of two rolls of a fair four-sided die.
The the joint pmf of $X$ and $Y$ is represented in Table \@ref(tab:dice-joint-dist-twoway).
We can express the table a little more compactly as $$
p_{X, Y}(x, y)
=
\begin{cases}
2/16, & y = 2, 3, 4, \qquad x = y+1, \ldots, 2y-1\\
1/16, & y = 1, 2, 3, 4, \quad x = 2y\\
0, & \text{otherwise}
\end{cases}
$$ Notice the specification of the possible $(x, y)$ pairs.

```{example, poisson-hr-joint}

Let $X$ be the number of home runs hit by the home team, and $Y$ the number of home runs hit by the away team in a randomly selected Major League Baseball game.
Suppose that $X$ and $Y$ have joint pmf
```

$$
p_{X, Y}(x, y)
=
\begin{cases}
e^{-2.3}\frac{1.2^{x}1.1^{y}}{x!y!}, & x = 0, 1, 2, \ldots; y = 0, 1, 2, \ldots,\\
0, & \text{otherwise.}
\end{cases}
$$

1.  Compute and interpret the probability that the home teams hits 2 home runs and the away team hits 1 home run.
2.  Construct a two-way table representation of the joint pmf.
3.  Compute and interpret the probability that each team hits at most 4 home runs.
4.  Compute and interpret the probability that both teams combine to hit a total of 3 home runs.
5.  Compute and interpret the probability that the home team and the away team hit the same number of home runs.

```{solution poisson-hr-joint-sol}
to Example \@ref(exm:poisson-hr-joint)
```

```{asis, fold.chunk = TRUE}

1. Plug $x=2, y=1$ into the joint pmf: $\IP(X = 2, Y = 1) = p_{X, Y}(2, 1) = e^{-2.3}\frac{1.2^2 1.1^1}{2!1!}= 0.0794$.
Over many games, the home teams hits 2 home runs and the away team hits 1 home run in about 7.9% of games.
1. See Table \@ref(tab:poisson-hr-joint-table) and Figure \@ref(fig:poisson-hr-joint-plot) below. The possible values of $x$ are 0, 1, 2, $\ldots$, and similarly for $y$, and any $(x, y)$ pair of these values is possible.  Plug each $(x, y)$ pair into $p_{X,Y}$ like in the previous part.
1. Sum the joint pmf over the top left corner of the table representing the 25 $(x, y)$ pairs where both coordinates are at most 4: (0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 0), $\ldots$, (4, 4).
There is no easy way to do it other than adding all the numbers.
    \begin{align*}
     \IP(X \le 4, Y \le 4) & = p_{X, Y}(0, 0) +  p_{X, Y}(0, 1)+p_{X, Y}(0, 2)+p_{X, Y}(0, 3)+p_{X, Y}(0, 4) + p_{X, Y}(1, 0)\cdots+p_{X, Y}(4, 4)\\
     & = 0.100 + 0.110 + 0.061 + 0.022 + 0.006 + \cdots + 0.0005 = 0.987
    \end{align*}
    While technically any pair of nonnegative integers is possible, almost all of the probability is concentrated on pairs where neither coordinate is more than 4.
    Over many games, neither team hits more than 4 home runs in 98.7% of games.
1. The corresponding $(x, y)$ pairs are (0, 3), (1, 2), (2, 1), (3, 0).
    \begin{align*}
     \IP(X + Y = 3) & = p_{X, Y}(0, 3) +  p_{X, Y}(1, 2)+p_{X, Y}(2, 1)+p_{X, Y}(3, 0)\\
     & = 0.022 + 0.073 + 0.079 + 0.029 = 0.203
    \end{align*}
    We will see an easier way to do this later.
    Over many games, both teams combine to hit a total of 3 home runs in about 20.3% of games.
1. Sum over values where $x=y$, the diagonal cells of the table.
    \begin{align*}
    \IP(X = Y) & = p_{X, Y}(0, 0) + p_{X, Y}(1, 1) + p_{X, Y}(2, 2) + \cdots \\
    & = 0.100 + 0.132 + 0.044 + \cdots = 0.283.
    \end{align*}
    Over many games, the two teams hit in the same number of home runs in about 28.3% of games.
    Technically this is an infinite series, but the contributions of the terms after (6, 6) are negligible.
    The sum can be written as an infinite series, but there isn't a shortcut to evaluate it.
    $$
    \IP(X = Y) = \sum_{u = 0}^\infty e^{-2.3}\frac{1.2^u 1.1^u}{u!u!} = 0.283.
    $$

```

(ref:cap-poisson-hr-joint-table) Two-way table representing the joint distribution of $X$ and $Y$ in Example \@ref(exm:poisson-hr-joint).

```{r, poisson-hr-joint-table, echo = FALSE}

table2 <- NULL

xmax = 8

for (x in 0:xmax){
  table2 <- rbind(table2, dpois(x, 1.2) * dpois(0:xmax, 1.1))
}

table2_totals <- rbind(table2,
                rbind(c(rep(NA, xmax + 1)),
                      dpois(0:xmax, 1.1)))

table2_totals <- cbind(table2_totals,
                rep(NA, xmax + 3),
                c(dpois(0:xmax, 1.2), NA, 1))

options(knitr.kable.NA = '')


kbl(
  data.frame(c(0:xmax, "...", "Total"), table2_totals),
  col.names = c("x, y", 0:xmax, "...", "Total"),
  booktabs = TRUE,
  caption = "(ref:cap-poisson-hr-joint-table)",
  digits = 5
) %>%
  kable_styling(fixed_thead = TRUE) %>%
  scroll_box(width = "100%", height = "100%")

```

(ref:cap-poisson-hr-joint-plot) Tile plot representing the joint distribution of $X$ and $Y$ in Example \@ref(exm:poisson-hr-joint).

```{r poisson-hr-joint-plot, echo = FALSE, fig.cap="(ref:cap-poisson-hr-joint-plot)"}

joint_poisson = expand_grid(x = 0:xmax, y = 0:xmax) %>%
  mutate(pxy = dpois(x, 1.2) * dpois(y, 1.1))

ggplot(joint_poisson %>%
         mutate(x = factor(x), y = factor(y)),
       aes(x, y, fill = pxy)) +
  geom_tile() +
  scale_fill_viridis_c() +
  theme_classic() +
  labs(fill = expression(p[X][","][Y](x, y)))


```

Recall that we can obtain marginal distributions from a joint distribution.
*Marginal pmfs* are determined by the joint pmf via the law of total probability.
If we imagine a plot with blocks whose heights represent the joint probabilities, the marginal probability of a particular value of one variable can be obtained by "stacking" all the blocks corresponding to that value.
In terms of a two-way table, a marginal distribution can be obtained by "collapsing" the table by summing rows or columns.

```{=tex}
\begin{align*}
p_X(x) & = \sum_y p_{X,Y}(x,y) & & \text{a function of $x$ only}
\\
p_Y(y) & = \sum_x p_{X,Y}(x,y) & & \text{a function of $y$ only}
\\
\end{align*}
```
```{example, poisson-hr-joint-marginal}

Continuining Example \@ref(exm:poisson-hr-joint).
Let $X$ be the number of home runs hit by the home team, and $Y$ the number of home runs hit by the away team in a randomly selected Major League Baseball game.
Suppose that $X$ and $Y$ have joint pmf
```

$$
p_{X, Y}(x, y)
=
\begin{cases}
e^{-2.3}\frac{1.2^{x}1.1^{y}}{x!y!}, & x = 0, 1, 2, \ldots; y = 0, 1, 2, \ldots,\\
0, & \text{otherwise.}
\end{cases}
$$

1.  Compute and interpret the probability that the home team hits 2 home runs.
2.  Find the marginal pmf of $X$, and identify the marginal distribution by name.
3.  Compute and interpret the probability that the away team hits 1 home run.
4.  Find the marginal pmf of $Y$, and identify the marginal by name.
5.  Use the joint pmf to compute the probability that the home team hits 2 home runs and the away team hits 1 home run. How does it relate to the marginal probabilities from the previous parts? What does this imply about the events $\{X = 2\}$ and $\{Y = 1\}$?
6.  How does the joint pmf relate to the marginal probabilities from the previous parts? What do you think this implies about $X$ and $Y$?
7.  In light of the previous part, how you could use spinners to simulate and $(X, Y)$ pair?

```{solution poisson-hr-joint-marginal-sol}
to Example \@ref(exm:poisson-hr-joint-marginal)
```

```{asis, fold.chunk = TRUE}

1. Sum over the values of $y$ in the $x=2$ row of the joint pmf table. Mathematically, we plug 2 in for $x$ in the joint pmf and sum over all the possible values of $y$
    \begin{align*}
    \IP(X = 2) & = \sum_{y=0}^\infty p_{X, Y}(2, y)\\
    & = \sum_{y=0}^\infty   e^{-2.3}\frac{1.2^2 1.1^y}{2!y!}\\
    & = e^{-2.3}\frac{1.2^2}{2!}\sum_{y=0}^\infty   \frac{1.1^y}{y!}\\
    & = e^{-2.3}\frac{1.2^2}{2!}\left(e^{1.1}\right)\\
    & = e^{-1.2}\frac{1.2^2}{2!} = 0.217.
    \end{align*}
    Over many games, the home team hits 2 home runs in about 21.7% of games.
1. The possible values of $X$ are $0, 1, 2, \ldots$. For corresponding probabilities see the total column containing the row sums in Table \@ref(tab:poisson-hr-joint-table).
Mathematically, for a given $x$ (like 2 in the previous part) we plug it into the joint pmf and sum over the possible $y$ values. Basically, we repeat the calculation from the previous part but for a generic possible value of $x= 0, 1, 2\ldots$
    \begin{align*}
    p_X(x) & = \sum_{y=0}^\infty p_{X, Y}(x, y)\\
    & = \sum_{y=0}^\infty   e^{-2.3}\frac{1.2^x 1.1^y}{x!y!}\\
    & = e^{-2.3}\frac{1.2^x}{x!}\sum_{y=0}^\infty   \frac{1.1^y}{y!}\\
    & = e^{-2.3}\frac{1.2^x}{x!}\left(e^{1.1}\right)\\
    & = e^{-1.2}\frac{1.2^x}{x!}.
    \end{align*}
    The marginal distribution of $X$ is the Poisson(1.2) distribution. Notice that the marginal pmf of $X$ is a function of values of $X$ alone: $p_X(x)= e^{-1.2}\frac{1.2^x}{x!}, x = 0, 1, 2\ldots$. 
1. Sum over the values of $x$ in the $y=1$ column of the joint pmf table. Mathematically, we plug 1 in for $y$ in the joint pmf and sum over all the possible values of $x$
    \begin{align*}
    \IP(Y = 1) & = \sum_{x=0}^\infty p_{X, Y}(x, 1)\\
    & = \sum_{x=0}^\infty   e^{-2.3}\frac{1.2^x 1.1^1}{x!1!}\\
    & = e^{-2.3}\frac{1.1^1}{1!}\sum_{x=0}^\infty   \frac{1.2^x}{x!}\\
    & = e^{-2.3}\frac{1.1^1}{1!}\left(e^{1.2}\right)\\
    & = e^{-1.1}\frac{1.1^1}{1!} = 0.366.
    \end{align*}
    Over many games, the away team hits 1 home run in about 36.6% of games.
1. The possible values of $Y$ are $0, 1, 2, \ldots$. For corresponding probabilities see the total row containing the column sums in Table \@ref(tab:poisson-hr-joint-table).
Mathematically, for a given $y$ (like 1 in the previous part) we plug it into the joint pmf and sum over the possible $x$ values. Basically, we repeat the calculation from the previous part but for a generic possible value of $y= 0, 1, 2\ldots$
    \begin{align*}
    \IP(Y = y)&  = \sum_{x=0}^\infty p_{X, Y}(x, y)\\
    & = \sum_{x=0}^\infty   e^{-2.3}\frac{1.2^x 1.1^y}{x!y!}\\
    & = e^{-2.3}\frac{1.1^y}{y!}\sum_{x=0}^\infty   \frac{1.2^x}{x!}\\
    & = e^{-2.3}\frac{1.1^y}{y!}\left(e^{1.2}\right)\\
    & = e^{-1.1}\frac{1.1^y}{y!}.
    \end{align*}
    The marginal distribution of $Y$ is the Poisson(1.1) distribution. Notice that the marginal pmf of $Y$ is a function of values of $Y$ alone: $p_Y(y)= e^{-1.1}\frac{1.1^y}{y!}, y = 0, 1, 2\ldots$.
1. The joint probability turns out to be the product of the marginal probabilities we computed earlier in this example.
$$
\scriptsize{
\IP(X = 2, Y=1) = p_{X,Y}(2, 1) = 0.079= e^{-2.3}\frac{1.2^{2}1.1^{1}}{2!1!} = \left(e^{-1.2}\frac{1.2^{2}}{2!}\right)\left(e^{-1.1}\frac{1.1^{1}}{1!}\right) = (0.217)(0.366) = p_X(2)p_Y(1) = \IP(X = 2)\IP(Y = 1)}
$$
Therefore, the $\{X = 2\}$ and $\{Y = 1\}$ are independent events.
1. Similar to the previous part, we see that the joint pmf factors into the product of the marginal pmfs  we identified earlier in this example.
$$
\scriptsize{
\IP(X = x, Y=y) = p_{X,Y}(x, y) = e^{-2.3}\frac{1.2^{x}1.1^{y}}{x!y!} = \left(e^{-1.2}\frac{1.2^{x}}{x!}\right)\left(e^{-1.1}\frac{1.1^{y}}{y!}\right) = p_X(x)p_Y(y) = \IP(X = x)\IP(Y = y)}
$$
Recalling the general relationship "independent if and only if joint equals product of marginals" this seems to imply $X$ and $Y$ are independent random variables.
1. We could construct a spinner corresponding to the joint distribution that returns $(X, Y)$ pairs.
But since $X$ and $Y$ are independent we can spin a Poisson(1.2) distribution spinner to simulate $X$, and spin a Poisson(1.1) distribution spinner to simulate $Y$.
That is, since $X$ and $Y$ are independent we can simulate their values separately.
```

In the previous example, the marginal distribution of $X$ is the Poisson(1.2) distribution and the marginal distribution of $Y$ is the Poisson(1.1) distribution.
Furthermore $X$ and $Y$ are also *independent*, because their marginal pmf is the product of their joint pmfs.
We will study independence of random variables in more detail soon.

The code below simulates $(X, Y)$ pairs from the joint distribution in Example \@ref(exm:poisson-hr-joint); compare the results to Table \@ref(tab:poisson-hr-joint-table) and Figure \@ref(fig:poisson-hr-joint-plot) The `*` syntax indicates that $X$ and $Y$ are independent.
The code `X, Y = RV(Poisson(1.2) * Poisson(1.1))` defines the joint distribution of $X$ and $Y$ by specifying

-   The marginal distribution of $X$ is Poisson(1.2)
-   The marginal distribution of $Y$ is Poisson(1.1)
-   The joint distribution of $X$ and $Y$ is the product (`*`) of the marginal distributions, so $X$ and $Y$ are independent.

(ref:cap-poisson-joint-ggplot) Tile plot of $(X, Y)$ pairs simulated from the joint distribution in Example \@ref(exm:poisson-hr-joint).

```{python}

X, Y = RV(Poisson(1.2) * Poisson(1.1))

x_and_y = (X & Y).sim(10000)

x_and_y
```

```{python}
x_and_y.tabulate(normalize = True)
```

```{python, eval = FALSE}
x_and_y.plot('tile')
```

```{python, echo = FALSE, poisson-joint-ggplot, fig.cap="(ref:cap-poisson-joint-ggplot)"}

plt.figure()
x_and_y.plot('tile')
plt.show()

```

```{example chicken-egg}
(Blitzstein's chicken and egg story.) Suppose $N$, the number of eggs a chicken lays in a randomly selected week, has a Poisson($6.5$) distribution.
Each egg hatches with probability $0.8$, independently of all other eggs.
Let $X$ be the number of eggs that hatch.
Let $p_{N, X}$ denote the joint pmf of $N$ and $X$.
```

1.  Identify the possible $(N, X)$ pairs.
2.  Identify the conditional distribution of $X$ given $N=7$.
3.  Compute and interpret $p_{N, X}(7, 7)$. Hint: compute $\IP(X = 7|N = 7)$ and use the multiplication rule.
4.  Compute and interpret $p_{N, X}(7, 5)$
5.  Identify the conditional distribution of $X$ given $N=n$ for a generic $n=0, 1, 2, \ldots$.
6.  Find an expression for the joint pmf $p_{N, X}$. Be sure to specify the possible values.
7.  Are $N$ and $X$ independent?
8.  Make a table of the joint pmf.

```{solution chicken-egg-sol}
to Example \@ref(exm:chicken-egg)
```

```{asis, fold.chunk = TRUE}
1.  Since $N$ has a Poisson distribution its possible values are $0, 1, 2, \ldots$. $X$ also takes nonnegative integer values, but since the number of eggs that hatch can't be more than the number of eggs, we must have $X \le N$.
So the possible $(n, x)$ pairs satisfy
$$
n = 0, 1, 2, \ldots; x = 0, 1, \ldots, n
$$
1.  Given $N=7$ eggs, each of the 7 eggs either hatches or not, with probability 0.8, independently, and $X$ counts the number of eggs that hatch.
This is a Binomial situation with 7 independent trials and probability of success 0.8 on each trial.
Therefore the conditional distribution of $X$ given $N=7$ is the Binomial(7, 0.8) distribution.
1.  Given that there are 7 eggs, the probability that they all hatch (independently) is $\IP(X = 7|N = 7) = 0.8^7$.
\begin{align*}
p_{N, X}(7, 7) & = \IP(N = 7, X = 7) & & \\
& = \IP(X = 7 | N = 7)\IP(N = 7) & & \text{multiplication rule}\\
& = 0.8^7\IP(N=7) & & \text{conditional Binomial}\\
& = 0.8^7\left(e^{-6.5}\frac{6.5^7}{7!}\right) & & \text{marginal Poisson}\\
& = e^{-6.5}\frac{(0.8\times 6.5)^7}{7!} = 0.030 & & 
\end{align*}
In about 3% of weeks the chicken lays 7 eggs and all 7 hatch.
1.  Given that there are 7 eggs, the probability that exactly 5 hatch (independently) is $\IP(X = 5|N = 7) = \binom{7}{5}(0.8)^5(1-0.8)^{7-5}$, since the conditional distribution of $X$ given $N=7$ is the Binomial(7, 0.8) distribution.
\begin{align*}
p_{N, X}(7, 5) & = \IP(N = 7, X = 5) & & \\
& = \IP(X = 5 | N = 7)\IP(N = 7) & & \text{multiplication rule}\\
& = \left(\binom{7}{5}(0.8)^5(1-0.8)^{7-5}\right)\IP(N=7) & & \text{conditional Binomial}\\
& = \left(\binom{7}{5}(0.8)^5(1-0.8)^{7-5}\right)\left(e^{-6.5}\frac{6.5^7}{7!}\right) & & \text{marginal Poisson}\\
& = e^{-6.5}\left(\frac{(0.8\times 6.5)^5}{5!}\right)\left(\frac{((1-0.8)\times 6.5)^{7-5}}{(7-5)!}\right) = 0.040 & & \text{algebra with binomial coefficient} 
\end{align*}
In about 4% of weeks the chicken lays 7 eggs and exactly 5 hatch.
1.  For any $n=0, 1, 2, \ldots$, the conditional distribution of $X$ given $N=n$ is the Binomial($n$, 0.8) distribution. (If $N=0$, this is a "degenerate" Binomial distribution with $\IP(X = 0 | N = 0) = 1$.) 
1.  Repeat calculations like the ones above for generic $n=0, 1, 2, \ldots$ and $x = 0, 1, \ldots, n$:
\begin{align*}
p_{N, X}(n, x) & = \IP(N = x, X = x) & & \\
& = \IP(X = x | N = n)\IP(N = n) & & \text{multiplication rule}\\
& = \left(\binom{n}{x}(0.8)^x(1-0.8)^{n-x}\right)\IP(N=n) & & \text{conditional Binomial}\\
& = \left(\binom{n}{x}(0.8)^x(1-0.8)^{n-x}\right)\left(e^{-6.5}\frac{6.5^n}{n!}\right) & & \text{marginal Poisson}\\
& = e^{-6.5}\left(\frac{(0.8\times 6.5)^x}{x!}\right)\left(\frac{((1-0.8)\times 6.5)^{n-x}}{(n-x)!}\right)  & & \text{algebra with binomial coefficient} 
\end{align*}
Therefore the joint pmf is
$$
p_{N, X}(n, x) = 
\begin{cases}
e^{-6.5}\left(\frac{(0.8\times 6.5)^x}{x!}\right)\left(\frac{((1-0.8)\times 6.5)^{n-x}}{(n-x)!}\right), & n=0, 1, 2, \ldots; x = 0, 1, \ldots, n\\
0, & \text{otherwise}
\end{cases}
$$
1. No! The possible values of $X$ depend on $N$.
For example $\IP(X= 2)>0$ and $\IP(N = 1)>0$ but $\IP(N = 1, X = 2) = 0\neq \IP(N=1)\IP(X = 2)$.
1. See Table \@ref(tab:chicken-egg-table) and Figure \@ref(fig:chicken-egg-plot). The expression for $p_{N, X}(n, x)$ is enough to specify the joint distribution, but the table makes it a little more concrete, and the plot helps visualizing the shape of the joint distribution.
We see a pretty strong positive association between $N$ and $X$.

```

(ref:cap-chicken-egg-table) Two-way table representing the joint distribution of $N$ and $X$ in the chicken and egg problem in Example \@ref(exm:chicken-egg). Values are displayed as percents; e.g. $p_{N, X}(7, 5)$ is 4.25%.

```{r, chicken-egg-table, echo = FALSE}

table2 <- NULL

xmax = 13

for (x in 0:xmax){
  table2 <- rbind(table2, 100 * dpois(x, 6.5) * dbinom(0:xmax, x, 0.8))
}

table2_totals <- rbind(table2,
                rbind(c(rep(NA, xmax + 1)),
                      100 * dpois(0:xmax, 6.5)))

table2_totals <- cbind(table2_totals,
                rep(NA, xmax + 3),
                c(100 * dpois(0:xmax, 6.5 * 0.8), NA, 1))

options(knitr.kable.NA = '')


kbl(
  data.frame(c(0:xmax, "...", "Total"), table2_totals),
  col.names = c("n, x", 0:xmax, "...", "Total"),
  booktabs = TRUE,
  caption = "(ref:cap-chicken-egg-table)",
  digits = 3
) %>%
  kable_styling(fixed_thead = TRUE) %>%
  scroll_box(width = "100%", height = "100%")

```

(ref:cap-chicken-egg-plot) Tile plot representing the joint distribution of $N$ and $X$ in the chicken and egg problem in Example \@ref(exm:chicken-egg).

```{r chicken-egg-plot, echo = FALSE, fig.cap="(ref:cap-chicken-egg-plot)"}

nmax = 13

chicken_egg = expand_grid(n = 0:nmax, x = 0:nmax) %>%
  mutate(pxy = dpois(n, 6.5) * dbinom(x, n, 0.8))

ggplot(chicken_egg %>%
         mutate(n = factor(n), x = factor(x)),
       aes(x = n, y = x, fill = pxy)) +
  geom_tile() +
  scale_fill_viridis_c() +
  theme_classic() +
  labs(fill = expression(p[N][","][X](n, x)))


```



In the previous example we constructed the joint distribution using the general relationship
$$
\text{joint} = \text{conditional}\times\text{marginal}
$$
We'll study conditional distributions in more detail in the next section.



Be sure to distinguish between joint and marginal distributions.

-   The joint distribution is a distribution on $(X, Y)$ pairs. A mathematical expression of a joint distribution is a function of both values of $X$ and values of $Y$. Pay special attention to the possible values; the possible values of one variable might be restricted by the value of the other.
-   The marginal distribution of $Y$ is a distribution on $Y$ values only, regardless of the value of $X$. A mathematical expression of a marginal distribution will have only values of the single variable in it; for example, an expression for the marginal distribution of $Y$ will only have $y$ in it (no $x$, not even in the possible values).

### Joint probability density fuctions




Recall that the *joint distribution* of random variables $X$ and $Y$ (defined on the same probability space) is a probability distribution on $(x, y)$ *pairs*.
The joint distribution of two continuous random variables can be specified by a joint pdf, a *surface* specifying the density of $(x, y)$ pairs.
The probability that the $(X,Y)$ pair of random variables lies is some region is the *volume* under the joint pdf surface over the region.

Bivariate Normal distributions are one commonly used family of joint continuous distribution.
For example, the following plots represent the joint pdf surface of the Bivariate Normal distribution in the meeting problem in Section \@ref(sec-sim-bvn), with correlation 0.7.

```{r, echo = FALSE}
x = seq(0, 60, 1)
y = seq(0, 60, 1)
fxy = function(x, y) dbvn(x, y, 30, 30, 10, 10, 0.7)

z = outer(x, y, fxy)

persp(x, y, z, theta = -30, phi = 25, 
      shade = 0.75, col = "skyblue", expand = 0.5, r = 2, 
      ltheta = 25, ticktype = "detailed", 
      xlab = "R",
      ylab = "Y",
      zlab = "Density")
```

```{r meeting-probmeasure333, ref.label = 'meeting-probmeasure33', echo = FALSE, warning = FALSE, message = FALSE, fig.cap="A Bivariate Normal distribution"}

```

```{example sketch-joint}

Suppose that

- $X$ has a Normal(0, 1) distribution
- $U$ has a Uniform(-2, 2) distribution
- $X$ and $U$ are generated independently
- $Y = UX$.

Sketch a plot representing the joint pdf of $X$ and $Y$.
Be sure to label  axes with appropriate values.
```

```{solution sketch-joint-sol}
to Example \@ref(exm:sketch-joint)
```

```{asis, fold.chunk = TRUE}

Since $X$ and $Y$ are two continuous random variables, a scatterplot or joint density plot is appropriate.
First identify possible values.

Since $X$ has a  Normal(0, 1) distribution, almost all of the values of $X$ will fall between -3 and 3, so we can label the $X$ axis from -3 to 3. (Or -2 to 2 is also fine for a sketch.)

The value of $Y$ depends on both $X$ and $U$.
For example, if $X = 2$ then $Y=2U$. Since $U$ has a Uniform distribution, so does $2U$, in particular, $2U$ has a Uniform(-4, 4) distribution, so $Y$ must be between -4 and 4 if $X = 2$; similarly if $X = -2$.
In general, if $X=x$ then  $Y$ has a Uniform($-2|x|$, $2|x|$) distribution, so larger values of $|X|$ correspond to more extreme values of $Y$.
Since most values of X lie between -3 and 3, most values of Y lie between -6 and 6, so we can label the Y axis from -6 to 6. (Or -4 to 4 for a sketch.)

But as noted above, not all (X, Y) pairs are possible; only pairs within the region bounded by the lines $y=2x$ and $y=-2x$ have nonzero density.

Because $X$ has a marginal Normal(0, 1) distribution, values of $X$ near 0 are more likely, and values far from 0 are less likely.
Within each vertical strip corresponding to an $x$ value, the Y values are distributed uniformly between $-2|x|$ and $2|x|$.
So as we move away from $x=0$, the density height along each vertical strip between $-2x$ and $2x$ decreases, both because values away from 0 are less likely, and because the density is stretched thinner over longer vertical strips.


```

(ref:cap-sketch-joint) Hand sketch of a plot representing the joint pdf in \@ref(exm:sketch-joint)

```{r sketch-joint-plot, echo=FALSE, fig.cap="(ref:cap-sketch-joint)"}

knitr::include_graphics(c("_graphics/sketch-joint.png"))

```

Here are a few questions to ask yourself when sketching plots.

-   *What is one possible plot point? A few possible points?* It can often be difficult to know where to start, so just identifying a few possibilities can help.
-   *What type of plot is appropriate?* Depends on the number (one or two) and types (discrete or continuous) of random variables involved.
-   *What are the possible values of the random variable(s)?* After you answer this question you can start labeling your variable axes. For two random variables, be sure to identify possible *pairs* of values; identify the regions in the plot corresponding to possible pairs.
-   *What ranges of values are more likely? Less likely?* The first few questions help you get a solid skeleton of a plot. Now you can start to sketch the shape of the distribution.

```{definition joint-pdf}

The **joint probability density function (pdf)** of two *continuous* random variables $(X,Y)$ defined on a probability space with probability measure $\IP$ is the function $f_{X,Y}:\reals^2\mapsto[0,\infty)$ which satisfies, for any $S\subseteq \reals^2$,
\[
 \IP[(X,Y)\in S] = \iint\limits_{S}  f_{X,Y}(x,y)\, dx dy 
\]


```

A joint pdf is a probability distribution on $(x, y)$ *pairs*.
A joint pdf is a surface with height $f_{X,Y}(x,y)$ at $(x, y)$.
The probability that the $(X,Y)$ pair of random variables lies in the region $A$ is the *volume* under the pdf surface over the region $A$

```{r joint-pdf-bvn, echo=FALSE, fig.show="hold", out.width="50%", fig.cap="(Left) A joint pdf. (Right) Volume under the surface represents probability. (Images from the [SOCR Bivariate Normal distribution applet](http://socr.ucla.edu/htmls/HTML5/BivariateNormal/).)"}

knitr::include_graphics(c("_graphics/socr-bvn.png", "_graphics/socr-bvn2.png"))

```

A valid joint pdf must satisfy \begin{align*}
f_{X,Y}(x,y) & \ge 0\\
\int_{-\infty}^\infty\int_{-\infty}^\infty f_{X,Y}(x,y)\, dx dy &  = 1
\end{align*} Given a specific pdf, the generic bounds $(-\infty, \infty)\times(-\infty, \infty)$ in the above integral should be replaced by the range of possible pairs of values, that is, those $(x, y)$ pairs for which $f_{X, Y}(x, y)>0$.

Recall that density is not probability.
The height of the density surface at a particular $(x,y)$ pair is related to the probability that $(X, Y)$ takes a value "close to[^distribution-12]" $(x, y)$ $$
\IP(x-\ep/2<X <  x+\ep/2,\; y-\ep/2<Y < y+\ep/2) = \ep^2 f_{X, Y}(x, y) \qquad \text{for small $\ep$}
$$

[^distribution-12]: You can have different precisions for $X$ and $Y$, e.g., $\ep_x, \ep_y$, but using one $\ep$ makes the notation a little simpler.

We now consider a continuous analog of rolling two fair four-sided dice.
Instead of rolling a die which is equally likely to take the values 1, 2, 3, 4, we spin a Uniform(1, 4) spinner that lands uniformly in the continuous interval $[1, 4]$.
When studying continuous random variables, it is often helpful to think about how a discrete analog behaves.
So take a minute to recall the joint and marginal distributions of $X$ and $Y$ in the dice rolling example before proceeding.



```{example uniform-sum-max-pdf}
Spin the Uniform(1, 4) spinner twice, and let $X$ be the sum of the two spins, and $Y$ the larger spin (or the common value if there is a tie).


```

1.  Identify the possible values of $X$, the possible values of $Y$, and the possible $(X, Y)$ pairs. (Hint: think about the possible values of the spins and how $X$ and $Y$ are defined.)
2.  If $U_1$ and $U_2$ are the results of the two spins, explain why $\text{P}(U_1 = U_2) = 0$.
3.  Explain intuitively why the joint density is constant over the region of possible $(X, Y)$ pairs. (Hint: compare to the discrete four-sided die case. There the probability of $(x, y)$ pairs was 2/16 for most possible pairs, because there were two pairs of rolls associated with each $(x, y)$ pair, e.g., rolls of (3, 2) or (2, 3) yield (5, 4) for $(X, Y)$. The exception was the $(x, y)$ pairs with probability 1/16 which correspond to rolling doubles, e.g. roll of (2, 2) yield (4, 2) for $(X, Y)$. Explain why we don't need to worry about treating doubles as a separate case with the Uniform(1, 4) spinner.)
4.  Sketch a plot of the joint pdf.
5.  Find the joint pdf of $(X, Y)$.
6.  Use geometry to find $\IP(X <4, Y > 2.5)$.

```{solution uniform-sum-max-pdf-sol}
to Example \@ref(exm:uniform-sum-max-pdf)
```

```{asis, fold.chunk = TRUE}

1. Marginally, $X$ takes values in $[2, 8]$ and $Y$ takes values in $[1, 4]$. However, not every value in $[2, 8]\times [1, 4]$ is possible.
    - We must have  $Y \ge 0.5 X$, or equivalently, $X \le 2Y$. For example, if $X=4$ then $Y$ must be at least 2, because if the larger of the two spins were less than 2, then both spins must be less than 2, and the sum must be less than 4.
    - We must have $Y \le X - 1$, or equivalently, $X \ge Y + 1$. For example, if $Y=3$, then one of the spins is 3 and the other one is at least 1, so the sum must be at least 4.
    - So the possible $(x, y)$ pairs must satisfy $0.5x \le y \le x-1$ (or equivalently $y +1\le x \le 2y$), as well as $2\le x\le8$ and $1\le y \le 4$
1. $\text{P}(U_1 = U_2) = 0$ because $U_1$ and $U_2$ are continuous (and independent). Whatever $U_1$ is, say 2.35676355643243455..., the probability that $U_2$ equals that value, with infinite precision, is 0.
1. In the four-sided-die rolling situation there are basically two cases.  Each $(X, Y)$ pair that correspond to a tie --- that is each $(X, Y)$ pair with $X = 2Y$ --- has probability 1/16 (rolls of (1, 1), (2, 2), (3, 3), (4, 4).  Each of the other possible $(X, Y)$ pairs has probability 2/16.
    In our continuous situation, consider a single $(X, Y)$ pair, say (5.7, 3.1).  There are two outcomes --- that is, pairs of spins --- for which $X=5.7, Y=3.1$, namely (3.1, 2.6) and (2.6, 3.1).  Like (5.7, 3.1), most of the possible $(X, Y)$ values correspond to exactly two outcomes.  The only ones that do not are the values with $Y = 0.5X$ that lie along the south/east border of the triangular region.
    The pairs $(X, 0.5X)$ only correspond to exactly one outcome.  For example, the only outcome corresponding to (6, 3) is the $(U_1, U_2)$ pair (3, 3); that is, the only way to have $X=6$ and $Y=3$ is to spin 3 on both spins.  But because of the previous part, we don't really need to worry about the ties as we did in the discrete case.
    Excluding ties, roughly, each pair in the triangular region of possible $(X, Y)$ pairs corresponds to exactly two outcomes (pairs of spins), and since the outcomes are uniformly distributed (over $[1, 4]\times[1, 4]$) then the $(X, Y)$ pairs are also uniformly distributed (over the triangular region of possible values).
1. See Figure \@ref(fig:dice-continuous-sum-max-joint-plot). The joint density is constant over the triangular region of possible values. The plot provided is really a three-dimensional plot.  The base is the triangular region which represents the possible $(X, Y)$ pairs.  There is a surface floating above this region which has constant height.
1. The joint density of $(X, Y)$ is constant over the range of possible values.  The region $\{(x, y): 2<x<8, 1<y<4, x/2<y<x-1\}$ is a triangle with area 4.5.  The joint pdf is a surface of constant height floating above this triangle.  The volume under the density surface is the volume of this triangular "wedge".  If the constant height is $1/4.5 = 2/9\approx 0.222$, then the volume under the surface will be 1.  Therefore, the joint pdf of $(X, Y)$ is
    \[
    f_{X, Y}(x, y) =
      \begin{cases}
    2/9, & 2<x<8,\; 1<y<4,\; x/2<y<x-1,\\
    0, & \text{otherwise}
    \end{cases}
    \]
1. $\IP(X <4, Y > 2.5) = 1/36$. The probability is the volume under the pdf over the region of interest.  The base of the triangular wedge is $\{(x, y): 3.5<x<4, 2.5<y<3, x/2<y\}$, a region which has area $(1/2)(4-3.5)(3-2.5) = 1/8$.  Therefore, the volume of the triangular wedge that has constant height 2/9 is $(2/9)(1/8) = 1/36$.
```

(ref:cap-dice-continuous-sum-max-joint-pdf) Joint distribution of $X$ (sum) and $Y$ (max) of two spins of the Uniform(1, 4) spinner. The triangular region represents the possible values of $(X, Y)$ the height of the density surface is constant over this region and 0 outside of the region.

```{r dice-continuous-sum-max-joint-plot, echo = FALSE, fig.cap="(ref:cap-dice-continuous-sum-max-joint-pdf)"}
# | label: dice-continuous-sum-max-joint
# | echo: false
# | fig-cap: ""


dfA <- data.frame(x = c(2, 8, 5, 2),
                 y = c(1, 4, 4, 1),
                 v = c(1, 1, 1, 1))

pA <- ggplot(data = dfA, aes(x = x, y = y)) +
  geom_polygon(fill="#21908cff", show.legend = FALSE) +
  scale_x_continuous(breaks = 0:8, limits = c(0, 8), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 4), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression("X")) +
  ylab(expression("Y")) +
  theme(plot.title = element_text(hjust = 0.5))

plot(pA)

```

Marginal pdfs can be obtained from the joint pdf by the law of total probability.
In the discrete case, to find the marginal probability that $X$ is equal to $x$, sum the joint pmf $p_{X, Y}(x, y)$ over all possible $y$ values.
The continuous analog is to integrate the joint pdf $f_{X,Y}(x,y)$ over all possible $y$ values to find the marginal density of $X$ at $x$.
This can be thought of as "stacking" or "collapsing" the joint pdf.

\begin{align*}
f_X(x) & = \int_{-\infty}^\infty f_{X,Y}(x,y) dy & & \text{a function of $x$ only}
\\
f_Y(y) & = \int_{-\infty}^\infty f_{X,Y}(x,y) dx & & \text{a function of $y$ only}
\end{align*}


The marginal distribution of $X$ is a distribution on $x$ values only.
For example, the pdf of $X$ is a function of $x$ only (and not $y$).
(Similarly the pdf of $Y$ is a function of $y$ only and not $x$.)

In general the marginal distributions do not determine the joint distribution, unless the RVs are independent.
In terms of a table: you can get the totals from the interior cells, but in general you can't get the interior cells from the totals.

```{example uniform-sum-max-pdf-marginal}
Continuing Example \@ref(exm:uniform-sum-max-pdf). Spin the Uniform(1, 4) spinner twice, and let $X$ be the sum of the two spins, and $Y$ the larger spin.


```

1.  Sketch a plot of the marginal distribution of $Y$. Be sure to specify the possible values.
1.  Suggest an expression for the marginal pdf of $Y$.
1.  Use calculus to derive $f_Y(2.5)$, the marginal pdf of $Y$ evaluated at $y=2.5$.
1.  Use calculus to derive $f_Y$, the marginal pdf of $Y$.
1.  Find $\IP(Y > 2.5)$.
1.  Sketch a plot of the marginal distribution of $X$. Be sure to specify the possible values. (Hint: think "collapsing/stacking" the joint distribution; compare with the dice rolling example.)
1.  Suggest an expression for the marginal pdf of $X$.
1.  Use calculus to derive $f_X(4)$, the marginal pdf of $X$ evaluated at $x=4$.
1.  Use calculus to derive $f_X(6.5)$, the marginal pdf of $X$ evaluated at $x=6.5$.
1. Use calculus to derive $f_X$, the marginal pdf of $X$. Hint: consider $x<5$ and $x>5$ separately.
1. Find $\IP(X < 4)$.
1. Find $\IP(X < 6.5)$.

```{solution uniform-sum-max-pdf-marginal-sol}
to Example \@ref(exm:uniform-sum-max-pdf-marginal)
```

```{asis, fold.chunk = TRUE}

1.  Marginally, the possible values of $Y$ are [1, 4]. For each possible $y$, "collapse out" the $X$ values by "stacking" each horizontal slice. Density will be smallest at 1 and highest at 4; see the plot below and compare to the positively sloped impulse plot in the dice rolling case.
1. Density is 0 at $y=1$ increasing to a maximum at $y=4$.
If we assume that the marginal density of $Y$ is linear in $y$ (similar to the discrete dice rolling case), with a height of 0 at $y=1$ at a height of $c$ at $y=4$, we might guess
    $$
     f_Y(y) = 
       \begin{cases}
     c(y - 1), & 1<y<4,\\
     0, & \text{otherwise.}
          \end{cases}
    $$
    Then find $c$ to make the total area under the pdf equal 1.  The area under the pdf is the area of a triangle with base 4-1 and height $c(4-1)$ so setting $1=(1/2)(4-1)(c(4-1))$ yields $c=2/9$.
    $$
      f_Y(y) =  (2/9)(y-1), \qquad 1<y<4
    $$
    We confirm below that this is the marginal pdf of $Y$.
1. We find the marginal pdf of $Y$ evaluated at $y=2.5$ by "stacking" the density at each pair $(x, 2.5)$ over the possible $x$ values.  For discrete variables, the stacking is achieved by summing the joint pmf over the possible $x$ values. For continuous random variables we integrate the joint pdf over the possible $x$ values corresponding to $y=2.5$.  If $y=2.5$ then $3.5 < x< 5$.  "Integrate out the $x$'s" by computing a $dx$ integral: 
    \[
      f_Y(2.5) = \int_{3.5}^5 (2/9)\, dx = (2/9)x \Bigg|_{x=3.5}^{x=5} = 1/3
    \]
    This agrees with the result of plugging in $y=2.5$ in the expression in the previous part.
1. To find the marginal pdf of $Y$ we repeat the calculation from the previous part for each possible value of $y$.  Fix a $1<y<4$, and replace 2.5 in the previous part with a generic $y$.  The possible values of $x$ corresponding to a given $y$ are $y + 1 < x < 2y$. "Integrate out the $x$'s" by computing a $dx$ integral.  Within the $dx$ integral, $y$ is treated like a constant.
    \[
      f_Y(y) = \int_{y+1}^{2y} (2/9)\, dx = (2/9)x \Bigg|_{x=y+1}^{x=2y} = (2/9)(y-1), \qquad 1<y<4
    \]
    Thus, we have confirmed that our suggested form of the marginal pdf is correct.
1. If we have already derived the marginal pdf of $Y$, we can treat this just like a one variable problem.  Integrate the pdf of $Y$ over $(2.5, 4)$
    \[
      \IP(Y > 2.5) = \int_{2.5}^4 (2/9)(y-1)\, dy = (1/9)(y-1)^2\Bigg|_{y=2.5}^{y=4} = 0.25
    \]
    (Integration is not really needed; instead, sketch a picture and use geometry.)
1.  Marginally, the possible values of $X$ are [2, 8]. For each possible $x$, "collapse out" the $Y$ values by "stacking" each vertical slice. Density will be smallest at 2 and 8 and highest at 5; see the plot below and compare to the triangular impulse plot in the dice rolling case.
1. We see that the density is 0 at $x=2$ and $x=8$ and has a triangular shape with a peak at $x=5$.  If $c$ is the density at $x=5$, then $1 = (1/2)(8-2)c$ implies $c=1/3$. We might guess
    \[
     f_X(x) = 
       \begin{cases}
       (1/9)(x-2), & 2 < x< 5,\\
       (1/9)(8-x), & 5<x<8,\\
       0, & \text{otherwise.}
       \end{cases}
    \]
    We could also write this as $f_X(x) = 1/3 - (1/9)|x - 5|, 2<x<8$.
    We confirm below that this is the marginal pdf of $X$.
1. We find the marginal pdf of $X$ evaluated at $x=4$ by "stacking" the density at each pair $(4, y)$ over the possible $y$ values.  For discrete variables, the stacking is achieved by summing the joint pmf over the possible $y$ values. For continuous random variables we integrate the joint pdf over the possible $y$ values corresponding to $x=4$.  If $x=4$ then $2 < y< 3$.  "Integrate out the $y$'s" by computing a $dy$ integral: 
    \[
      f_X(4) = \int_{2}^3 (2/9)\, dy = (2/9)y \Bigg|_{y=2}^{y=3} = 2/9 \approx 0.222
    \]
    This agrees with the result of plugging in $x=4$ in the expression in the previous part.
1. This is similar to the previous part, but for $x=6.5$, we hit the upper bound of 4 on $y$ values; that is, the range isn't just from $x/2$ to $x-1$, but rather $x/2$ to 4.  If $x=6.5$ then $3.25 < y< 4$.  "Integrate out the $y$'s" by computing a $dy$ integral: 
    \[
      f_X(6.5) = \int_{3.25}^4 (2/9)\, dy = (2/9)y \Bigg|_{y=3.25}^{y=4} = 1/6 \approx 0.167
    \]
    This agrees with the result of plugging in $x=6.5$ in the expression in two parts ago.
1. For $2<x<5$ the bounds on possible $y$ values are $x/2$ to $x-1$
    \[
      f_X(x) = \int_{x/2}^{x-1} (2/9)\, dy = (2/9)y \Bigg|_{y=x/2}^{y=x-1} = (1/9)(x - 2), \qquad 2<x<5.
    \]
    For $5<x<8$ the bounds on possible $y$ values are $x/2$ to $4$
    \[
      f_X(x) = \int_{x/2}^{4} (2/9)\, dy = (2/9)y \Bigg|_{y=x/2}^{y=4} = (1/9)(8 - x), \qquad 5<x<8.
    \]
    So the calculus matches what we did a few parts ago.
1. If we have already derived the marginal pdf of $X$, we can treat this just like a one variable problem.  Integrate the pdf of $X$ over $(2, 4)$
    \[
      \IP(X  < 4) = \int_{2}^4 (1/9)(x-2)\, dx = (1/18)(x-2)^2\Bigg|_{x=2}^{x=4} = 2/9 \approx 0.222
    \]
    (Integration isn't really needed here; instead, sketch a picture and use geometry.)
1. If we have already derived the marginal pdf of $X$, we can treat this just like a one variable problem.  It's easiest to use the complement rule and integrate the pdf of $X$ over $(6.5, 8)$
    \[
      \IP(X  < 6.5) = 1 - \IP(X > 6.5) = 1 - \int_{6.5}^8 (1/9)(8-x)\, dx =1 -  (-1/18)(8-x)^2\Bigg|_{x=6.5}^{x=8} = 1 - 1/8 = 7/8  = 0.875
    \]
    (Integration isn't really needed here; instead, sketch a picture and use geometry.)

```

(ref:uniform-sum-max-pdf-marginal-sum) Marginal distribution of $X$, the sum of two spins of a Uniform(1, 4) spinner.

```{r uniform-sum-max-pdf-marginal-sum-plot, echo = FALSE, fig.cap="(ref:uniform-sum-max-pdf-marginal-sum)"}
# | echo: false
# | layout-ncol: 1
# | fig-cap: "Marginal distribution of X"


ggplot(dfA,
             aes(x = x, y = 1)) +
  geom_segment(aes(x = 2, y = 0, xend = 5, yend = 1 / 3),
               color = "skyblue", size = 2) +
  geom_segment(aes(x = 5, y = 1 / 3, xend = 8, yend = 0),
               color = "skyblue", size = 2) +
  scale_x_continuous(limits = c(2, 8), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 0.7), expand = c(0, 0)) +
  labs(x = "x",
       y = "Density") +
  theme_classic()

```


(ref:uniform-sum-max-pdf-marginal-max) Marginal distribution of $Y$, the larger of two spins of a Uniform(1, 4) spinner.


```{r uniform-sum-max-pdf-marginal-max-plot, echo = FALSE, fig.cap="(ref:uniform-sum-max-pdf-marginal-max)"}
# | echo: false
# | layout-ncol: 1
# | fig-cap: "Marginal distribution of Y"


ggplot(dfA,
             aes(x = x, y = 1)) +
  geom_segment(aes(x = 1, y = 0, xend = 4, yend = 2 / 3),
               color = "orange", size = 2) +
  scale_x_continuous(limits = c(1, 4), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 0.7), expand = c(0, 0)) +
  labs(x = "y",
       y = "Density") +
  theme_classic()

```



```{example joint-quakes}
Let $X$ be the time (hours), starting now, until the next earthquake (of any magnitude) occurs in SoCal, and let $Y$ be the time (hours), starting now, until the second earthquake from now occurs (so that $Y-X$ is the time between the first and second earthquake).
Suppose that $X$ and $Y$ are continuous RVs with joint pdf

```
$$
f_{X, Y}(x, y) = 
\begin{cases}
4e^{-2y}, & 0 < x< y < \infty,\\
0, & \text{otherwise}
\end{cases}
$$

1. Is the joint pdf a function of both $x$ and $y$?  How?
1. Why is $f_{X, Y}(x, y)$ equal to 0 if $y < x$?
1. Sketch a plot of the joint pdf. What does its shape say about the distribution of $X$ and $Y$ in this context?
1. Set up the integral to find $\IP(X > 0.5, Y < 1)$.



```{solution joint-quakes-sol}
to Example \@ref(exm:joint-quakes)
```

```{asis, fold.chunk = TRUE}
1. Yes, the pdf is a function of both $x$ and $y$. Pay attention to the possible values. We need to know both $x$ and $y$ to determine if the density is 0 or not.
1. $X$ is the time from now until the the first earthquake and $Y$ is the time from now until the second, so we must have $Y \ge X$ since the second can't happen before the first!
1. The density is 0 below the line $y = x$. Given any $x$ value, the density over each vertical strip is highest at $y = x$ and then decreases exponentially as $y$ increases.
Given any $y$, the density is constant along the horizontal strip between 0 and $x = y$.
Since as $y$ increases the range of corresponding possible $x$ values --- 0 to $x = y$ --- increases, the density along each horizontal strip gets stretched over longer regions, so the constant height decreases as $y$ increases.
    The density will be highest for pairs where the time until the first earthquake is close to 0, and the second earthquake occurs soon after the first.
    See Figure \@ref(fig:joint-quakes-plot).
1. We will see easier ways of doing this later. But in principle, $\IP(X > 0.5, Y < 1)$ is the volume under the joint pdf over the triangular region $\{x>0.5, y < 1, x < y\}$. The integral is a double integral; you can integrate either $dx$ or $dy$ first, but careful about the bounds on the integrals.
$$
\int_{0.5}^1\left(\int_x^1 4e^{-2y}dy\right)dx = \int_{0.5}^1 4e^{-2y}\left(\int_{0.5}^{y}dx\right)dy = 0.097
$$
For about 9.7% of earthquakes the next two earthquakes happen between 30 minutes and 1 hour later.


```

(ref:cap-joint-quakes-plot) Heat map representing the joint pdf of $X$ and $Y$ in Example \@ref(exm:joint-quakes).


```{r joint-quakes-plot, echo = FALSE, warning = FALSE, message=FALSE, fig.cap="(ref:cap-joint-quakes-plot)"}

x = seq(0.01, 1.5, 0.1)
y = seq(0.01, 1.5, 0.1)

joint_quakes = expand_grid(x = x, y = y) %>%
  mutate(fxy = dgamma(y, shape = 2, rate = 2) * dunif(x, 0, max = y))


ggplot(joint_quakes,
       aes(x = x,
           y = y,
           z = fxy)) +
  geom_raster(aes(fill = fxy), interpolate = TRUE) +
  geom_contour(color = "white") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 1.5)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1.5)) +
  scale_fill_viridis() +
  labs(x = "X",
       y = "Y",
       fill = "Density") +
  coord_fixed() +
  theme_classic()
```



```{example joint-quakes-marginal}
Continuing Example \@ref(exm:joint-quakes).
Let $X$ be the time (hours), starting now, until the next earthquake (of any magnitude) occurs in SoCal, and let $Y$ be the time (hours), starting now, until the second earthquake from now occurs (so that $Y-X$ is the time between the first and second earthquake).
Suppose that $X$ and $Y$ are continuous RVs with joint pdf

```
$$
f_{X, Y}(x, y) = 
\begin{cases}
4e^{-2y}, & 0 < x< y < \infty,\\
0, & \text{otherwise}
\end{cases}
$$




1. Sketch a plot of the marginal pdf of $X$. Be sure to specify possible values.
1. Find the marginal pdf of $X$ at $x=0.5$.
1. Find the marginal pdf of $X$.  Be sure to specify possible values.
Identify the marginal distribution of $X$ be name.
1. Compute and interpret $\IP(X > 0.5)$.
1. Sketch the marginal pdf of $Y$. Be sure to specify possible values.
1. Find the marginal pdf of $Y$ at $y=1.5$.
1. Find the marginal pdf of $Y$.  Be sure to specify possible values of $Y$.
1. Compute and interpret find $\IP(Y < 1)$.
1. Is $\IP(X > 0.5, Y < 1)$ equal to the product of $\IP(X > 0.5)$ and $\IP(Y < 1)$? Why? 



```{solution joint-quakes-marginal-sol}
to Example \@ref(exm:joint-quakes-marginal)
```

```{asis, fold.chunk = TRUE}
1. See Figure \@ref(fig:joint-quakes-plot-exp).
Marginally $X$ can take any positive value.
Collapse out the $Y$ values in the joint pdf.
The vertical strips corresponding to $x$ near 0 are the longest and have the highest density.
So when the vertical strips are collapsed the marginal density will be highest at $x=0$ and decrease as $x$ increases.
1. Collapse the vertical strip corresponding to $x=0.5$ by integrating out the $y$ values; if $x = 0.5$ then density is positive for $y>0.5$.
$$
f_X(0.5) = \int_{0.5}^\infty 4e^{-2y}dy = 2e^{-2(0.5)}
$$
1. Repeat the previous part with 0.5 replaced by a generic $x>0$.
$$
f_X(x) = \int_{x}^\infty 4e^{-2y}dy = 2e^{-2x}
$$
$X$ has an Exponential distribution with rate parameter 2.
1. Once we have the marginal pdf of $X$ we can integrate as usual. But we can also use properties of Exponential distributions: $\IP(X>0.5) = e^{-2(0.5)}=0.368$
For about 36.8% of earthquakes the next earthquake occurs after 30 minutes.
1. See Figure \@ref(fig:joint-quakes-plot-gamma).
Marginally $Y$ can take any positive value.
Collapse out the $X$ values in the joint pdf.
There are two features at work.
The horizontal strips corresponding to $y$ near 0 are the shortest, but they have the highest density.
As $y$ increases, the corresponding horizontal strips get longer, but lower.
It's hard to tell exactly what the shape will be when the  horizontal strips are collapsed, but one guess is that  the marginal density will start low at $y=0$, then increase over some range of $y$ values, but then decrease again.
1. Collapse the vertical strip corresponding to $y=1.5$ by integrating out the $x$ values; if $y = 1.5$ then density is positive for $x<1.5$.
$$
f_Y(1.5) = \int_{0}^{1.5} 4e^{-2(1.5)}dx = 4(1.5)e^{-2(1.5)} 
$$
1. Repeat the previous part with 1.5 replaced by a generic $y>0$.
$$
f_Y(y) = \int_{0}^y 4e^{-2y}dx = 4ye^{-2y}
$$
That is, the marginal pdf of $Y$ is $f_Y(y) = 4ye^{-2y}, y>0$. (This distribution is the "Gamma" distribution with shape parameter 2 and rate parameter 2.).
1. Once we have the marginal pdf of $Y$ we can integrate it as usual
$$
\IP(Y < 1) = \int_0^1 4y e^{-2y}dy = 0.594
$$
For about 59.4% of earthquakes, at least two earthquakes happen within 2 hours.
1. No, the joint probability is not equal to the product of the marginal probabilities because $X$ and $Y$ are not independent since we must have $X\le Y$.

```


(ref:cap-joint-quakes-plot-exp) Marginal distribution of $X$ in Example \@ref(exm:joint-quakes-marginal).

```{r joint-quakes-plot-exp, echo = FALSE, fig.cap="(ref:cap-joint-quakes-plot-exp)"}
ggplot(data = data.frame(x = c(0, 5)), aes(x)) +
  stat_function(fun = dexp, n = 101, args = list(rate = 2),
                col = "orange", size = 2) +
  labs(y = "Density",
       x = "x") +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic()
```



(ref:cap-joint-quakes-plot-gamma) Marginal distribution of $Y$ in Example \@ref(exm:joint-quakes-marginal).

```{r joint-quakes-plot-gamma, echo = FALSE, fig.cap="(ref:cap-joint-quakes-plot-gamma)"}
ggplot(data = data.frame(x = c(0, 5)), aes(x)) +
  stat_function(fun = dgamma, n = 101, args = list(rate = 2, shape = 2),
                col = "skyblue", size = 2) +
  labs(y = "Density",
       x = "y") +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic()
```


Be sure to distinguish between joint and marginal distributions.

-   The joint distribution is a distribution on $(X, Y)$ pairs. A mathematical expression of a joint distribution is a function of both values of $X$ and values of $Y$. Pay special attention to the possible values; the possible values of one variable might be restricted by the value of the other.
-   The marginal distribution of $Y$ is a distribution on $Y$ values only, regardless of the value of $X$. A mathematical expression of a marginal distribution will have only values of the single variable in it; for example, an expression for the marginal distribution of $Y$ will only have $y$ in it (no $x$, not even in the possible values).





<!-- ### Transformations of multiple random variables -->

<!-- Earlier in this chapter we studied the joint distribution of the sum and max of two fair-four sided dice rolls.  Now we consider a continuous analog.  Instead of rolling a die which is equally likely to take the values 1, 2, 3, 4, we spin a Uniform(1, 4) spinner that lands uniformly in the continuous interval $[1, 4]$.   Let $\IP$ be the probability space corresponding to two spins of the Uniform(1, 4) spinner, and let $X$ be the sum of the two spins, and $Y$ the larger spin (or the common value if a tie).  We saw that in Section @ref(symbulate-discrete-uniform), we could model two rolls of a fair-four sided die using `DiscreteUniform(1, 4) ** 2`.  Similarly, we can model two spins of the Uniform(1, 4) spinner with `Uniform(1, 4) ** 2`. -->

<!-- We start by looking at the joint distribution of the two spins,  $(U_1, U_2)$, which take values in $[1, 4]\times[1, 4]$. -->

<!-- ```{python} -->

<!-- P = Uniform(1, 4) ** 2 -->

<!-- U1, U2 = RV(P) -->

<!-- u1u2 = (U1 & U2).sim(100) -->

<!-- ``` -->

<!-- ```{python} -->

<!-- plt.figure() -->

<!-- u1u2.plot() -->

<!-- plt.show() -->

<!-- ``` -->

<!-- We see that the $(U_1, U_2)$ pairs are roughly "evenly spread" throughout $[1, 4]\times [1, 4]$.  The scatterplot displays each individual pair.  We can summarize the distribution  of many pairs with a two-dimensional histogram.  To construct the histogram, the space of values $[1, 4]\times[1, 4]$ is chopped into rectangular bins and the relative frequency of pairs which fall within each bin is computed. In a histogram of a single variable, area represents relative frequency; in a histogram of two variables, volume represents relative frequency, with the height of each rectangular bin on a "density" scale represented by its color intensity. -->

<!-- ```{python, warning = FALSE, error = TRUE, message = FALSE} -->

<!-- plt.figure() -->

<!-- (U1 & U2).sim(10000).plot('hist') -->

<!-- plt.show() -->

<!-- ``` -->

<!-- Now we let $X$ be the sum and $Y$ the max of the two spins^[Remember that a probability space outcome corresponds to the pair of spins, so we can define random variables on this space as we have done.  We could also first define random variables `U1, U2 = RV(P)` corresponding to the individual spins, and then define the sum as `X = U1 + U2`.  For technical reasons the syntax for `max` is a little different: `Y = (U1 & U2).apply(max)`.].  First consider the possible values of $(X, Y)$. Marginally, $X$ takes values in $[2, 8]$ and $Y$ takes values in $[1, 4]$.  However, not every value in $[2, 8]\times [1, 4]$ is possible.  Before proceeding,  sketch a picture representing the possible values of $(X, Y)$ pairs. -->

<!-- - We must have  $Y \ge 0.5 X$, or equivalently, $X \le 2Y$. For example, if $X=4$ then $Y$ must be at least 2, because if the larger of the two spins were less than 2, then both spins must be less than 2, and the sum must be less than 4. -->

<!-- - We must have $Y \le X - 1$, or equivalently, $X \ge Y + 1$. For example, if $Y=3$, then one of the spins is 3 and the other one is at least 1, so the sum must be at least 4. -->

<!-- Therefore, the possible values of $(X, Y)$ lie in the set -->

<!-- \[ -->

<!-- \{(x, y): 2\le x\le 8, 1 \le y\le 4, 0.5x \le y \le x-1\} -->

<!-- \] -->

<!-- which can also be written as $\{(x, y): 2\le x \le 8, 0.5 x\le y \le \min(4, x-1)\}$.  This set is represented by the triangular region in the plots below. -->

<!-- ```{python, warning = FALSE, error = TRUE, message = FALSE} -->

<!-- P = Uniform(1, 4) ** 2 -->

<!-- U = RV(P) -->

<!-- X = RV(P, sum) -->

<!-- Y = RV(P, max) -->

<!-- (U & X & Y).sim(100) -->

<!-- ``` -->

<!-- ```{python, warning = FALSE, error = TRUE, message = FALSE} -->

<!-- plt.figure() -->

<!-- (X & Y).sim(100).plot() -->

<!-- plt.show() -->

<!-- ``` -->

<!-- ```{python, warning = FALSE, error = TRUE, message = FALSE} -->

<!-- plt.figure() -->

<!-- (X & Y).sim(10000).plot('hist') -->

<!-- plt.show() -->

<!-- ``` -->

<!-- Compare the joint histogram above to the tile plot in Section @ref(sym-joint).  In the dice rolling situation there are basically two cases.  Each $(X, Y)$ pair that correspond to a tie --- that is each $(X, Y)$ pair with $X = 2Y$ --- has probability 1/16.  Each of the other possible $(X, Y)$ pairs has probability 2/16.  -->

<!-- Back to the continuous analog, the histogram shows that $(X, Y)$ pairs are roughly uniformly distributed within the triangular region of possible values.  Consider a single $(X, Y)$ pair, say (0.8, 0.5).  There are two outcomes --- that is, pairs of spins --- for which $X=0.8, Y=0.5$, namely (0.5, 0.3) and (0.3, 0.5).  Like (0.8, 0.5), most of the possible $(X, Y)$ values correspond to exactly two outcomes.  The only ones that do not are the values with $Y = 0.5X$ that lie along the south/east border of the triangular region. The pairs $(X, 0.5X)$ only correspond to exactly one outcome.  For example, the only outcome corresponding to (6, 3) is the $(U_1, U_2)$ pair (3, 3); that is, the only way to have $X=6$ and $Y=3$ is to spin 3 on both spins.  In general, the event $\{Y = 0.5X\}$ is the same as the event that both spins are exactly the same, $\{U_1=U_2\}$. However, as discussed in Section @ref(non-uniform-prob-measure), the probability that $U_1=U_2$ exactly is 0.  Therefore, we don't really need to worry about the ties as we did in the discrete case.  Excluding ties, roughly, each pair in the triangular region of possible $(X, Y)$ pairs corresponds to exactly two outcomes (pairs of spins), and since the outcomes are uniformly distributed (over $[1, 4]\times[1, 4]$) then the $(X, Y)$ pairs are also uniformly distributed (over the triangular region of possible values). -->

<!-- The plot below represents the joint distribution of $(X, Y)$.  This is really a three-dimensional plot.  The base is the triangular region which represents the possible $(X, Y)$ pairs.  There is a surface floating above this region which represents the density at each point.  For a single variable, the density is a smooth curve approximating the idealized shape of the histogram.  Likewise, for two variables, the density is a smooth surface approximating the idealized shape of the joint histogram. The height of this surface is depicted in the two-dimensional plot via the color intensity.  Since the $(X, Y)$ pairs are uniformly distributed over their range of possible values, the height of the surface and hence the color intensity is constant over the range of possible values, and the height is 0 (white) for impossible $(X, Y)$ pairs.  Careful: this plot is not the same as the ones in Section @ref(non-uniform-prob-measure).  Those plots were just depicting events, and the color was just used to shade the region of interest. The plot below is depicting a joint distribution, and the color represents the height of the density surface at each $(X, Y)$ pair; white areas correspond to a height of 0. -->

<!-- (ref:cap-dice-continuous-sum-max-joint) Joint distribution of $X$ (sum) and $Y$ (max) of two spins of the Uniform(1, 4) spinner.  The triangular region represents the possible values of $(X, Y)$ the height of the density surface is constant over this region and 0 outside of the region. -->

<!-- ```{r dice-continuous-sum-max-joint, echo = FALSE, fig.cap="(ref:cap-dice-continuous-sum-max-joint)"} -->

<!-- dfA <- data.frame(x = c(2, 8, 5, 2), -->

<!--                  y = c(1, 4, 4, 1), -->

<!--                  v = c(1, 1, 1, 1)) -->

<!-- pA <- ggplot(data = dfA, aes(x = x, y = y)) + -->

<!--   geom_polygon(fill="cornflowerblue", show.legend = FALSE) + -->

<!--   scale_x_continuous(limits = c(2, 8), expand = c(0, 0)) +  -->

<!--   scale_y_continuous(limits = c(1, 4), expand = c(0, 0)) + -->

<!--   theme_classic() + -->

<!--   theme(panel.border = element_rect(linetype = "solid", fill = NA)) + -->

<!--   theme(plot.margin=unit(c(1,1,1,1),"cm")) + -->

<!--   xlab(expression("X")) + -->

<!--   ylab(expression("Y")) + -->

<!--   theme(plot.title = element_text(hjust = 0.5)) -->

<!-- plot(pA) -->

<!-- ``` -->

<!-- We now consider the marginal distributions of $X$ and $Y$.  Before proceeding, try to sketch the marginal distributions. -->

<!-- Here is a plot showing the joint histogram representing the joint distribution of $(X, Y)$, along with histograms representing each of the marginal distributions. -->

<!-- ```{python, warning = FALSE, error = TRUE, message = FALSE} -->

<!-- plt.figure() -->

<!-- (X & Y).sim(10000).plot(['hist', 'marginal']) -->

<!-- plt.show() -->

<!-- ``` -->

<!-- Let's look a little more closely at the marginal distribution of $X$. -->

<!-- (ref:cap-dice-continuous-sum-marginal) Histogram representing the marginal distribution of the sum ($X$) of two spins of the Uniform(1, 4) spinner. -->

<!-- ```{python dice-continuous-sum-marginal, echo = FALSE, warning = FALSE, error = TRUE, message = FALSE, fig.cap="(ref:cap-dice-continuous-sum-marginal)"} -->

<!-- plt.figure() -->

<!-- X.sim(10000).plot() -->

<!-- plt.show() -->

<!-- ``` -->

<!-- The marginal distribution of $X$ has highest density near 5 and lowest density near 2 and 8.  Intuitively, there is only one pair of spins --- (1, 1) --- for which the sum is 2; similarly for a sum of 8.  But there are many pairs for which the sum is 5: (2.5, 2.5), (3, 2), (2, 3), (1.2, 2.8), etc.  Recall that for the dice rolls, we could obtain the marginal distribution of $X$ by summing the joint distribution over all $Y$ values.  Similarly, we can find the marginal density of $X$ by aggregating over all possible values of $Y$.  For each possible value of $X$, "collapse" the joint histogram vertically over all possible values of $Y$.  Imagine that within the region of possible $(X, Y)$ pairs, the joint histogram is composed of stacks of blocks, one for each bin, each stack of the same height (because the values are uniformly distributed over the triangular region).  To get the marginal density for a particular $x$, take all the stacks corresponding to that $x$, for different values of $y$, and stack them on top of one another.  There will be the most stacks for $x$ values near 5  and the fewest stacks for $x$ values near 2 or 8.  In other words, the aggregated density along "vertical strips" is largest for the vertical strip for $x=5$. -->

<!-- (In this case, the joint distribution is uniform over the range of possible pairs, so the stacks all have the same height.  That won't be true in general, so the marginal distributions will depend both on the number of and the height of the "stacks".) -->

<!-- Similarly reasoning applies to find the marginal distribution of $Y$.  Now we find the marginal density for a particular $y$ value by collapsing/stacking the histogram horizontally over all possible value of $X$. We see that the density increases with values of $y$.  Intuitively, there is only one pair of spins, (1, 1), for which $Y=1$, but many pairs of spins for which $Y=4$, e.g., (1, 4), (4, 1), (4, 2), (2.5, 4), etc. -->

<!-- (ref:cap-dice-continuous-max-marginal) Histogram representing the marginal distribution of the larger ($Y$) of two spins of the Uniform(1, 4) spinner. -->

<!-- ```{python dice-continuous-max-marginal, echo = FALSE, fig.cap="(ref:cap-dice-continuous-max-marginal)"} -->

<!-- plt.figure() -->

<!-- Y.sim(10000).plot() -->

<!-- plt.show() -->

<!-- ``` -->

<!-- What about the long run averages?  The sums of the two spins is $X= U_1 + U_2$. The long run average of each of $U_1$ and $U_2$ is 2.5 (the midpoint of the interval [1, 4]).  We can see from its marginal distribution that the long run average of $X$ is 5.  Therefore, the average of the sum is the sum of averages. -->

<!-- ```{python} -->

<!-- X.sim(10000).mean() -->

<!-- ``` -->

<!-- However, the average of $Y=\max(U_1, U_2)$ is 3, which is not $\max(2.5, 2.5)$.  Therefore, the average of the maximum is not the maximum of the averages.  Remember that in general, Average of $g(X, Y)$ $\neq$ $g$(Average of $X$, Average of $Y$). -->

<!-- ```{python} -->

<!-- Y.sim(10000).mean() -->

<!-- ``` -->

<!-- Finally, observe that the plots in this section look like continuous versions of the plots for the dice rolling example earlier in the chapter. However, it took a little more work in this section to think about what the joint or marginal distributions might look like.  When studying continuous random variables, it is often helpful to think about how a discrete analog behaves. -->

<!-- ```{python, eval = FALSE, include = FALSE,  echo=FALSE, out.width='50%', fig.show='hold'} -->

<!-- P = DiscreteUniform(1, 4) ** 2 -->

<!-- X = RV(P, sum) -->

<!-- Y = RV(P, max) -->

<!-- plt.figure() -->

<!-- (X & Y).sim(10000).plot(['tile', 'marginal']) -->

<!-- plt.show() -->

<!-- P = Uniform(1, 4) ** 2 -->

<!-- X = RV(P, sum) -->

<!-- Y = RV(P, max) -->

<!-- plt.figure() -->

<!-- (X & Y).sim(10000).plot(['hist', 'marginal']) -->

<!-- plt.show() -->

<!-- ``` -->




## Conditional distributions


Most interesting problems involve two or more^[We mostly focus on the case of two random variables, but analogous definitions and concepts apply for more than two (though the notation can get a bit messier).] random variables defined on the same probability space. In these situations, we can consider how the variables vary together, or jointly, and study their relationships. The *joint distribution* of random variables $X$ and $Y$ (defined on the same probability space) is a probability distribution on $(x, y)$ *pairs*, and describes how the values of $X$ and $Y$ vary together or jointly.

We can also study the *conditional distribution* of one random variable given the value of another.  How does the distribution of $Y$ change for different values of $X$ (and vice versa)?



The **conditional distribution of $Y$ given $X=x$** is the distribution of $Y$  values over only those outcomes for which $X=x$.   It is a distribution on values of $Y$ only; treat $x$ as a fixed constant when conditioning on the event $\{X=x\}$.
Conditional distributions can be obtained from a joint distribution by *slicing and renormalizing*.

### Discrete random variables: Conditional probability mass functions


```{example, dice-coin-pmf}

Roll a fair four-sided die once and let $X$ be the number rolled.  Then flip a fair coin $X$ times and let $Y$ be the number of heads.

```

1. Identify the possible values of $X$.
1. Identify the possible values of $Y$.
1. Find the conditional distribution of $Y$ given $X=4$.
1. Find the conditional distribution of $Y$ given $X=3$.
1. Find the probability that $X=3$ and $Y=2$.
1. Find the probability that $X=3$ and $Y=y$ for $y = 0, 1, 2, 3, 4$.
1. Find the joint distribution of $X$ and $Y$.
1. Find the marginal distribution of $Y$.
1. Find the conditional distribution of $X$ given $Y=2$.




```{solution, dice-coin-pmf-sol}
to Example \@ref(exm:dice-coin-pmf)
```

```{asis, fold.chunk = TRUE}

1. $X$ takes values 1, 2, 3, 4.
1. $Y$ takes values 0, 1, 2, 3, 4. You might say: but the value that $Y$ can take depends on what $X$ is.  True, but here we are identifying the overall *possible* values of $Y$.  It is possible that $Y$ can be 4.  Now if $X=3$ then $Y=4$ is no longer possible, but without knowing the value of $X$ then it is possible for $Y$ to be 0, 1, 2, 3, 4. (If you still object, then you should have objected to the previous question too. For example, if $Y=4$ then $X=3$ is no longer possible. But we suspect you didn't have any difficulty saying that the possible values of $X$ were 1, 2, 3, 4.  Remember, it doesn't matter what is "first" or "second"; it's what information you are conditioning on. Without knowing the value of $X$ we have to consider all the possible values of $Y$, and vice versa.)
1. If $X=4$ we flip the coin four times. The 16 possible equally likely outcomes are in the first column of Table \@ref(tab:mscoin). (Note: $X$ and $Y$ are defined differently here than in the table.) Given $X=4$, $Y$ takes values 0, 1, 2, 3, 4, with respective probability 1/16, 4/16, 6/16, 4/16, 1/16.
1. If $X=3$ we flip the coin three times. We can use Table \@ref(tab:coin-transform-tab2).  Given $X=3$, $Y$ takes values 0, 1, 2, 3, with respective probability 1/8, 3/8, 3/8, 1/8.
1. We have a conditional probability and a marginal probability so we can use the multiplication rule.
\[
p_{X, Y}(3, 2) = \IP(X=3, Y = 2) = \IP(Y = 2|X = 3)\IP(X=3) = p_{Y|X}(2|3)p_X(3) = (3/8)(1/4) = 3/32 = 6/64.
\]
1. Use the multiplication rule as in the previous part.
\[
p_{X, Y}(3, y) = \IP(X=3, Y = y) = \IP(X = 3|Y = y)\IP(X=3) = p_{Y|X}(y|3)p_X(3).
\]
The conditional pmf of $Y$ given $X=3$, $p_{Y|X}(y|3)$, was identified in part 4.
Note that $\IP(Y = 4|X = 3) = 0$ so $p_{X, Y}(3, 4) = \IP(X = 3, Y = 4) = 0.$
See the row corresponding to $X=3$ in the table below.
1. Find the joint pmf as the product of the marginal distribution of $X$ and the family of conditional distributions of $Y$ given values of $X$. Proceed as in the previous part to fill in each row in the table below.

    | $p_{X, Y}(x, y)$ |       |       |       |      |      |          |
    |------------------|------:|------:|------:|-----:|-----:|---------:|
    | $x$ \\ $y$       |     0 |     1 |     2 |    3 |    4 | $p_X(x)$ |
    | 1                |  8/64 |  8/64 |     0 |    0 |    0 |      1/4 |
    | 2                |  4/64 |  8/64 |  4/64 |    0 |    0 |      1/4 |
    | 3                |  2/64 |  6/64 |  6/64 | 2/64 |    0 |      1/4 |
    | 4                |  1/64 |  4/64 |  6/64 | 4/64 | 1/64 |      1/4 |
    | $p_Y(y)$         | 15/64 | 26/64 | 16/64 | 6/64 | 1/64 |          |

    Note that the possible pairs of $(X, Y)$ satisfy: $x = 1, 2, 3, 4$, $y = 0, 1, \ldots, x$. That is, not every possible value of $Y$ can be paired with every possible value of $X$.

1. Marginally, $Y$ can take values 0, 1, 2, 3, 4; see part 2.  Find the corresponding probabilies by summing over $x$ values in the joint pmf.

    | $y$ | $p_Y(y)$ |
    |-----|---------:|
    | 0   |    15/64 |
    | 1   |    26/64 |
    | 2   |    16/64 |
    | 3   |     6/64 |
    | 4   |     1/64 |

1. Slice the column of the joint distribution table corresponding to $Y=2$. Given $Y=2$, $X$ can take values 2, 3, 4, and $Y$ is equally likely to be 3 and 4 and each of these values is 1.5 times more likely than 2.

    | $x$ |   $p_{X|Y}(x|2)$ |
    |----:|-----------------:|
    |   2 |              2/8 |
    |   3 |              3/8 |
    |   4 |              3/8 |
  
```

(ref:cap-dice-coin-pmf) Tile plot representation of the joint distribution of $X$ and $Y$ in Example \@ref(exm:dice-coin-pmf).

```{r dice-coin-pmf-plot, echo = FALSE, fig.cap="(ref:cap-dice-coin-pmf)"}


xy = expand_grid(x = 1:4, y = 0:4) %>%
  filter(y <= x) %>%
  mutate(p = dbinom(y, size = x, 0.5)) %>%
  mutate(p = p / sum(p))


xy %>%
  ggplot(aes(x = x,
              y = y,
              fill = p)) +
  geom_tile(colour = "grey50") +
  geom_text(aes(label = p), col = "white") +
  scale_x_continuous(breaks = 1:4) +
  labs(fill = "Probability") +
  theme_classic()

```




(ref:cap-dice-coin-pmf-mosaic1) Mosaic plot representation of conditional distributions of $Y$ given values of $X$ in Example \@ref(exm:dice-coin-pmf).

```{r dice-coin-pmf-mosaic1, echo = FALSE, fig.cap="(ref:cap-dice-coin-pmf-mosaic1)"}

knitr::include_graphics(c("_graphics/dice-coin-y-given-x.png"))
```


(ref:cap-dice-coin-pmf-mosaic2) Mosaic plot representation of conditional distributions of $X$ given values of $Y$ in Example \@ref(exm:dice-coin-pmf).

```{r dice-coin-pmf-mosaic2, echo = FALSE, fig.cap="(ref:cap-dice-coin-pmf-mosaic2)"}

knitr::include_graphics(c("_graphics/dice-coin-x-given-y.png"))
```

```{definition conditional-pmf}

Let $X$ and $Y$ be two *discrete* random variables defined on a probability space with probability measure $\IP$.  For any fixed $x$ with $\IP(X=x)>0$, the **conditional probability mass function (pmf)** of $Y$ given $X=x$ is a function $p_{Y|X}:\reals \mapsto [0, 1]$ defined by $p_{Y|X}(y|x)=\IP(Y=y|X=x)$.
\begin{align*}
p_{Y|X}(y|x) = \IP(Y=y|X=x) & = \frac{\IP(X=x,Y=y)}{\IP(X=x)}  = \frac{p_{X,Y}(x,y)}{p_X(x)}& &  \text{a function of $y$ for fixed $x$}
\end{align*}

```

To emphasize, the notation $p_{Y|X}(\cdot|x)$ represents the distribution of the random variable $Y$ given a fixed value $x$ of the random variable $X$.
In the expression $p_{Y|X}(y|x)$, $y$ is treated as the variable and $x$ is treated like a fixed constant.

Notice that the pmfs satisfy
$$
\text{conditional} = \frac{\text{joint}}{\text{marginal}}
$$

Conditional distributions can be obtained from a joint distribution by *slicing and renormalizing*.

The conditional pmf of $Y$ given $X=x$ can be thought of as:

- the slice of the joint pmf $p_{X, Y}(x, y)$ of $(X, Y)$ corresponding to $X=x$, a function of $y$ alone,
- renormalized --- by dividing by $p_X(x)$ --- so that the probabilitiess, corresponding to different $y$ values, for the slice sum to 1. 
$$
1 = \sum_y p_{Y|X}(y|x) \quad \text{for any fixed } x
$$

For a fixed $x$, the shape of the conditional pmf of $Y$ given $X=x$ is determined by the shape of the $x$-slice of the joint pmf, $p_{X, Y}(x, y)$.  That is,


$$
\text{As a function of values of $Y$}, \quad p_{Y|X}(y|x) \propto p_{X, Y}(x, y)
$$

For each fixed $x$, the conditional pmf $p_{Y|X}(\cdot |x)$ is a different distribution on values of the random variable $Y$.  There is not one "conditional distribution of $Y$ given $X$", but rather a family of conditional distributions of $Y$ given different values of $X$.


Rearranging the definition of a conditional pmf yields the **multiplication rule for pmfs of discrete random variables**
\begin{align*}
p_{X,Y}(x,y) & = p_{Y|X}(y|x)p_X(x)\\
& = p_{X|Y}(x|y)p_Y(y)\\
\text{joint} & = \text{conditional}\times\text{marginal} 
\end{align*}

Marginal distributions can be obtained from the joint distribution by collapsing/stacking using the law of total probability. The **law of total probability for pmfs** is

\begin{align*}
p_{Y}(y) & =  \sum_x p_{X,Y}(x, y)\\
& =\sum_x p_{Y|X}(y|x)p_X(x)
\end{align*}


**Bayes rule for pmfs** is

\begin{align*}
p_{X|Y}(x|y) & = \frac{p_{Y|X}(y|x)p_X(x)}{p_Y(y)}
\\
p_{X|Y}(x|y)  & \propto p_{Y|X}(y|x)p_X(x)
\end{align*}








Conditioning on the value of a random variable involves treating that random variable as a constant.
It is sometimes possible to identify one-way conditional distributions ($Y$ given $X$, or $X$ given $Y$) simply by inspecting the joint pmf, without doing any calculations.


```{example, conditional-uniform-joint-pmf}

$X$ and $Y$ are discrete random variables with joint pmf

\[
p_{X, Y} (x, y) = \frac{1}{4x}, \qquad x = 1, 2, 3, 4; y = 1, \ldots, x  
\]

```

1. Donny Dont says: "Wait, the joint pmf is supposed to be a function of both $x$ and $y$ but $\frac{1}{4x}$ is only a function of $x$." Explain to Donny how $p_{X, Y}$ here is, in fact, a function of both $x$ and $y$.
1. In which direction will it be easier to find the conditional distributions by inspection - $Y$ given $X$ or $X$ given $Y$?
1. Without doing any calculations, find the conditional distribution of $Y$ given $X = 3$.
1. Without summing over the joint pmf, find the marginal probability that $X = 3$.
1. Without doing any calculations, find a general expression for the conditional distribution of $Y$ given $X = x$.
1. Without summing over the joint pmf, find the marginal pmf of $X$.
1. Describe a dice rolling scenario in which ($X$, $Y$) pairs would follow this joint distribution. (Hint: you might need multiple kinds of dice.)
1. Construct a two-way table representing the joint pmf, and use it to verify your answers to the previous parts.
1. Find the marginal pmf of $Y$.  Be sure to identify the possible values.
1. Find the conditional pmf of $X$ given $Y=2$. Be sure to identify the possible values.






```{solution, conditional-uniform-joint-pmf-sol}
to Example \@ref(exm:conditional-uniform-joint-pmf)
```

```{asis, fold.chunk = TRUE}

1. Don't forget the possible values. For example, $p_{X, Y}(3, 3) = 1/12$ but $p_{X, Y}(3, 4) = 0$. So $p_{X, Y}$ is a function of both $x$ and $y$.
1. Given $x$, the possible values of $Y$ are $1, \ldots, x$. $x$ also shows up in $\frac{1}{4x}$ and $y$ doesn't.  So it seems like it would be helpful to know the value of $x$. If we treat $x$ as a constant then we find the conditional distribution of $Y$ given $X=x$.
1. Treat $X$ as the constant 3 in the joint pmf and slice to find the shape of the conditional pmf of $Y$ given $X=3$.
\begin{align*}
p_{Y|X} (y | 3) & \propto \frac{1}{12}, \qquad y = 1, \ldots, 3 \\
& \propto \text{constant}, \qquad y = 1, \ldots, 3 
\end{align*}
This is a distribution on $Y$ values alone. The conditional pmf of $Y$ given $X=3$ is constant over its possible values, so given $X=3$, $Y$ is equally to be 1, 2, 3. Renormalize to get
\[
p_{Y|X} (y | 3) = \frac{1}{3}, \qquad y = 1, 2, 3  
\]
1. "Joint = conditional $\times$ marginal":
\begin{align*}
p_{X, Y}(3, y) & = p_{Y|X}(y|3)p_X(3)\\
\frac{1}{4(3)} & = \left(\frac{1}{3}\right)p_X(3)  
\end{align*}
So $p_X(3) = 1/4$. That is, $\IP(X = 3)=1/4$.
1. We use the same process as above but with the value 3 replaced by a generic possible value $x$. But you are still treating $x$ as constant in the joint pmf. Slice to find the shape of the conditional pmf of $Y$ given $X=x$.
\begin{align*}
p_{Y|X} (y | x) & \propto \frac{1}{4x}, \qquad y = 1, \ldots, x  \\
& \propto \text{constant}, \qquad y = 1, \ldots, x 
\end{align*}
This is a distribution on $Y$ values alone. Given $x$, the possible values of $Y$ are $1, \ldots, x$. $x$ is treated as constant so $\frac{1}{4x}$ is also constant, and the conditional pmf of $Y$ given $X=x$ is constant over its possible values. That is, given $X=x$, $Y$ is equally to be $1, \ldots, x$. Renormalize --- each of the $x$ equally likely values of $Y$ occurs with probability $1/x$ --- to get
\[
p_{Y|X} (y | x) = \frac{1}{x}, \qquad y = 1, \ldots, x  
\]
1. "Joint = conditional $\times$ marginal". For $x = 1, 2, 3, 4$:
\begin{align*}
p_{X, Y}(x, y) & = p_{Y|X}(y|x)p_X(x)\\
\frac{1}{4x} & = \left(\frac{1}{x}\right)p_X(x)  
\end{align*}
So $p_X(x) = 1/4, x = 1, 2, 3, 4$. That is, $X$ is equally like to be 1, 2, 3, 4.
1. Roll a fair four-sided die once and let $X$ be the result. Then, given $X=x$, roll a fair "$x$-sided die"^[A "one-sided" die always returns 1. A "two-sided" die could be a coin with heads labeled 1 and tails labeled 2. A "three-sided" die could be a six-sided die with two sides each labeled 1, 2, 3.] once and let $Y$ be the result.
1. See the table below. Note that we can also write the possible pairs as: $y = 1, 2, 3, 4; x = y, \ldots, 4$.

    | $p_{X, Y}(x, y)$ |       |       |      |      |            |
    |------------------|------:|------:|-----:|-----:|-----------:|
    | $x$ \\ $y$       |     1 |     2 |    3 |    4 | $p_{X}(x)$ |
    | 1                | 12/48 |     0 |    0 |    0 |        1/4 |
    | 2                |  6/48 |  6/48 |    0 |    0 |        1/4 |
    | 3                |  4/48 |  4/48 | 4/48 |    0 |        1/4 |
    | 4                |  3/48 |  3/48 | 3/48 | 3/48 |        1/4 |
    | $p_Y(y)$         | 25/48 | 13/48 | 7/38 | 3/48 |            |
  
1. Marginally, the possible values of $Y$ are 1, 2, 3, 4. The marginal pmf of $Y$ is a function of $y$ alone, but there is no simple closed form expression in this case.

    | $y$ | $p_Y(y)$ |
    |-----|---------:|
    | 1   |    25/48 |
    | 2   |    13/48 |
    | 3   |     7/48 |
    | 4   |     3/48 |
  
1. Given $Y=2$, $X$ takes possible values 2, 3, 4. Slice the column of the joint pmf table corresponding to $Y=2$ and renormalize. Given $Y=2$, $X$ is two times more likely to be 2 than to be 4, and $X$ is 1.5 times more likely to be 2 than to be 3. The conditional pmf of $X$ given $Y=2$ is a distribution of values of $X$ alone, treating $Y=2$ as constant.

    | $x$ |   $p_{X|Y}(x|2)$ |
    |----:|-----------------:|
    |   2 |             6/13 |
    |   3 |             4/13 |
    |   4 |             3/13 |
  
```

The code below defines a custom probability space corresponding to the two-stage conditional simulation in the previous example, and then defines random variables on this probability space.


```{python}
def conditional_dice():
  x = DiscreteUniform(1, 4).draw()
  if x == 1:
    y = 1
  else:
    y = DiscreteUniform(1, x).draw()
  return x, y

X, Y = RV(ProbabilitySpace(conditional_dice))
```


```{python, eval = FALSE}
(X & Y).sim(10000).plot('tile')
```

```{python, echo = FALSE}
plt.figure()
(X & Y).sim(10000).plot('tile')
plt.show()
```


```{python, eval = FALSE}
Y.sim(10000).plot()
```


```{python, echo = FALSE}
plt.figure()
Y.sim(10000).plot()
plt.show()
```

```{python, eval = FALSE}
(X | (Y == 2) ).sim(10000).plot()
```

```{python, echo = FALSE}
plt.figure()
(X | (Y == 2) ).sim(10000).plot()
plt.show()
```


Be sure to distinguish between joint, conditional, and marginal distributions.


- The joint distribution is a distribution on $(X, Y)$ pairs. A mathematical expression of a joint distribution is a function of both values of $X$ and values of $Y$.  In particular, the joint pmf $p_{X, Y}$ is a function of both values of $X$ and values of $Y$. Pay special attention to the possible values; the possible values of one variable might be restricted by the value of the other.
- The conditional distribution of $Y$ given $X=x$ is a distribution on $Y$ values (among $(X, Y)$ pairs with a fixed value of $X=x$). A mathematical expression of a conditional distribution will involve both $x$ and $y$, but $x$ is treated like a fixed constant and $y$ is treated as the variable.  In particular, the conditional pmf $p_{Y|X}$ is a function of values of $Y$ for a fixed value of $x$; treat $x$ like a constant and $y$ as the variable. Note: the possible values of $Y$ might depend on the value of $x$, but $x$ is treated like a constant.
- The marginal distribution of $Y$ is a distribution on $Y$ values only, regardless of the value of $X$. A mathematical expression of a marginal distribution will have only values of the single variable in it; for example, an expression for the marginal distribution of $Y$ will only have $y$ in it (no $x$, not even in the possible values). In particular, the marginal pmf $p_Y$ is a function of values of $Y$ only.

### Continuous random variables: Conditional probability density functions

Remember that continuous random variables are described by probability density functions which can be integrated to find probabilities.


Bivariate Normal distributions are one commonly used family of joint continuous distribution.
For example, the following plots represent the joint pdf surface of the Bivariate Normal distribution in the meeting problem in Section \@ref(sec-sim-bvn), with correlation 0.7.
In $X$ and $Y$ have a Bivariate Normal distribution, then the conditional distribution of $X$ given any value of $Y$ is a Normal distribution (likewise for $Y$ given any value of $X$.)



```{r meeting-bvn-conditional-again, ref.label = 'meeting-bvn-conditional', echo = FALSE, warning = FALSE, message = FALSE, fig.cap="A Bivariate Normal distribution with two $X$-slices highlighted."}

```

(ref:cap-meeting-bvn-conditional-slice-again) Conditional distributions of $Y$ given $X=x$ for the two $x$-slices in Figure \@ref(fig:meeting-bvn-conditional-again).

```{r meeting-bvn-conditional-slice-again, ref.label = 'meeting-bvn-conditional-slice', echo = FALSE, warning = FALSE, message = FALSE, fig.cap="(ref:cap-meeting-bvn-conditional-slice-again)"}

```

```{definition conditional-pdf}

Let $X$ and $Y$ be two *continuous* random variables with joint pdf $f_{X,Y}$ and marginal pdfs $f_X, f_Y$.  For any fixed $x$ with $f_X(x)>0$, the **conditional probability density function (pdf)** of $Y$ given $X=x$ is a function $f_{Y|X}:\reals \mapsto [0, \infty)$ defined by
\begin{align*}
f_{Y|X}(y|x) &= \frac{f_{X,Y}(x,y)}{f_X(x)}& &  \text{a function of $y$ for fixed $x$}
\end{align*}

```

To emphasize, the notation $f_{Y|X}(y|x)$ represents a conditional distribution of the random variable $Y$ for a fixed value $x$ of the random variable $X$.  In the expression $f_{Y|X}(y|x)$, $x$ is treated like a constant and $y$ is treated as the variable.

Notice that the pdfs satisfy
\[
\text{conditional} = \frac{\text{joint}}{\text{marginal}}
\]

Conditional distributions can be obtained from a joint distribution by *slicing and renormalizing*.

The conditional pdf of $Y$ given $X=x$ can be thought of as:

- the slice of the joint pdf $f_{X, Y}(x, y)$ of $(X, Y)$ corresponding to $X=x$, a function of $y$ alone,
- renormalized --- by dividing by $f_X(x)$ --- so that the density heights, corresponding to different $y$ values, for the slice are such that the total area under the density slice is 1. 
$$
1 = \int_{-\infty}^\infty f_{Y|X}(y|x)\, dy \quad \text{for any fixed } x
$$

For a fixed $x$, the shape of the conditional pdf of $Y$ given $X=x$ is determined by the shape of the $x$-slice of the joint pdf, $f_{X, Y}(x, y)$.  That is,

$$
\text{As a function of values of $Y$}, \quad f_{Y|X}(y|x) \propto f_{X, Y}(x, y)
$$

For each fixed $x$, the conditional pdf $f_{Y|X}(\cdot |x)$ is a different distribution on values of the random variable $Y$.  There is not one "conditional distribution of $Y$ given $X$", but rather a family of conditional distributions of $Y$ given different values of $X$.

Rearranging the definition of a conditional pdf yields the **multiplication rule for pdfs of continuous random variables**
\begin{align*}
f_{X,Y}(x,y) & = f_{Y|X}(y|x)f_X(x)\\
& = f_{X|Y}(x|y)f_Y(y)\\
\text{joint} & = \text{conditional}\times\text{marginal} 
\end{align*}

Marginal distributions can be obtained from the joint distribution by collapsing/stacking using the law of total probability. The **law of total probability for pdfs** is

\begin{align*}
f_{Y}(y) & =  \int_{-\infty}^\infty f_{X,Y}(x, y)\, dx\\
& =\int_{-\infty}^\infty f_{Y|X}(y|x)f_X(x)\, dx
\end{align*}


**Bayes rule for pdfs** is

\begin{align*}
f_{X|Y}(x|y) & = \frac{f_{Y|X}(y|x)f_X(x)}{f_Y(y)}
\\
f_{X|Y}(x|y)  & \propto f_{Y|X}(y|x)f_X(x)
\end{align*}


```{example, uniform-sum-max-conditional}

Recall Example \@ref(exm:uniform-sum-max-pdf). Consider the probability space corresponding to two spins of the Uniform(1, 4) spinner and let $X$ be the sum of the two spins and $Y$ the larger to the two spins (or the common value if a tie). Recall that the joint pdf is
    \[
    f_{X, Y}(x, y) =
      \begin{cases}
    2/9, & 2<x<8,\; 1<y<4,\; x/2<y<x-1,\\
    0, & \text{otherwise,}
    \end{cases}
    \]
the marginal pdf of $Y$ is
    \[
      f_Y(y) =
      \begin{cases}
      (2/9)(y-1), &   1<y<4,\\
      0, & \text{otherwise,}
      \end{cases}
    \]
and the marginal pdf of $X$ is
    \[
     f_X(x) = 
       \begin{cases}
       (1/9)(x-2), & 2 < x< 5,\\
       (1/9)(8-x), & 5<x<8,\\
       0, & \text{otherwise.}
       \end{cases}
    \]

```




1. Find $f_{X|Y}(\cdot|3)$, the conditional pdf of $X$ given $Y=3$.
1. Find $\IP(X > 5.5 | Y = 3)$.
1. Find $f_{X|Y}(\cdot|4)$, the conditional pdf of $X$ given $Y=4$.
1. Find $\IP(X > 5.5 | Y = 4)$.
1. Find $f_{X|Y}(\cdot|y)$, the conditional pdf of $X$ given $Y=y$, for $1<y<4$.
1. Find $f_{Y|X}(\cdot|3.5)$, the conditional pdf of $Y$ given $x=3.5$.
1. Find $f_{Y|X}(\cdot|6)$, the conditional pdf of $Y$ given $x=6$.
1. Find $f_{Y|X}(\cdot|x)$, the conditional pdf of $Y$ given $x$.



```{solution, uniform-sum-max-conditional-sol}
to Example \@ref(exm:uniform-sum-max-conditional)
```

```{asis, fold.chunk = TRUE}

1. See the top left plot of Figure \@ref(fig:uniform-sum-max-conditional-plot); the conditional pdf corresponds to the renormalized slice of the joint pdf along $Y=3$. If $Y=3$ then $X$ takes values between 4 and 6, and the density is constant over this interval of length 2.
\[
 f_{X|Y}(x| 3) = 
   \begin{cases}
 \frac{1}{2}, & 4 < x < 6,\\
 0, & \text{otherwise.}
 \end{cases}
\]
    We could also use "conditional is joint divided by marginal".  The marginal density of $Y$ at $y=3$ is $f_Y(3) = (2/9)(3-1) = 4/9$. For the slice of the joint pdf along $y=3$, be careful to note that $f_{X, Y}(x, 3)=2/9$ only if $4<x<6$ and $f_{X, Y}(x, 3) = 0$ for $x$ outside of this range.
      \[
       f_{X|Y}(x| 3) = \frac{f_{X, Y}(x, 3)}{f_Y(3)} =  
         \begin{cases}
       \frac{2/9}{4/9}, & 4 < x< 6\\
       0, & \text{otherwise.}
       \end{cases}
      \]
1. Use the conditional pdf of $X$ given $Y=3$ to compute $\IP(X > 5.5 | Y = 3)$.  Since the conditional pdf is Uniform(4, 6), the probability should just be $(6-5.5)/(6-4) = 0.25$.  To compute Via integration, note we integrate over values of $X$
\[
\IP(X > 5.5 | Y = 3) = \int_{5.5}^6 \frac{1}{2}dx = \frac{x}{2}\Bigg|_{x=5.5}^{x=6} = 0.25 
\]
1. See the top right plot of Figure \@ref(fig:uniform-sum-max-conditional-plot); the conditional pdf corresponds to the renormalized slice of the joint pdf along $Y=4$. If $Y=4$ then $X$ takes values between 5 and 8, and the density is constant over this interval of length 3.
\[
 f_{X|Y}(x| 3) = 
   \begin{cases}
 \frac{1}{3}, & 5 < x < 8,\\
 0, & \text{otherwise.}
 \end{cases}
\]
1. Use the conditional pdf of $X$ given $Y=4$ to compute $\IP(X > 5.5 | Y = 4)$.  Since the conditional pdf is Uniform(5, 8), the probability should just be $(8-5.5)/(8-5) = 5/6$.  Via integration; note we integrate over values of $X$
\[
\IP(X > 5.5 | Y = 4) = \int_{5.5}^8 \frac{1}{3}dx = \frac{x}{3}\Bigg|_{x=5.5}^{x=8} = 5/6 
\]
1. Treat $y$ as a constant, like in the previous parts. The conditional pdf corresponds to the renormalized slice of the joint pdf along $Y=y$. If $Y=y$ then $X$ takes values between $y+1$ and $2y$, and the density is constant over this interval of length $2y - (y +1) = y - 1$.
\[
 f_{X|Y}(x| y) = 
   \begin{cases}
 \frac{1}{y-1}, & y+1 < x < 2y,\\
 0, & \text{otherwise.}
 \end{cases}
\]
    We could also use "conditional is joint divided by marginal".  The marginal density of $Y$ at $y$ is $f_Y(y) = (2/9)(y-1)$. For the slice of the joint pdf along $y$, be careful to note that $f_{X, Y}(x, y)=2/9$ only if $y+1<x<2y$ and $f_{X, Y}(x, y) = 0$ for $x$ outside of this range.
      \[
       f_{X|Y}(x| y) = \frac{f_{X, Y}(x, y)}{f_Y(y)} =  
         \begin{cases}
       \frac{2/9}{2/9(y-1)}, & y+1 < x< 2y\\
       0, & \text{otherwise.}
       \end{cases}
      \]
    Remember, we are treating $y$ as given and fixed.  The conditional pdf of $X$ given $Y=y$ is a density on values of $X$.
1. See the bottom left plot of Figure \@ref(fig:uniform-sum-max-conditional-plot); the conditional pdf corresponds to the renormalized slice of the joint pdf along $X=3.5$. If $X=3.5$ then $Y$ takes values between 1.75 and 2.5, and the density is constant over this interval of length 0.75.
\[
 f_{Y|X}(y| 4) = 
   \begin{cases}
 \frac{1}{0.75}, & 1.75 < y < 2.5,\\
 0, & \text{otherwise.}
 \end{cases}
\]
    We could also use "conditional is joint divided by marginal".  The marginal density of $X$ at $x=3.5$ is $f_X(3.5) = (1/9)(3.5-2) = 1.5/9$. For the slice of the joint pdf along $x=3.5$, be careful to note that $f_{X, Y}(3.5, y)=2/9$ only if $1.75< y <2.5$ and $f_{X, Y}(3.5, y) = 0$ for $y$ outside of this range.
      \[
       f_{Y|X}(y| 3.5) = \frac{f_{X, Y}(3.5, y)}{f_X(3.5)} =  
         \begin{cases}
       \frac{2/9}{1.5/9}, & 1.75 < y< 2.5\\
       0, & \text{otherwise.}
       \end{cases}
      \]
1. See the bottom right plot of Figure \@ref(fig:uniform-sum-max-conditional-plot); the conditional pdf corresponds to the renormalized slice of the joint pdf along $X=6$. If $X=6$ then $Y$ takes values between 3 and 4, and the density is constant over this interval of length 1.
\[
 f_{Y|X}(y| 6) = 
   \begin{cases}
 1, & 3 < y < 4,\\
 0, & \text{otherwise.}
 \end{cases}
\]
1. There are two general cases.  If $2<x<5$ then $Y$ takes values between $0.5x$ and $x-1$, and $f_X(x) = (1/9)(x-2)$. If $5<x<8$ then $Y$ takes values between $0.5x$ and $4$, and $f_X(x) = (1/9)(8-x)$.
      \[
       f_{Y|X}(y| x) = \frac{f_{X, Y}(x, y)}{f_X(x)} =  
         \begin{cases}
       \frac{2/9}{(1/9)(x-2)}, & 2<x<5, 0.5x< y< x-1\\
       \frac{2/9}{(1/9)(8-x)}, & 5<x<8, 0.5x< y< 4\\
       0, & \text{otherwise.}
       \end{cases}
      \]
    Therefore,
      \[
       f_{Y|X}(y| x) =   
         \begin{cases}
       \frac{1}{0.5x - 1}, & 2<x<5, 0.5x< y< x-1\\
       \frac{1}{4 - 0.5x}, & 5<x<8, 0.5x< y< 4\\
       0, & \text{otherwise.}
       \end{cases}
      \]
    Even though the above expression involves both $x$ and $y$, we are treating $x$ as fixed as $y$ as the variable.  For a given $x$, the conditional pdf of $Y$ given $X=x$ is a density of values of $Y$ only.
    
```

(ref:cap-uniform-sum-max-conditional) Illustration of the joint pdf and conditional pdfs in Example \@ref(exm:uniform-sum-max-conditional).


```{r uniform-sum-max-conditional-plot, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-uniform-sum-max-conditional)"}

dfA <- data.frame(x = c(2, 8, 5, 2),
                 y = c(1, 4, 4, 1),
                 v = c(1, 1, 1, 1))

p <- ggplot(data = dfA, aes(x = x, y = y)) +
  geom_polygon(fill="#21908cff", show.legend = FALSE) +
  scale_x_continuous(limits = c(2, 8), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(1, 4), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression("X")) +
  ylab(expression("Y")) +
  theme(plot.title = element_text(hjust = 0.5)) 

pA <- p +
  geom_hline(yintercept=3, size=2, color = "orange")

pB <- p +
  geom_hline(yintercept=4, size=3, color = "orange")

pC <- p +
  geom_vline(xintercept=3.5, size=2, color = "orange")

pD <- p +
  geom_vline(xintercept=6, size=2, color = "orange")

ggarrange(pA, pB, pC, pD, ncol = 2, nrow = 2)


```

The conditional pdf of $Y$ given $X=x$ can be integrated to find conditional probabilities of events involving $Y$ given $X=x$.  
\begin{align*}
\IP(a \le Y \le b \vert X = x) & =\int_a^b f_{Y|X}(y|x) dy, \qquad \text{for all } -\infty \le a \le b \le \infty
\end{align*}
In the above, $x$ is treated as fixed and $y$ as the variable being integrated over.  The resulting probability $\IP(a<Y<b|X=x)$ will be a function of $x$.  For a given $x$, $\IP(a<Y<b|X=x)$ is a single number.

**Be careful** when conditioning with continuous random variables. Remember to specify possible values!  And to note how conditioning can change the possible values.

```{example, uniform-sum-max-conditional-sim-forever2}

Donny Don't writes the following Symbulate code approximate the conditional distribution of $X$ given $Y=3$ in Example \@ref(exm:uniform-sum-max-conditional).
What do you think will happen when Donny runs his code?
 
```

```{python, eval = FALSE, message = FALSE, warning = FALSE}

P = (Uniform(0, 1) ** 2)

X = RV(P, sum)

Y = RV(P, max)

(X | (Y == 3) ).sim(10000)

```







```{solution, uniform-sum-max-conditional-sim-forever2-sol}
to Example \@ref(exm:uniform-sum-max-conditional-sim-forever2)
```

```{asis, fold.chunk = TRUE}

Donny's code will run forever! Remember, $Y$ is a *continuous* random variable, so $\IP(Y = 3)=0$.
The simulation will never return a single value of $Y$ equal to $3.0000000\ldots$, let alone 10000 of them.

```

**Be careful** when conditioning with continuous random variables. Remember that the probability that a continuous random variable is equal to a particular value is 0; that is, for continuous $X$, $\IP(X=x)=0$. When we condition on $\{X=x\}$ we are really conditioning on $\{|X-x|<\ep\}$ and seeing what happens in the idealized limit when $\ep\to0$.

When simulating, *never* condition on $\{X=x\}$; rather, condition on $\{|X-x|<\ep\}$ where $\ep$ represents some suitable degree of precision (e.g. $\ep=0.005$ if rounding to two decimal places).

Remember pdfs do not return probabilities directly; $f_{Y|X}(y|x)$ is not a probability of anything. But $f_{Y|X}(y|x)$ is related to the probability that $Y$ is "close to" $y$ given that $X$ is "close to" $x$:
\[
\IP(y-\ep/2<Y < y+\ep/2\; \vert\; x-\ep/2<X <  x+\ep/2) \approx \ep f_{Y|X}(y|x)
\]

The code below approximates conditioning on the event $\{Y = 3\}$ by conditioning instead on the event that $Y$ rounded to the nearest tenth is equal to 3, $\{|Y - 3| < 0.05\} = \{2.95<Y<3.05\}$.

```{python}

U1, U2 = RV(Uniform(1, 4) ** 2)

X = U1 + U2

Y = (U1 & U2).apply(max)

( (X & Y) | (abs(Y - 3) < 0.01) ).sim(10)

```

The code below simulates the approximate conditional distribution of $X$ given $Y=3$.
The event $\{|Y - 3|<0.05\}$ has approximate probability $0.1f_Y(3) = 0.1(4/9)=0.044$.
In order to obtain 10000 repetitions for which the event $\{|Y - 3|<0.05\}$, we would have to run around 225,000 repetitions in total.

```{python}
plt.figure()
( X | (abs(Y - 3) < 0.05) ).sim(10000).plot()
plt.show()

```




Given a joint pdf $f_{X, Y}$, it is sometimes possible to identify the  conditional pdfs in one direction just by inspection.  Remember that the conditional pdf of $Y$ given $X=x$ is a distribution of values of $Y$ alone.  Therefore, treat $x$ as fixed and view $f_{X, Y}$ as a function of $y$ alone; is the resulting function a recognizable pdf?  By "recognizable" we mean is it among a common named family of distributions like Uniform, Exponential, Normal, etc.  If the conditional pdf $f_{Y|X}$ can be identified, then the marginal pdf $f_X$ can be found without any calculus.  If fixing $x$ does not lead to a recognizable pdf, try fixing $y$ instead.

```{example, exponential-uniform-joint}

Suppose $X$ and $Y$ are continuous RVs with joint pdf
\[
f_{X, Y}(x, y) = \frac{1}{x}e^{-x}, \qquad x > 0,\quad 0<y<x.
\]

```

1. Donny Dont says: "Wait, the joint pdf is supposed to be a function of both $x$ and $y$ but $\frac{1}{x}e^{-x}$ is only a function of $x$." Explain to Donny how $f_{X, Y}$ here is, in fact, a function of both $x$ and $y$.
1. Identify by name the one-way conditional distributions that you can obtain from the joint pdf (without doing any calculus or computation).
1. Identify by name the marginal distribution you can obtain without doing any calculus or computation.
1. Describe how could you use the Exponential(1) spinner and the Uniform(0, 1) spinner to generate an $(X, Y)$ pair.
1. Sketch a plot of the joint pdf.
1. Sketch a plot of the marginal pdf of $Y$.
1. Set up the calculation you would perform to find the marginal pdf of $Y$.



```{solution, exponential-uniform-joint-sol}
to Example \@ref(exm:exponential-uniform-joint)
```

```{asis, fold.chunk = TRUE}

1. Don't forget the possible values. For example, $f_{X, Y}(1, 0.5) = e^{-1}$ but $f_{X, Y}(1, 2) = 0$. So $f_{X, Y}$ is a function of both $x$ and $y$.
1. Since $x$ shows up in more places, try conditioning on $x$ first and treat it as fixed.
If $x$ is treated as constant, then $\frac{1}{x}e^{-x}$ is also treated as constant.
The conditional density of $Y$ given $X=x$, as a function of $y$ has the form
\[
f_{Y|X}(y|x) \propto \text{constant}, \quad 0<y<x
\]
The height along the $x$ slice is constant as a function of $y$, but don't forget to include the possible values.  That is, the conditional pdf of $Y$ is constant for values of $y$ between 0 and $x$ (and 0 otherwise).  Therefore, given $X=x$ the conditional distribution of $Y$ is the Uniform(0, $x$) distribution.  Now we just we need to fill in the correct constant that makes the density integrate to 1.  
\[
f_{Y|X}(y|x) = \frac{1}{x}, \quad 0<y<x
\]
Remember this is a density on values of $Y$ with $x$ fixed.  
1. Since we have the joint pdf and the conditional pdfs of $Y$ given each value of $X$, we can find the marginal pdf of $X$. For possible $(x, y)$ pairs
\begin{align*}
\text{Joint} & = \text{Conditional}\times \text{Marginal}\\
f_{X, Y}(x, y) & = f_{Y|X}(y|x)f_X(x) \\
 \frac{1}{x}e^{-x} & = \left(\frac{1}{x}\right) f_X(x) 
\end{align*}
Therefore
\[
f_X(x) = e^{-x}, \qquad x>0
\]
That is, the marginal distribution of $X$ is  the Exponential(1) distribution.
1. Spin the Exponential(1) spinner to generate $X$.
Then, given $X=x$, spin the Uniform(0, $x$) spinner to generate $Y$.
For example, if $X=2$ we want to generate $Y$ from a Uniform(0, 2) distribution.
Remember that we can generate values from any Uniform distribution by generating a value from a Uniform(0, 1) distribution and then applying an appropriate linear rescaling.
Spin the Uniform(0, 1) spinner to generate $U$, and let $Y=XU$.
For example, if $X=2$ then $Y=2U$ follows a Uniform(0, 2) distribution. In summary,

    - Spin the Exponential(1) spinner once and let $X$ be the result
    - Spin the Uniform(0, 1) spinner once and let $U$ be the result
    - Let $Y = XU$.

1. See the simulation output below for an illustration. Start by sketching $X$ values; the density will be highest near 0 and decrease as $x$ increases.  Then, for each value of $x$ sketch $y$ values uniformly between 0 and $x$.
1. Overall, $Y$ can take any positive value $y>0$. For each $y$, collapse the joint pdf over all $x$ values.  The density will be highest at $y=0$ and then decrease as $y$ increases. 
1. We can find the marginal pdf of $Y$ by integrating out the $x$'s in the joint pdf.  Remember that the joint pdf is 0 unless $y<x$. For $y>0$, 
\begin{align*}
f_Y(y) & = \int_{y}^\infty \frac{1}{x}e^{-x} \, dx 
\end{align*}
There is no simple closed form expression, but it is a function of $y$ alone, for $y>0$; the $x$'s have been integrated out.

```

```{python}

X, U = RV(Exponential(1) * Uniform(0, 1))

Y = X * U

plt.figure()
(X & Y).sim(1000).plot()
plt.show()

```

```{python}
plt.figure()
Y.sim(10000).plot()
plt.show()

```





<!-- Cumulative distribution functions can also be used to derive the joint pdf of multiple random variables.  If $F_{X, Y}$ is the joint cdf of $X$ and $Y$ then the joint pdf of $X$ and $Y$ is -->

<!-- \[ -->
<!-- f_{X, Y}(x, y) = \frac{\partial^2}{\partial x\partial y} F_{X, Y}(x, y) -->
<!-- \] -->

<!-- Remember: when taking a partial derivative with respect to one variable, treat the other variables like constants. -->

<!-- ```{example, uniform-sum-max-joint-pdf} -->

<!-- Recall the example in Section \@ref(sim-transform-joint). Let $\IP$ be the probability space corresponding to two spins of the Uniform(1, 4) spinner, and let $X$ be the sum of the two spins, and $Y$ the larger spin (or the common value if a tie). -->

<!-- ```  -->



<!-- ```{solution uniform-sum-max-joint-pdf-sol} -->
<!-- to Example \@ref(exm:uniform-sum-max-joint-pdf) -->
<!-- ``` -->


<!-- 1. Let $F_{X, Y}$ denote the joint cdf of $X$ and $Y$. Find $F_{X, Y}(3.5, 2)$. -->
<!-- 1. Find the joint cdf $F_{X, Y}$. -->
<!-- 1. Find the joint pdf $f_{X, Y}$. -->

<!-- ```{asis, fold.chunk = TRUE} -->

<!-- 1. $F_{X, Y}(3.5, 2) = \IP(X \le 3.5, Y \le 2)$. The $(U_1, U_2)$ pairs take values in the square $[1, 4]\times[1, 4]$.  Figure \@ref(fig:uniform-sum-max-joint-pdf-event) illustrates the event $\{X \le 3.5, Y \le 2\}$.  The shaded region has area $(1)(1)-(1/2)(0.5)(0.5) = 0.875$.  Since $(U_1, U_2)$ pairs are uniformly distributed over the square region with area 9, $\IP(X \le 3.5, Y \le 2) = 0.875 / 9 = 0.0972$. -->
<!-- 1. $F_{X, Y}(x, y) = \IP(X \le x, Y \le y)$. We repeat the calculation from the previous part with a generic $(x, y)$.  Let $(x, y)$ be a possible value of $(X, Y)$; that is, $2<x<8$, $1<y<4$, and $y + 1 < x < 2y$. The event $\{X \le x, Y \le y\}$ will have a shape like the one in Figure \@ref(fig:uniform-sum-max-joint-pdf-event), with area $(y-1)^2-(1/2)(2y-x)^2$.  Since $(U_1, U_2)$ pairs are uniformly distributed over the square region with area 9 -->
<!--     \[ -->
<!--     F_{X, Y}(x, y) = (1/9)\left((y-1)^2-(1/2)(2y-x)^2\right), \quad 2 < x < 8, 1 < y < 4, y + 1< x < 2y. -->
<!--     \] -->
<!-- 1. Differentiate the cdf with respect to both $x$ and $y$ -->
<!--     \begin{align*} -->
<!--         F_{X, Y}(x, y) & = (1/9)\left((y-1)^2-(1/2)(2y-x)^2\right)\\ -->
<!--         \Rightarrow \frac{\partial}{\partial x}F_{X, Y}(x, y) & = \frac{\partial}{\partial x}(1/9)\left((y-1)^2-(1/2)(2y-x)^2\right)\\ -->
<!--         & = (1/9)(2y-x)\\ -->
<!--                 \Rightarrow \frac{\partial^2}{\partial x\partial y}F_{X, Y}(x, y) & = \frac{\partial}{\partial y}(1/9)(2y-x)\\ -->
<!--         = 2/9 -->
<!--     \end{align*} -->
<!--     Therefore -->
<!--     \[ -->
<!--     f_{X, Y}(x, y) = -->
<!--       \begin{cases} -->
<!--     2/9, & 2<x<8,\; 1<y<4,\; x/2<y<x-1,\\ -->
<!--     0, & \text{otherwise} -->
<!--     \end{cases} -->
<!--     \] -->


<!-- ``` -->


<!-- (ref:cap-uniform-sum-max-joint-pdf-event) The event $\{X \le 3.2, Y \le 2\}$ for $X=U_1+U_2$, the sum, and $Y=\max(U_1, U_2)$, the max, of two spins $U_1, U_2$ of a Uniform(1, 4) spinner. -->


<!-- ```{r uniform-sum-max-joint-pdf-event, echo=FALSE, fig.cap="(ref:cap-uniform-sum-max-joint-pdf-event)"} -->

<!-- dfA <- data.frame(x = c(2, 2, 1.5, 1, 1), -->
<!--                  y = c(1, 1.5, 2, 2, 1), -->
<!--                  v = c(1, 1, 1, 1, 1)) -->

<!-- pA <- ggplot(data = dfA, aes(x = x, y = y)) + -->
<!--   geom_polygon(fill="cornflowerblue", show.legend = FALSE) + -->
<!--    geom_segment(aes(x = 2.5, y = 1, xend = 1, yend = 2.5), color = "orange", linetype = "dashed") + -->
<!--   geom_segment(aes(x = 1, y = 2, xend = 2, yend = 2), color = "seagreen", linetype = "dotted") + -->
<!--   geom_segment(aes(x = 2, y = 1, xend = 2, yend = 2), color = "seagreen", linetype = "dotted") + -->
<!--   scale_x_continuous(limits = c(1, 4), expand = c(0, 0)) +  -->
<!--   scale_y_continuous(limits = c(1, 4), expand = c(0, 0)) + -->
<!--   theme_classic() + -->
<!--   theme(panel.border = element_rect(linetype = "solid", fill = NA)) + -->
<!--   theme(plot.margin=unit(c(1,1,1,1),"cm")) + -->
<!--   xlab(expression(U[1])) + -->
<!--   ylab(expression(U[2])) + -->
<!--   ggtitle("Event {U1 + U2 <= 3.5, max(U1, U2) <= 2} is shaded") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->

<!-- plot(pA) -->

<!-- # library(ggpubr) -->
<!-- # ggarrange(pA, pB, pC, pD, ncol = 2, nrow = 2) -->

<!-- ``` -->



<!-- ```{example meeting-waiting-uniform-pdf} -->

<!-- Continuing Example \@ref(exm:meeting-waiting-uniform), let $R$ be the random variable representing Regina's arrival time in $[0, 1]$, and $Y$ for Cady. The random variable $T=\min(R, Y)$ represents the time in $(0, 1)$ at which the first person arrives. The random variable $W = |R - Y|$ represents the amount of time the first person to arrive waits for the second person to arrive.   -->

<!-- ``` -->

<!-- 1. Let $F_W$ be the cdf of $W$.  Find $F_{W}(0.25)$. -->
<!-- 1. Find the cdf $F_W$. -->
<!-- 1. Find the pdf of $W$.  What does this tell you about the distribution of waiting times? -->
<!-- 1. Let $F_T$ be the cdf of $T$.  Find $F_{T}(0.25)$. -->
<!-- 1. Find the cdf $F_T$. -->
<!-- 1. Find the pdf of $T$.  What does this tell you about the time of the first arrival? -->
<!-- 1. Are $T$ and $W$ the same random variable? -->
<!-- 1. Do $T$ and $W$ have the same distribution? -->


<!-- ```{solution meeting-waiting-uniform-pdf-sol} -->

<!-- to Example \@ref(exm:meeting-waiting-uniform-pdf) -->

<!-- ``` -->

<!-- ```{asis, fold.chunk = TRUE} -->

<!-- 1. $F_W(0.25) = \IP(W \le 0.25)$.  We computed this in Example \@ref(exm:meeting-waiting-uniform); $F_W(0.25) = 1 - (1-0.25)^2$.  See the plot on the left in Figure \@ref(fig:meeting-waiting-uniform-pdf-plot) below. -->
<!-- 1. We repeat the calculation in the previous part for a generic $w$ in $(0, 1)$. -->
<!--     \[ -->
<!--     F_W(w) = 1 - (1 - w)^2, \quad 0 < w <1 -->
<!--     \] -->
<!-- 1. Differentiate the cdf with respect to $w$. -->
<!--     \[ -->
<!--     f_W(w) = 2(1 - w), \quad 0 < w <1 -->
<!--     \] -->
<!--     Waiting time has highest density for short waiting times and lowest density for long waiting times. -->
<!-- 1. $F_T(0.25) = \IP(T \le 0.25)$.  See the plot on the right in Figure \@ref(fig:meeting-waiting-uniform-pdf-plot) below. $\IP(T \le 0.25) = 1 - (1-0.25)^2$. -->
<!-- 1. We repeat the calculation in the previous part for a generic $t$ in $(0, 1)$. -->
<!--     \[ -->
<!--     F_T(t) = 1 - (1 - t)^2, \quad 0 < t <1 -->
<!--     \] -->
<!-- 1. Differentiate the pdf with respect to $t$. -->
<!--     \[ -->
<!--     f_T(t) = 2(1 - t), \quad 0 < t <1 -->
<!--     \] -->
<!--     Time of first arrival has highest density near 0 and lowest density near 1. -->
<!-- 1. No, $T$ and $W$ are not the same random variable.  For example, if they both arrive at time 0.5, then $T$ is 0.5 but $W$ is 0. -->
<!-- 1. Yes, they do have the same distribution.  They have the same pdf (and cdf). -->

<!-- ``` -->

<!-- (ref:cap-meeting-waiting-uniform-pdf) Illustration of the events $\{W \le 0.25\}$ in Example \@ref(exm:meeting-waiting-uniform-pdf). The square represents the sample space $\Omega=[0,1]\times[0,1]$. -->


<!-- ```{r meeting-waiting-uniform-pdf-plot, echo=FALSE, warning=FALSE, message=FALSE, fig.show="hold", out.width="50%", fig.cap="(ref:cap-meeting-waiting-uniform-pdf)"} -->


<!-- dfC <- data.frame(x = c(0, 0.25, 1, 1, 0.75, 0), -->
<!--                  y = c(0, 0, 0.75, 1, 1, 0.25), -->
<!--                  v = c(1, 1, 1, 1, 1, 1)) -->

<!-- pC <- ggplot(data = dfC, aes(x = x, y = y)) + -->
<!--   geom_polygon(fill="cornflowerblue", show.legend = FALSE) + -->
<!--   scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +  -->
<!--   scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) + -->
<!--   theme_classic() + -->
<!--   theme(panel.border = element_rect(linetype = "solid", fill = NA)) + -->
<!--   theme(plot.margin=unit(c(1,1,1,1),"cm")) + -->
<!--   xlab(expression(R)) + -->
<!--   ylab(expression(Y)) + -->
<!--   ggtitle("Event {|R - Y| < 0.25}") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->

<!-- plot(pC) -->

<!-- dfB <- data.frame(x = c(0.25, 1, 1, 0.25), -->
<!--                   y = c(0.25, 0.25, 1, 1), -->
<!--                   v = c(1, 1, 1, 1)) -->

<!-- pB <- ggplot(data = dfB, aes(x = x, y = y)) + -->
<!--   geom_polygon(fill = "white", show.legend = FALSE) + -->
<!--   scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +  -->
<!--   scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) + -->
<!--   theme_classic() + -->
<!--   theme(panel.border = element_rect(linetype = "solid", fill = NA)) + -->
<!--   theme(panel.background = element_rect(fill = "cornflowerblue", -->
<!--                                     colour = "cornflowerblue")) + -->
<!--   theme(plot.margin=unit(c(1,1,1,1),"cm")) + -->
<!--   xlab(expression(R)) + -->
<!--   ylab(expression(Y)) + -->
<!--   ggtitle("Event {min(R, Y) < 0.25}") + -->
<!--   theme(plot.title = element_text(hjust = 0.5)) -->



<!-- plot(pB) -->

<!-- ``` -->


<!-- ```{python} -->

<!-- R, Y = RV(Uniform(0, 1) ** 2) -->

<!-- W = abs(R - Y) -->

<!-- T = (R & Y).apply(min) -->

<!-- plt.figure() -->
<!-- W.sim(10000).plot() -->
<!-- T.sim(10000).plot() -->
<!-- plt.show() -->

<!-- ``` -->






Be sure to distinguish between joint, conditional, and marginal distributions.

- The joint distribution is a distribution on $(X, Y)$ pairs. A mathematical expression of a joint distribution is a function of both values of $X$ and values of $Y$.  In particular, a joint pdf $f_{X, Y}$ is a function of both values of $X$ and values of $Y$. Pay special attention to the possible values; the possible values of one variable might be restricted by the value of the other.
- The conditional distribution of $Y$ given $X=x$ is a distribution on $Y$ values (among $(X, Y)$ pairs with a fixed value of $X=x$). A mathematical expression of a conditional distribution will involve both $x$ and $y$, but $x$ is treated like a fixed constant and $y$ is treated as the variable.  In particular, a conditional pdf $f_{Y|X}$ is a function of values of $Y$ for a fixed value of $x$; treat $x$ like a constant and $y$ as the variable. Note: the possible values of $Y$ might depend on the value of $x$, but $x$ is treated like a constant.
- The marginal distribution of $Y$ is a distribution on $Y$ values only, regardless of the value of $X$. A mathematical expression of a marginal distribution will have only values of the single variable in it; for example, an expression for the marginal distribution of $Y$ will only have $y$ in it (no $x$, not even in the possible values). In particular, a marginal pdf $f_Y$ is a function of values of $Y$ only.

## Independence of random variables


```{example, independent-rv-event}

Suppose $X$ and $Y$ are random variables whose joint pmf is represented by the following table.

|             | $p_{X, Y}(x, y)$ |      |      |      |          |
|------------:|-----------------:|-----:|-----:|-----:|---------:|
|  $x$ \\ $y$ |                  |    1 |    2 |    3 | $p_X(x)$ |
|           0 |                  | 0.20 | 0.50 | 0.10 |     0.80 |
|           1 |                  | 0.05 | 0.10 | 0.05 |     0.20 |
|    $p_Y(y)$ |                  | 0.25 | 0.60 | 0.15 |          |

```

1. Are the events $\{X=0\}$ and $\{Y=1\}$ independent?
1. Are the random variables $X$ and $Y$ are independent?  Why?
1. What would the joint pmf need to be in order for random variables with these marginal pmfs to be independent?



```{solution, independent-rv-event-sol}
to Example \@ref(exm:independent-rv-event)
```

```{asis, fold.chunk = TRUE}

1. Yes. $\IP(X=0, Y=1) = p_{X, Y}(0, 1) = 0.20 = (0.80)(0.25) = p_X(0)p_Y(1) = \IP(X=0)\IP(Y=1)$.
1. No. In particular, $\IP(X = 0) = 0.8$, but $\IP(X = 0|Y=3) = 0.10/0.15 = 2/3$. Knowing the value of one of the variables changes the distribution of the other.
1. We need each joint probability to be the product of the corresponding marginal probability as in part 1.

    |             | $p_{X, Y}(x, y)$ |      |      |      |          |
    |------------:|-----------------:|-----:|-----:|-----:|---------:|
    |  $x$ \\ $y$ |                  |    1 |    2 |    3 | $p_X(x)$ |
    |           0 |                  | 0.20 | 0.48 | 0.12 |     0.80 |
    |           1 |                  | 0.05 | 0.12 | 0.03 |     0.20 |
    |    $p_Y(y)$ |                  | 0.25 | 0.60 | 0.15 |          |

``` 




```{definition independent-rvs}

Two random variables $X$ and $Y$ defined on a probability space with probability measure $\IP$ are **independent** if $\IP(X\le x, Y\le y) = \IP(X\le x)\IP(Y\le y)$ for all $x, y$.  That is, two random variables are independent if their joint cdf is the product of their marginal cdfs.

```

Random variables $X$ and $Y$ are independent if and only if the joint distribution factors into the product of the marginal distributions. The definition is in terms of cdfs, but analogous statements are true for pmfs and pdfs. Intuitively, random variables $X$ and $Y$ are independent if and only if the conditional distribution of one variable is equal to its marginal distribution  regardless of the value of the other.


Discrete random variables are independent if and only if the joint pmf is the product of the marginal pmfs, and if and only if the conditional pmfs are equal to the corresponding marginal pmfs. For discrete random variables $X$ and $Y$, the following are equivalent.

\begin{align*}
\text{Discrete RVs $X$ and $Y$} & \text{ are independent}\\
\Longleftrightarrow p_{X,Y}(x,y) & = p_X(x)p_Y(y) & & \text{for all $x,y$}\\
\Longleftrightarrow p_{X|Y}(x|y) & = p_X(x)  & &  \text{for all $x,y$}
\\
\Longleftrightarrow p_{Y|X}(y|x)  & = p_Y(y)  & &  \text{for all $x,y$}
\end{align*}


Continuous random variables are independent if and only if the joint pdf is the product of the marginal pdfs, and if and only if the conditional pdfs are equal to the corresponding marginal pdfs. For continuous random variables $X$ and $Y$, the following are equivalent.

\begin{align*}
\text{Continuous RVs $X$ and $Y$} & \text{ are independent}\\
\Longleftrightarrow f_{X,Y}(x,y) & = f_X(x)f_Y(y) & & \text{for all $x,y$}\\
\Longleftrightarrow f_{X|Y}(x|y) & = f_X(x)  & &  \text{for all $x,y$}
\\
\Longleftrightarrow f_{Y|X}(y|x)  & = f_Y(y)  & &  \text{for all $x,y$}
\end{align*}

If $X$ and $Y$ are independent, then the (renormalized) distributions of $Y$ values along each $X$-slice have the same shape as each other, and the same shape as the marginal distribution of $Y$.
Likewise, If $X$ and $Y$ are independent, then the (renormalized) distributions of $X$ values along each $Y$-slice have the same shape as each other, and the same shape as the marginal distribution of $X$.

```{example, poisson-hr-conditional}

Recall Example \@ref(exm:poisson-hr-joint). Let $X$ be the number of home runs hit by the home team, and $Y$ the number of home runs hit by the away team in a randomly selected MLB game.  Suppose that $X$ and $Y$ have joint pmf

\[
p_{X, Y}(x, y)
=
\begin{cases}
e^{-2.3}\frac{x^{1.2}y^{1.1}}{x!y!}, & x = 0, 1, 2, \ldots; y = 0, 1, 2, \ldots,\\
0, & \text{otherwise.}
\end{cases}
\]

The marginal pmf of $X$ is
\[
p_{X}(x) = e^{-1.2}\frac{x^{1.2}}{x!},\quad  x = 0, 1, 2, \ldots
\]

The marginal pmf of $Y$ is
\[
p_{Y}(y) = e^{-1.1}\frac{y^{1.1}}{y!},\quad  y = 0, 1, 2, \ldots
\]

```

1. Find the probability that the home teams hits 2 home runs.
1. Are $X$ and $Y$ independent?  (Note: we're asking about independence in terms of the assumed probability model, not for your opinion based on your knowledge of baseball.)
1. Find the probability that the home teams hits 2 home runs and the away team hits 1 home run.
1. Find the probability that the home teams hits 2 home runs given the away team hits 1 home run.
1. Find the probability that the home teams hits 2 home runs given the away team hits at least 1 home run.


```{solution poisson-hr-conditional-sol}
to Example \@ref(exm:poisson-hr-conditional)
```

```{asis, fold.chunk = TRUE}

1. Use the marginal pmf of $X$. $\IP(X = 2) = p_{X}(2) =e^{-1.2}\frac{1.2^2}{2!}=0.217$.
1. Yes, the joint pmf is the product of the marginal pmfs.
1. Since $X$ and $Y$ are independent $\IP(X = 2, Y = 1) = p_{X, Y}(2, 1) = p_{X}(2)p_{Y}(1) = e^{-1.2}\frac{1.2^2}{2!}e^{-1.1}\frac{1.1^1}{1!}= 0.0794$.
1. Since $X$ and $Y$ are independent $\IP(X = 2|Y = 1) = \IP(X = 2) = p_{X}(2) =e^{-1.2}\frac{1.2^2}{2!}= 0.217$.
1. Since $X$ and $Y$ are independent $\IP(X = 2|Y \ge 1) = \IP(X = 2) = p_{X}(2) =e^{-1.2}\frac{1.2^2}{2!}= 0.217$.

```

Are home and away team home runs in baseball games really independent? Compare Figured \@ref(fig:poisson-hr-mosaic) and \@ref(fig:poisson-hr-mosaic-data). The mosaic plot in Figure \@ref(fig:poisson-hr-mosaic) represents Example \@ref(exm:poisson-hr-conditional) in which $X$ and $Y$ are assumed to be independent.


(ref:cap-poisson-hr-mosaic) Mosaic plot for Example \@ref(exm:poisson-hr-conditional) where $X$ and $Y$ are assumed to be independent.  The plot represents conditioning on the value of home team home runs, $X$. The colors represent different values of away team home runs, $Y$.

```{r poisson-hr-mosaic, echo=FALSE, fig.cap="(ref:cap-poisson-hr-mosaic)"}

knitr::include_graphics(c("_graphics/hr-mosaic-poisson.png"))

```


Let's compare with some real data. The mosaic plot in Figure \@ref(fig:poisson-hr-mosaic-data) is based on actual home run data from the 2018 MLB season.  While there is not an exact match, the model from Example \@ref(exm:poisson-hr-conditional) seems to describe the data reasonably well.  In particular, there does not appear to be much dependence between home runs hit by the two teams in a game.

(ref:cap-poisson-hr-mosaic-data) Mosaic plot based on home run data from the 2018 MLB season. The plot represents conditioning on the value of home team home runs, $X$. The colors represent different values of away team home runs, $Y$.

```{r poisson-hr-mosaic-data, echo=FALSE, fig.show="hold", out.width="50%", fig.cap="(ref:cap-poisson-hr-mosaic-data)"}

knitr::include_graphics(c("_graphics/hr-mosaic-2018.png"))

```

```{example, exponential-uniform-independent}
Let $X$ and $Y$ be continuous random variables with joint pdf

\[
f_{X, Y}(x, y) = e^{-x}, \qquad x>0,\; 0<y<1. 
\]

```

1. Without doing any calculations, find the conditional distributions and marginal distributions.
1. Are $X$ and $Y$ independent?
1. Sketch a plot of the joint pdf of $X$ and $Y$.
1. Find $\IP(X<0.2, Y<0.4)$.
1. Find $\IP(X<0.2| Y<0.4)$.

```{solution exponential-uniform-independent-sol}
to Example \@ref(exm:exponential-uniform-independent)
```

```{asis, fold.chunk = TRUE}

1. Fix $y$.  Then as a function of $x$, the conditional pdf of $X$ given $Y=y$ is proportional to $e^{-x}, x>0$.  That is true for any $0<y<1$. So regardless of the value of $Y$, $X$ follows an Exponential(1) distribution.  Now fix $x$. Regardless of the value of $x$, the conditional pdf of $Y$ given $X=x$ is constant over the (0, 1) range of possible $y$ values. That is, regardless of the value of $X$,  $Y$ follows a Uniform(0, 1) distribution.
1. Yes, because of the previous part. The joint pdf factors in the product of marginal pdfs. By carefully inspecting the joint pdf, we can determine, without any calculations, that $X$ and $Y$ are independent, $X$ has a marginal Exponential(1) distribution, and $Y$ has a Uniform(0, 1) distribution.
1. See the simulation results below.  Each vertical slice, corresponding to the distribution of values of $Y$ for a given value of $X$, has constant Uniform(0, 1) density. Each horizontal slice, corresponding to the distribution of values of $X$ for a given value of $Y$, has an Exponential(1) density.
1. $X$ and $Y$ are independent so $\IP(X<0.2, Y<0.4)=\IP(X<0.2)\IP(Y<0.4) = (1-e^{-0.2})(0.4)=0.073$.
1. $X$ and $Y$ are independent so $\IP(X<0.2| Y<0.4)=\IP(X<0.2) = 1-e^{-0.2}=0.181$.

```


Continuous random variables $X$ and $Y$ are independent if and only if their joint pdf can be factored into the product of a function of values of $X$ alone and a function of values of $Y$ alone.
That is, $X$ and $Y$ are independent if and only if there exist functions $g$ and $h$ for which
\[
f_{X,Y}(x,y) \propto g(x)h(y) \qquad \text{ for all $x$, $y$}
\]
Aside from normalizing constants, $g$ determines the shape of the marginal pdf for $X$, and $h$ for $Y$. Be careful: when determining if the pdfs can be factored, be sure to consider the range of possible $x$ and $y$ values. Random variables are not independent if the possible values of one variable can change given  the value of the other.

If $X$ and $Y$ are independent then the joint pdf factors into a product of the marginal pdfs.
The above result says that if the joint pdf factors into two separate factors --- one involving values of $X$ alone, and one involving values of $Y$ alone --- then $X$ and $Y$ are independent.
The above result is useful because it allows you to determine whether $X$ and $Y$ are independent without first finding their marginal distributions.

Some Symbulate code for Example \@ref(exm:exponential-uniform-independent) follows.  We can define two independent random variables by specifying their joint distribution as the product `*` of their marginal distributions.


```{python}

X, Y = RV(Exponential(1) * Uniform(0, 1))

plt.figure()
(X & Y).sim(10000).plot('density')
plt.show()

```

We can also simulate conditional distributions.  Remember: be careful when conditioning on the value of a continuous random variable.

```{python}
plt.figure()
(X | (abs(Y - 0.1) < 0.05) ).sim(1000).plot(bins = 20) # given Y "=" 0.1
(X | (abs(Y - 0.5) < 0.05) ).sim(1000).plot(bins = 20) # given Y "=" 0.5
(X | (abs(Y - 0.7) < 0.05) ).sim(1000).plot(bins = 20) # given Y "=" 0.7
plt.show()

```


```{python}
plt.figure()
(Y | (abs(X - 0.1) < 0.05) ).sim(1000).plot(bins = 20) # given X "=" 0.1
(Y | (abs(X - 0.5) < 0.05) ).sim(1000).plot(bins = 20) # given X "=" 0.5
(Y | (abs(X - 0.7) < 0.05) ).sim(1000).plot(bins = 20) # given X "=" 0.7
plt.show()

```

Donny Dont wants to code this example in Symbulate as 

```{python, eval = FALSE, }

X = RV(Exponential(1))
Y = RV(Uniform(0, 1))

```

Unfortunately, the above code returns an error when attempting `(X & Y).sim(1000)`.  The reason is that the above code only specifies the marginal distributions of $X$ and $Y$, but the joint distribution is needed to generate $(X, Y)$ pairs.  If you want $X$ and $Y$ to be independent, you need to make that explicit.  One way to fix Donny's code is to add a line which tells Symbulate that $X$ and $Y$ are independent, in which case it is enough to specify the marginal distributions.


```{python}

X = RV(Exponential(1))
Y = RV(Uniform(0, 1))

X, Y = AssumeIndependent(X, Y)

plt.figure()
(X & Y).sim(10000).plot('density')
plt.show()

```





