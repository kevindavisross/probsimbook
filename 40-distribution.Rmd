
# Distributions {#distchap}

<!-- \newcommand{\IP}{\textrm{P}} -->
<!-- \newcommand{\IQ}{\textrm{Q}} -->
<!-- \newcommand{\E}{\textrm{E}} -->
<!-- \newcommand{\Var}{\textrm{Var}} -->
<!-- \newcommand{\SD}{\textrm{SD}} -->
<!-- \newcommand{\Cov}{\textrm{Cov}} -->
<!-- \newcommand{\Corr}{\textrm{Corr}} -->
<!-- \newcommand{\Xbar}{\bar{X}} -->
<!-- \newcommand{\Ybar}{\bar{X}} -->
<!-- \newcommand{\xbar}{\bar{x}} -->
<!-- \newcommand{\ybar}{\bar{y}} -->
<!-- \newcommand{\ind}{\textrm{I}} -->
<!-- \newcommand{\dd}{\text{DDWDDD}} -->
<!-- \newcommand{\epsilon}{\epsilonsilon} -->
<!-- \newcommand{\reals}{\mathbb{R}} -->

Random variables can potentially take many different values, usually with some values or intervals of values more likely than others.  We have used simulation to investigate the pattern of variability of random variables.  Plots and summary statistics like the ones encountered in the previous chapters summarize distributions of random variables.

The **(probability) distribution** of a random variable specifies the possible values
of the random variable and a way of determining corresponding probabilities.  We will see several ways to describe distributions, some of which depend on the number and types of the random variables under investigation.

Commonly encountered random variables can be classified as discrete or
continuous (or a mixture of the two^[There is another type of weird random variable which has a "singular" distribution, like the [Cantor distribution](https://en.wikipedia.org/wiki/Cantor_distribution), but we're counting these random variables as not commonly encountered.]).

- A **discrete** random variable can take on only countably many isolated points on a
number line. These are often counting type variables.  Note that "countably many" includes  the case of countably infinite, such as $\{0, 1, 2, \ldots\}$.
- A **continuous** random variable can take any value within some uncountable interval, such as $[0, 1]$, $[0,\infty)$, or $(-\infty, \infty)$. These are
often measurement type variables.




We will see a few ways of specifying a distribution.

- A well labeled *plot*
- A *table* of possible values and corresponding probabilities for discrete random variables.  This could be a two-way table for the joint distribution of two discrete random variables. 
- A *probability mass function* for discrete random variables or a *probability density function* for continuous random variables which maps possible values $x$ --- or $(x, y)$ pairs, etc --- to their respective probability (for discrete) or density (for continuous).
- A *cumulative distribution function*, which provides all the percentiles of the distribution
- By *name, including values of relevant parameters*, e.g., "Exponential(1)", "Normal(500, 100)", "Binomial(5, 0.3)", "BivariateNormal(500, 500, 100, 100, 0.8)". Some probabilistic situations are so common that the corresponding distributions have special names.  Always be sure to specify values of relevant parameters, e.g., "Normal(500, 100) distribution" rather than just "Normal distribution".  Note that different named distributions have different identifying parameters. For example, the parameters 0 and 1 mean something different for the  Uniform(0, 1) distribution than for the Normal(0, 1) distribution.



## Do not confuse a random variable with its distribution {#distribution}



Heed the title.  A random variable measures a numerical quantity which depends on the outcome of a random phenomenon. The distribution of a random variable specifies the long run pattern of variation of values of the random variable over many repetitions of the underlying random phenomenon. The distribution of a random variable can be approximated by simulating an outcome of the random process, observing the value of the random variable for that outcome, repeating this process many times, and summarizing the results.  But a random variable is not its distribution.

A distribution is determined by:

- The underlying probability measure $\IP$, which represents all the assumptions about the random phenomenon.
- The random variable $X$ itself, that is, the function which maps sample space outcomes to numbers.

Changing either the probability measure or the random variable itself can change the distribution of the random variable. For example, consider the sample space of two rolls of a fair four-sided die.  In each of the following scenarios, the random variable $X$ has a different distribution.

1. The die is fair and $X$ is the sum of the two rolls
1. The die is fair and $X$ is the larger of the two rolls
1. The die is weighted to land on 1 with probability 0.1 and 4 with probability 0.4 and $X$ is the sum of the two rolls.

In particular, in (1) $X$ takes the value 2 with probability $1/16$, in (2) $X$ takes the value 2 with probability $3/16$, and in (3) $X$ takes the value 2 with probability 0.01. In (1) and (2), the probability measure is the same (fair die) but the function defining the random variable is different (sum versus max).  In (1) and (3), the function defining the random variable is the same, and the sample space of outcomes is the same, but the probability measure is different.

We often specify the distribution of a random variable directly without explicit mention of the underlying probability space or function defining the random variable.  For example, in the meeting problem we might assume that arrival times follow a Uniform(0, 60) or a Normal(30, 10) distribution. In situations like this, you can think of the probability space as being the distribution of the random variable and the function defining the random variable to be the identity function.  This idea corresponds to the "simulate from the distribution method": construct a spinner corresponding to the distribution of the random variable and spin it to simulate a value of the random variable.


```{example dd-same-distribution}

Donny Dont is thoroughly confused about the distinction between a random variable and its distribution.  Help him understand by by providing a simple concrete example of *two different random variables* $X$ and $Y$ that have the *same distribution*. Can you think of $X$ and $Y$ for which $\IP(X = Y) = 0$?  How about a discrete example and a continuous example?

```



```{solution dd-same-distribution-sol}
to Example \@ref(exm:dd-same-distribution)

```

```{asis, fold.chunk = TRUE}

Flip a fair coin 3 times and let $X$ be the number of heads and $Y$ be the number of tails.  Then $X$ and $Y$ have the same distribution, because they have the same long run pattern of variability. Each variable takes values 0, 1, 2, 3 with probability 1/8, 3/8, 3/8, 1/8.
But they are not the same random variable; they are measuring different things.
If the outcome is HHT then $X$ is 2 but $Y$ is 1. In this case $\IP(X = Y)=0$; in an odd number of flips it's not possible to have the same number of heads and tails on any single outcome.

In some cases of the meeting time problem we assumed the distribution of Regina's arrival time $R$ is Uniform(0, 60) and the distribution of Cady's arrival time $Y$ is Uniform(0, 60).  So $R$ and $Y$ have the same distribution.
But these are two random variables; one measures Regina's arrival time and one measure Cady's.  If Regina and Cady met every day for a year, then the day-to-day pattern of Regina's arrival times would look like the day-to-day pattern of Cady's arrival times.  But on any given day, their arrival times would not be the same, since $R$ and $Y$ are continuous random variables so $\IP(R = Y) = 0$.



```



A distribution, like a spinner, is a blueprint for simulating values of the random variable.  If two random variables have the same distribution, you could use the same spinner to simulate values of either random variable.  But a distribution is not the random variable itself. (In other words, "the map is not the territory.")




Two random variables can have the same (long run) distribution, even if the values of the two random variables are never equal on any particular repetition (outcome). If $X$ and $Y$ have the same distribution, then the spinner used to simulate $X$ values can also be used to simulate $Y$ values; in the long run the patterns would be the same.

At the other extreme, two random variables $X$ and $Y$ are the same random variable only if for every outcome of the random phenomenon the resulting values of $X$ and $Y$ are the same. That is, $X$ and $Y$ are the same random variable only if they are the same *function*: $X(\omega)=Y(\omega)$ for all $\omega\in\Omega$.  It is possible to have two random variables for which $\IP(X=Y)$ is large, but $X$ and $Y$ have different distributions.



Many commonly encountered distributions have special names. For example, the distribution of $X$,  the number of heads in 3 flips of a fair coin,  is called the "Binomial(3, 0.5)" distribution.  If a random variable has a Binomial(3, 0.5) distribution then it takes the possible values 0, 1, 2, 3, with respective probability 1/8, 3/8, 3/8, 1/8. The random variable in each of the following situations has a Binomial(3, 0.5) distribution.

- $Y$ is the number of Tails in three flips of a fair coin
- $Z$ is the number of even numbers rolled in three rolls of a fair six-sided die
- $W$ is the number of female babies in a random sample of three births at a hospital (assuming boys and girls are equally likely^[[Which isn't quite true](https://www.npr.org/sections/health-shots/2015/03/30/396384911/why-are-more-baby-boys-born-than-girls).])

Each of these situations involves a different sample space of outcomes (coins, dice, births) with a random variable which counts different things (Heads, Tails, evens, boys). But all the scenarios have some general features in common:

- There are 3 "trials" (3 flips, 3 rolls, 3 babies)
- Each trial can be classified as "success"^[There is no value judgment; sSuccess" just refers to whatever we're counting. Did the thing we're counting happen on this trial ("success) or not ("failure"). Success isn't necessarily good.]  (Tails, even, female) or "failure".
- Each trial is equally likely to result in success or not (fair coin, fair die, assuming boys and girls are equally likely)
- The trials are independent. For coins and dice the trials are physically independent. For births independence follows from random selection from a large population.
- The random variable counts the number of successes in the 3 trials (number of T, number of even rolls, number of female babies).

These examples illustrate that knowledge that a random variable has a specific distribution (e.g., Binomial(3, 0.5)) does not necessarily convey any information about the underlying outcomes or random variable (function) being measured.  (We will study Binomial distributions in more detail later.) 

The scenarios involving $W, X, Y, Z$ illustrate that two random variables do not have to be defined on the same sample space in order to determine if they have the same distribution.  This is in contrast to computing quantities like $\IP(X=Y)$: $\{X=Y\}$ is an event which cannot be investigated unless $X$ and $Y$ are defined for the same outcomes.  For example, you could not estimate the probability that a student has the same score on both SAT Math and Reading exams unless you measured pairs of scores for each student in a sample.  However, you could collect SAT Math scores for one set of students to estimate the marginal distribution of Math scores, and collect Reading scores for a separate set of students to estimate the marginal distribution of Reading scores.

A random variable can be defined explicitly as a function on a probability space, or implicitly through its distribution.
The distribution of a random variable is often assumed or specified directly,
without mention of the underlying probabilty space or the function
defining the random variable. For example, a problem might state "let $Y$ have a Binomial(3, 0.5) distribution" or "let $Y$ have a Normal(30, 10) distribution". But remember, such statements do not necessarily convey any
information about the underlying sample space outcomes or random variable (function) being
measured. In Symbulate the `RV` command can also be used to define a RV implicitly via its distribution. A definition like `X = RV(Binomial(3, 0.5))` effectively defines a
random variable `X` on an unspecified probability space via an
unspecified function.


```{python}

W = RV(Binomial(3, 0.5))

W.sim(10000).plot()
plt.show()

```



```{example dd-same-joint-distribution}

Suppose that $X$, $Y$, and $Z$ all have the same distribution.  Donny Dont says

1. The pair $(X, Y)$ has the same joint distribution as the pair $(X, Z)$.
1. $X+Y$ has the same distribution as $X+Z$.
1. $X+Y$ has the same distribution as $X+X=2X$.

Determine if each of Donny's statements is correct.  If not, explain why not using a simple example.


```



```{solution dd-same-joint-distibution-sol}
to Example \@ref(exm:dd-same-joint-distribution)
```


```{asis, fold.chunk = TRUE}

First of all, Donny's statements wouldn't even make sense unless the random variables were all defined on the same probability space.  For example, if $X$ is SAT Math score and $Y$ is SAT reading score it doesn't makes sense to consider $X+Y$ unless $(X, Y)$ pairs are measured for the same students.  But even assuming the random variables are defined on the same probability space, we can find counterexamples to Donny's statements.

As just one example, flip a fair coin 4 times and let

- $X$ be the number of heads in flips 1 through 3
- $Y$ be the number of tails in flips 1 through 3
- $Z$ be the number of heads in flips 2 through 4.

1. The joint distribution of $(X, Y)$ is not the same as the joint distribution of $(X, Z)$. For example, $(X, Y)$ takes the pair $(3, 3)$ with probability 0, but $(X, Z)$ takes the pair $(3, 3)$ with nonzero probability (1/16).
1. The distribution of $X+Y$ is not the same as the distribution of $X+Z$; $X+Y$ is 3 with probability 1, but the probability that $X+Z$ is 3 is less than 1 (4/16). 
1. The distribution of $X+Y$ is not the same as the distribution of $2X$; $X+Y$ is 3 with probability 1, but $2X$ takes values 0, 2, 4, 6 with nonzero probability. 



```


Remember that a joint distribution is a probability distribution on pairs of values.
Just because $X_1$ and $X_2$ have the same marginal distribution, and $Y_1$ and $Y_2$ have the same marginal distribution, doesn't necessary imply that $(X_1, Y_1)$ and $(X_2, Y_2)$ have the same joint distributions.  In general, information about the marginal distributions alone is not enough to determine information about the joint distribution.  We saw a related two-way table example in Section \@ref(dist-intro).  Just because two two-way tables have the same totals, they don't necessarily have the same interior cells.

The distribution of any random variable obtained via a transformation of multiple random variables will depend on the joint distribution of the random variables involved; for example, the distribution of $X+Y$ depends on the joint distribution of $X$ and $Y$.





```{example uniform-same-dist}

Consider the probability space corresponding to two spins of the Uniform(0, 1) spinner and let $U_1$ be the result of the first spin and $U_2$ the result of the second.  For each of the following pairs of random variables, determine whether or not they have the same distribution as each other.  No calculations necessary; just think conceptually.

```

1. $U_1$ and $U_2$
1. $U_1$ and $1-U_1$
1. $U_1$ and $1+U_1$
1. $U_1$ and $U_1^2$
1. $U_1+U_2$ and $2U_1$
1. $U_1$ and $1-U_2$
1. Is the joint distribution of $(U_1, 1-U_1)$ the same as the joint distribution of $(U_1, 1 - U_2)$?


```{solution uniform-same-dist-sol}
to Example \@ref(exm:uniform-same-dist)
```


```{asis, fold.chunk = TRUE}

1. Yes, each has a Uniform(0, 1) distribution.
1. Yes, each has a Uniform(0, 1) distribution.  For $u\in[0, 1]$, $1-u\in[0, 1]$, so $U_1$ and $1-U_1$ have the same possible values, and a linear rescaling does not change the shape of the distribution. Changing from $U_1$ to $1-U_1$ essentially amounts to switching the [0, 1] labels on the spinner from clockwise to counterclockwise.
1. No, the two variables do not have the same possible values.  The shapes would be similar though; $1+U_1$ has a Uniform(1, 2) distribution.
1. No, a non-linear rescaling generally changes the shape of the distribution.  For example, $\IP(U_1\le0.49) = 0.49$, but $\IP(U_1^2 \le 0.49) = \IP(U_1 \le 0.7) = 0.7$  Squaring a number in [0, 1] makes the number even smaller, so the distribution of $U_1^2$ places higher density on smaller values than $U_1$ does.
1. No, $U_1+U_2$ has a triangular shaped distribution on (0, 2) with a peak at 1. (The shape is similar to that of the distribution of $X$ in Section \@ref(sim-transform-joint), but the possible values are (0, 2) rather than (2, 8).) But $2U_1$ has a Uniform(0, 2) distribution.  Do not confuse a random variable with its distribution.  Just because $U_1$ and $U_2$ have the same distribution, you cannot replace $U_2$ with $U_1$ in transformations. The random variable $U_1+U_2$ is not the same random variable as $2U_1$; spinning a spinner and adding the spins will not necessarily produce the same value as spinner a spinner once and multiplying the value by 2.
1. Yes, just like $U_1$ and $1-U_2$ have the same distribution.
1. No. The marginal distributions are the same, but the joint distribution of $(U_1, 1-U_1)$ places all density along a line, while the joint density of $(U_1, 1-U_2)$ is distributed over the whole two-dimensional region $[0, 1]\times[0,1]$.

```


Do not confuse a random variable with its distribution. This is probably getting repetitive by now, but we're emphasizing this point for a reason.  Many common mistakes in probability problems involve confusing a random variable with its distribution.  For example, we will soon that if a continuous random variable $X$ has probability density function $f(x)$, then the probability density function of $X^2$ is NOT $f(x^2)$ nor $(f(x))^2$.  Mistakes like these, which are very common, essentially involve confusing a random variable with its distribution.  Understanding the fundamental difference between a random variable and its distribution will help you avoid many common mistakes, especially in problems involving a lot of calculus or mathematical symbols.



## Discrete random variables: Probability mass functions


Discrete random variables take at most countably many possible values (e.g. $0, 1, 2, \ldots$).  They are often, but not always, counting variables (e.g., $X$ is the number of Heads in 10 coin flips).  We have seen in several examples that the distribution of a discrete random variable can be specified via a table listing the possible values of $x$ and the corresponding probability $\IP(X=x)$.  Always be sure to specify the possible values of $X$.





In some cases, the distribution has a "formulaic" shape and  $\IP(X=x)$ can be written explicitly as a function of $x$. For example, let $X$ be the sum of two rolls of a fair four-sided die.  The distribution of $X$ is displayed below.  The probabilities of the possible $x$ values follow a clear triangular pattern as a function of $x$.

(ref:cap-dice-sum-dist-table22) The marginal distribution of $X$, the sum of two rolls of a fair four-sided die.

```{r, dice-sum-dist-table22, echo = FALSE}
y = 2:8
p = c(1, 2, 3, 4, 3, 2, 1) / 16
knitr::kable(
  data.frame(y, p),
  col.names = c("x", "P(X=x)"),
  booktabs = TRUE,
  caption = "(ref:cap-dice-sum-dist-table22)",
  digits = 4
)
```  

(ref:cap-dice-sum-dist-plot22) The marginal distribution of $X$, the sum of two rolls of a fair four-sided die.

```{r dice-sum-dist-plot22, echo = FALSE, fig.cap = "(ref:cap-dice-sum-dist-plot2)"}

ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 2:8) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "P(X = x)")

```


For each possible value $x$ of the random variable $X$, $\IP(X=x)$ can be obtained from the following formula

$$
p(x) =
\begin{cases}
\frac{4-|x-5|}{16}, & x = 2, 3, 4, 5, 6,7, 8,\\
0, & \text{otherwise.}
\end{cases}
$$

That is, $\IP(X = x) = p(x)$ for all $x$.
For example, $\IP(X = 2) = 1/16 = p(2)$; $\IP(X=5)=4/16=p(5)$; $\IP(X=7.5)=0=p(7.5)$.
To specify the distribution of $X$ we could provide Table \@ref(tab:dice-sum-dist-table22), or we could just provide the function $p(x)$ above. 
Notice that part of the specification of $p(x)$ involves the possible values of $x$; $p(x)$ is only nonzero for $x=2,3, \ldots, 8$.
Think of $p(x)$ as a compact way of representing Table \@ref(tab:dice-sum-dist-table22).
The function $p(x)$ is called the *probability mass function* of the discrete random variable $X$.




```{definition pmf}

The **probability mass function (pmf)** (a.k.a., density (pdf)^[We use "pmf" for discrete distributions and reserve "pdf" for continuous probability density functions.  The terms "pdf" and "density" are sometimes used in both discrete and continuous situations even though the objects the terms represent differ between the two situations (probability versus density).  In particular, in R the `d` commands (`dbinom`, `dnorm`, etc) are used for both discrete and continuous distributions. In Symbulate, you can use `.pmf()` for discrete distributions.]) of a *discrete* RV $X$, defined on a probability space with probability measure $\IP$, is a function $p_X:\mathbb{R}\mapsto[0,1]$ which specifies each possible value of the RV and the probability that the RV takes that particular value: $p_X(x)=\IP(X=x)$ for each possible value of $x$.

```


The axioms of probability imply that a valid pmf must satisfy
\begin{align*}
p_X(x) & \ge 0 \quad \text{for all $x$}\\
p_X(x) & >0 \quad \text{for at most countably many $x$ (the possible values, i.e., support)}\\
\sum_x p_X(x) & = 1
\end{align*}

The countable set of possible values of a discrete random variable $X$, $\{x: \IP(X=x)>0\}$, is called its **support**.

The pmf of a discrete random variable provides the probability of "equal to" events: $\IP(X = x)$.  Probabilities for other general events, e.g., $\IP(X \le x)$ can be obtained by summing the pmf over the range of values of interest.



We have seen that a distribution of a discrete random variable can be represented in a table, with a corresponding spinner.  Think of a pmf as providing a compact formula for constructing the table/spinner.



```{example, pmf-dice-max}

Let $Y$ be the larger of two rolls of a fair four-sided die.  Find the probability mass function of $Y$.

```



```{solution pmf-dice-max-sol}
to Example \@ref(exm:pmf-dice-max)
```





```{asis, fold.chunk = TRUE}

See Table \@ref(tab:dice-max-dist-table22) and  Figure \@ref(fig:dice-max-dist-plot22) below. As a function of $y=1, 2, 3, 4$, $\IP(Y=y)$ is linear with slope 2/16 passing through the point (1, 1/16).  The pmf of $Y$ is

\[
p_Y(y) =
\begin{cases}
\frac{2y-1}{16}, & y = 1, 2, 3, 4, \\
0, & \text{otherwise.}
\end{cases}
\]

For any $y$, $\IP(Y=y) = p_Y(y)$.  For example, $\IP(Y=2) = 3/16 = p_Y(2)$ and $\IP(Y = 3.3) = 0 = p_Y(3.3)$.

```




(ref:cap-dice-max-dist-table22) The marginal distribution of $Y$, the larger (or common value if a tie) of two rolls of a fair four-sided die.

```{r, dice-max-dist-table22, echo = FALSE}
y = 1:4
p = c(1, 3, 5, 7) / 16
knitr::kable(
  data.frame(y, p),
  col.names = c("y", "P(Y=y)"),
  booktabs = TRUE,
  caption = "(ref:cap-dice-max-dist-table2)",
  digits = 4
)
```  

(ref:cap-dice-max-dist-plot22) The marginal distribution of $Y$, the larger (or common value if a tie) of two rolls of a fair four-sided die.


```{r dice-max-dist-plot22, echo = FALSE, fig.cap = "(ref:cap-dice-max-dist-plot2)"}

ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "orange") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 2:8) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "y",
       y = "P(Y = y)")

```


```{example dd-pmf-label}

Donny Dont provides two answers to Example \@ref(exm:pmf-dice-max).  Are his answers correct?  If not, why not?
  
1. $p_Y(y) = \frac{2y-1}{16}$
1. $p_Y(x) = \frac{2x-1}{16},\; x = 1, 2, 3, 4$, and $p_Y(x)= 0$ otherwise.


```


```{solution dd-pmf-label-sol}
to Example \@ref(exm:dd-pmf-label)
```


```{asis, fold.chunk = TRUE}

1. Donny's solution is incomplete; he forgot to specify the possible values. It's possible that someone who sees Donny's expression would think that $p_Y(2.5)=4/16$.  You don't necessarily always need to write "0 otherwise", but do always provide the possible values.
1. Donny's answer is actually correct, though maybe a little confusing.  The important place to put a $Y$ is the subscript of $p$: $p_Y$ identifies this function as the pmf of the random variable $Y$, as opposed to any other random variable that might be of interest.  The argument of $p_Y$ is just a dummy variable that defines the function.  As an analogy, $g(u)=u^2$ is the same function as $g(x)=x^2$; it doesn't matter which symbol defines the argument.  It is convenient to represent the argument of the pmf of $Y$ as $y$, and the argument of the pmf of $X$ as $x$, but this is not necessary.  Donny's answer does provide a way of constructing Table \@ref(tab:dice-max-dist-table). 

```


When there are multiple discrete random variables of interest, we usually identify their marginal pmfs with subscripts: $p_X, p_Y, p_Z$, etc.

<!-- For example, if $X$ is the number of heads in three flips of a fair coin, then $\IP(X=x)$ can be written in terms of the following formula^[This is an example of a Binomial probability mass function.  We will see the details behind this formula in Section \@ref(sec-binomial).]. -->
<!-- \[ -->
<!-- \IP(X =x) = \frac{3!}{x!(3-x)!}(0.5)^{x}(0.5)^{3-x}, \qquad x = 0, 1, 2, 3. -->
<!-- \] -->
<!-- For example^[Recall factorial notation: if $k$ is a positive integer, then $k!=k(k-1)(k-2)\cdots (2)(1)$, e.g., $5! = 5(4)(3)(2)(1) = 120$.  Remember, $0!=1$ by definition.], $\IP(X=2) = \frac{3!}{2!(3-2)!}(0.5)^{2}(0.5)^{3-2}=3/8$. The above formula is an example of a *probability mass function*. -->


### Benford's law

We often specify the distribution of a random variable directly by providing its pmf.
Certain common distributions have special names.

```{example benford}

Randomly select a county in the U.S. Let $X$ be the leading digit in the county's population.  [For example](https://en.wikipedia.org/wiki/List_of_counties_in_California), if the county's population is 10,040,000 (Los Angeles County) then $X=1$; if 3,170,000 (Orange County) then $X=3$; if 283,000 (SLO County) then $X=2$; if 30,600 (Lassen County) then $X=3$.  The possible values of $X$ are $1, 2, \ldots, 9$.  You might think that $X$ is equally likely  to be any of its possible values.  However, a more appropriate model^[In a wide variety of data sets, the leading digit follows Benford's law.  This [Shiny app](http://shiny.calpoly.sh/BenfordData/) has a few examples.  Benford's law is often used in fraud detection.  In particular, if the leading digits in a series of values follows a distribution other than Benford's law, such as discrete uniform, then there is evidence that the values might have been fudged.  Benford's law has been used recently to test [reliability of reported COVID-19 cases and deaths](https://www.nature.com/articles/d41586-020-01565-5). [Here is a nice explanation](https://towardsdatascience.com/benfords-law-a-simple-explanation-341e17abbe75) of why leading digits might follow Benford's law in data sets that span multiple orders of magnitude] is to assume that  $X$ has pmf
\[
p_X(x) = 
\begin{cases}
\log_{10}(1+\frac{1}{x}), & x = 1, 2, \ldots, 9,\\
0, & \text{otherwise}
\end{cases}
\]
This distribution is known as [Benford's law](https://en.wikipedia.org/wiki/Benford's_law).

```

1. Construct a table specifying the distribution of $X$, and the corresponding spinner.
1. Find $\IP(X \ge 3)$



```{solution benford-sol}
to Example \@ref(exm:benford)
```

1. Table \@ref(tab:benford-table) and the spinner in Figure \@ref(fig:spinner-benford) below specify the distribution of $X$.
1. We can add the corresponding values from the pmf. $\IP(X \ge 3) = 1 - \IP(X <3) = 1 - (0.301 + 0.176) = 0.523$.

(ref:cap-benford-table) Benford's law.

```{r, benford-table, echo = FALSE}
y = 1:9
p = log(1 + 1 / y, base = 10)

knitr::kable(
  data.frame(y, p),
  col.names = c("x", "p(x)"),
  booktabs = TRUE,
  caption = "(ref:cap-benford-table)",
  digits = 3
)

```  


(ref:cap-spinner-benford) Spinner corresponding to Benford's law.

```{r spinner-benford, echo=FALSE, fig.cap="(ref:cap-spinner-benford)"}


x = 1:9
p = log(1 + 1 / x, base = 10)

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  theme_void() +
  # plot the possible values on the outside
  scale_y_continuous(breaks = plotp, labels=xp$x) +
  theme(axis.text.x=element_text(size=14, face="bold")) +
  # plot the probabilities as percents inside
  geom_text(aes(y = plotp,
                label = percent(p)), size=3,
            color=c(rep("black", 9))) +
  ggtitle(paste("Benford's law", sep=""))

spinner

```

### Poisson distributions


Poisson distributions are often used to model random variables that count "relatively rare events".





```{example, homerun-poisson}

Let $X$ be the number of home runs hit (in total by both teams) in a randomly selected Major League Baseball game.  Technically, there is no fixed upper bound on what $X$ can be, so mathematically it is convenient to consider $0, 1, 2, \ldots$ as the possible values of $X$. Assume that the pmf of $X$ is

\[
p_X(x) =
\begin{cases}
e^{-2.3} \frac{2.3^x}{x!}, & x = 0, 1, 2, \ldots\\
0, & \text{otherwise.}
\end{cases}
\]

This is known as the Poisson(2.3) distribution.  

```

1. Verify that $p_X$ is a valid pmf.
1. Compute $\IP(X = 3)$, and interpret the value as a long run relative frequency.
1. Construct a table and spinner corresponding to the distribution of $X$.
1. Find $\IP(X \le 13)$, and interpret the value as a long run relative frequency.  ([The most home runs ever hit in a baseball game is 13](https://www.mlb.com/news/d-backs-and-phillies-set-mlb-home-run-record).)
1. Find and interpret the ratio of $\IP(X = 5)$ to $\IP(X = 3)$.  Does the value $e^{-2.3}$ affect this ratio?
1. Use simulation to find the long run average value of $X$, and interpret this value.
1. Use simulation to find the variance and standard deviation of $X$.

```{solution homerun-poisson-sol}
to Example \@ref(exm:homerun-poisson)
```

```{asis, fold.chunk = TRUE}

1. We need to verify that the probabilities in the pmf sum to 1.
We can construct the corresponding table and just make sure the values sum to 1.
Technically, any value $0, 1, 2, \ldots$ is possible, but the probabilities will get closer and closer to 0 as $x$ gets larger.
So we can cut our table off at some reasonable upper bound for $X$, where the sum of the values in the table is close enough to 1 for practical purposes.
If we wanted to sum over all possible values of $X$, we need to use the [Taylor series expansion of $e^u$.](https://www.mathsisfun.com/algebra/taylor-series.html)
    \[
    \sum_{x=0}^\infty e^{-2.3} \frac{2.3^x}{x!} = e^{-2.3} \sum_{x=0}^\infty \frac{2.3^x}{x!} = e^{-2.3}e^{2.3} = 1   
    \]
The constant $e^{-2.3}\approx0.100$ simply ensures that the probabilities that follow the shape determined by $2.3^x/ x!$ sum to 1.
1.  Just plug $x=3$ into the pmf: $\IP(X=3)=p_X(3)=e^{-2.3}2.3^3/3! = 0.203$.  In the long run, 20.3% of baseball games have 3 home runs. 
1. See the table and Figure \@ref(fig:poisson-hr-pmf-plot) below.  Plug each value of $x$ into the pmf.  For example, $\IP(X = 5) =p_X(5)=e^{-2.3}2.3^5/5! = 0.054$.
1. Just sum the values of the pmf corresponding to the values $x = 0, 1, \ldots, 13$: $\IP(X \le 13) = \sum_{x=0}^{13} p_X(x)=0.9999998$.  There isn't any shorter way to do it. In the long run, almost all MLB games have at most 13 home runs. Even though the pmf assigns nonzero probability to all values 0, 1, 2, $\ldots$, the probability that $X$ takes a value greater than 13 is extremely small.
1. The ratio is
    \[
    \frac{\IP(X=3)}{\IP(X=5)} = \frac{p_X(3)}{p_X(5)} = \frac{e^{-2.3}2.3^3/3!}{e^{-2.3}2.3^5/5!} = \frac{2.3^3/3!}{2.3^5/5!} = 3.78
    \]
    Games with 3 home runs occur about 3.8 times more frequently than games with 5 home runs.  The constant $e^{-2.3}$ does not affect this ratio; see below for further discussion.
1. The simulation results below suggest that the long run average value of $X$ is equal to the parameter 2.3. Over many baseball games there are a total of 2.3 home runs per game on average.
1. The simulation results also suggest that variance of $X$ is equal to 2.3, and the standard deviation of $X$ is equal to $\sqrt{2.3}\approx 1.52$.

```


| $x$ |                         $p(x)$ |    Value |
|-----|-------------------------------:|---------:|
| 0   |     $e^{-2.3}\frac{2.3^0}{0!}$ | 0.100259 |
| 1   |     $e^{-2.3}\frac{2.3^1}{1!}$ | 0.230595 |
| 2   |     $e^{-2.3}\frac{2.3^2}{2!}$ | 0.265185 |
| 3   |     $e^{-2.3}\frac{2.3^3}{3!}$ | 0.203308 |
| 4   |     $e^{-2.3}\frac{2.3^4}{4!}$ | 0.116902 |
| 5   |     $e^{-2.3}\frac{2.3^5}{5!}$ | 0.053775 |
| 6   |     $e^{-2.3}\frac{2.3^6}{6!}$ | 0.020614 |
| 7   |     $e^{-2.3}\frac{2.3^7}{7!}$ | 0.006773 |
| 8   |     $e^{-2.3}\frac{2.3^8}{8!}$ | 0.001947 |
| 9   |     $e^{-2.3}\frac{2.3^9}{9!}$ | 0.000498 |
| 10  | $e^{-2.3}\frac{2.3^{10}}{10!}$ | 0.000114 |
| 11  | $e^{-2.3}\frac{2.3^{11}}{11!}$ | 0.000024 |
| 12  | $e^{-2.3}\frac{2.3^{12}}{12!}$ | 0.000005 |
| 13  | $e^{-2.3}\frac{2.3^{13}}{13!}$ | 0.000001 |
| 14  | $e^{-2.3}\frac{2.3^{14}}{14!}$ | 0.000000 |

Table: Table representing the Poisson(2.3) probability mass function.



(ref:cap-poisson-hr-pmf-plot) Impulse plot representing the Poisson(2.3) probability mass function.


```{r poisson-hr-pmf-plot, echo = FALSE, fig.cap = "(ref:cap-poisson-hr-pmf-plot)"}


y = 0:13
p = dpois(y, 2.3)
ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 0:12) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "p(x)")

```

Figure \@ref(fig:spinner-poisson-hr) displays a spinner corresponding to the Poisson(2.3) distribution.  To simplify the display we have lumped all values $6, 7, \ldots$ into one "6+" category.

(ref:cap-spinner-poisson-hr) Spinner corresponding to the Poisson(2.3) distribution.

```{r spinner-poisson-hr, echo=FALSE, fig.cap="(ref:cap-spinner-poisson-hr)"}


x = c(0:5, "6+")
p = c(dpois(0:5, 2.3), 1-ppois(5, 2.3))

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = plotp, labels=xp$x) +
  theme(axis.text.x=element_text(size=14, face="bold")) +
  # plot the probabilities as percents inside
  geom_text(aes(y = plotp,
                label = percent(p)), size=4,
            color=c(rep("black", 6), rep("white", 1))) +
  ggtitle(paste("Poisson(2.3) distribution", sep=""))

spinner

```


The constant $e^{-2.3}$ doesn't affect the shape of the probability mass function.  Rather, the constant $e^{-2.3}$ is what ensures that the probabilities sum to 1.  We could have written the pmf as

\[
p_X(x) \propto
\begin{cases}
\frac{2.3^x}{x!}, & x = 0, 1, 2, \ldots\\
0, & \text{otherwise.}
\end{cases}
\]

The symbol $\propto$ means "is proportional to".  This specification is enough to determine the shape of the distribution and relative likelihoods.  For example, the above is enough to determine that the probability that $X$ takes the value 3 is 3.78 times greater than the probability that $X$ takes the value 5. Once we have the shape of the distribution, we can "renormalize" by multiplying all values by a constant, in this case $e^{-2.3}$, so that the values sum to 1. We saw a similar idea in Example \@ref(exm:worldseries-proportional).  The constant is whatever it needs to be so that the values sum to 1; what's important is the relative shape.  The "is proportional to" specification defines the shape of the plot; the constant just rescales the values on the probability axis.

Why might we assume this particular Poisson(2.3) distribution for the number of home runs per game?  We'll discuss this point in more detail later.  For now we'll just present Figure \@ref(fig:poisson-hr-data) which displays the actual distribution of home runs over the 2431 games in the 2018 MLB season.  The spikes represent the observed relative frequencies; the connecting dots represent the theoretical Poisson(2.3) pmf.  We can see that the Poisson(2.3) distribution models the data reasonably well.


(ref:cap-poisson-hr-data) Data on home runs per game in the 2018 MLB season, compared with the Poisson(2.3) distribution.

```{r poisson-hr-data, echo=FALSE, fig.cap="(ref:cap-poisson-hr-data)"}

knitr::include_graphics("_graphics/poisson-hr-data.png")

```


A general Poisson distribution is defined by a single parameter $\mu>0$.  (In the home run example, $\mu=2.3$.)  

```{definition, def-poisson}
A discrete random variable $X$ has a **Poisson distribution** with parameter^[The parameter for a Poisson distribution is often denoted $\lambda$.  However, we use $\mu$ to denote the parameter of a Poisson distribution, and reserve $\lambda$  to denote the rate parameter of a *Poisson process* (which has mean $\lambda t$ at time $t$).] $\mu>0$ if its probability mass function $p_X$ satisfies
\begin{align*}
p_X(x) & \propto \frac{\mu^x}{x!}, \;\qquad x=0,1,2,\ldots\\
& = \frac{e^{-\mu}\mu^x}{x!}, \quad x=0,1,2,\ldots
\end{align*}
The function $\mu^x / x!$ defines the shape of the pmf.  The constant $e^{-\mu}$ ensures that the probabilities sum to 1.


If $X$ has a Poisson($\mu$) distribution then
\begin{align*}
\text{Long run average value of $X$} & = \mu\\
\text{Variance of $X$} & = \mu\\
\text{SD of $X$} & = \sqrt{\mu}
\end{align*}

```



There is not one "Poisson distribution" but rather a family of Poisson distributions.
Each Poisson distribution follows the general pattern specified by the Poisson pmf, with the particular shape determined by the parameter $\mu$.
While the possible values of a variable that follows a Poisson distribution are nonnegative integers, the parameter $\mu$ can be any positive value.

```{python, eval = FALSE}
Poisson(1).plot()
Poisson(2.3).plot()
Poisson(2.5).plot()
```


```{python, echo = FALSE}
plt.figure()
Poisson(1).plot()
Poisson(2.3).plot()
Poisson(3.5).plot()
plt.xlim(left = -0.2);
plt.show();
```


We will see more properties and uses of Poisson distributions later.


In Symbulate we can define a random variable with a Poisson(2.3) distribution and simulate values. 


```{python}
X =  RV(Poisson(2.3))

x = X.sim(10000)
x
```

The spikes in the plot below correspond to simulated relative frequencies.  The connecting dots displayed by `Poisson(2.3).plot()` are determined by the theoretical Poisson(2.3) pmf.

```{python, eval = FALSE}
x.plot() # plot the simulated values and their relative frequencies

Poisson(2.3).plot() # plot the theoretical Poisson(2.3) pmf
```

```{python, echo = FALSE}

plt.figure()
x.plot()
Poisson(2.3).plot()
plt.show()

```


The approximate long run average value and variance are both about equal to the parameter 2.3.

```{python}

x.mean(), x.var(), x.sd()
```


The Symbulate `pmf` method can be used to compute the pmf for named distributions.  The following compares the simulated relative frequency of $\{X = 3\}$ to the theoretical probability $p_X(3)$.

```{python}

x.count_eq(3) / x.count(), Poisson(2.3).pmf(3)

```

You can evaluate the pmf at multiple values.

```{python}

xs = list(range(5)) # the values 0, 1, ..., 4

Poisson(2.3).pmf(xs)

```

Below we use the Python package `tabulate` to construct a somewhat nicer table.  Don't get this tabulate confused with `.tabulate()` in Symbulate.

```{python}

xs = list(range(14)) # the values 0, 1, ..., 13

from tabulate import tabulate

print(tabulate({'x': xs,
                'p_X(x)': [Poisson(2.3).pmf(u) for u in xs]},
               headers = 'keys', floatfmt=".6f"))

```

We can obtain $\IP(X \le 13)$ by summing the corresponding probabilities from the pmf. We can also find $\IP(X \le 13)$ by evaluating the `cdf` at 13.  (We will see more about *cumulative distribution functions* (cdfs) soon.)

```{python}

Poisson(2.3).pmf(xs).sum(), Poisson(2.3).cdf(13)

```

### Binomial distributions

In some cases, a pmf of a discrete random variable can be derived from the assumptions about the underyling random phenomenon.

```{example, binomial-capture}
Capture-recapture sampling is a technique often used to estimate the size of a population.  Suppose you want to estimate $N$, the number of [monarch butterflies in Pismo Beach](https://www.parks.ca.gov/?page_id=30273).  (Assume that $N$ is a fixed but unknown number; the population size doesn't change over time.) You first capture a sample of $N_1$ butterflies, selected randomly, and tag them and release them.  At a later date, you then capture a second sample of $n$ butterflies, selected randomly^[We'll compare to the case of sampling without replacement later.] *with replacement*.  Let $X$ be the number of butterflies in the second sample that have tags (because they were also caught in the first sample).  (Assume that the tagging has no effect on behavior, so that selection in the first sample is independent of selection in the second sample.)

In practice, $N$ is unknown and the point of capture-recapture sampling is to estimate $N$.  But let's start with a simpler, but unrealistic, example where there are $N=52$ butterflies, $N_1 = 13$ are tagged and $N_0=52-13 = 39$ are not, and $n=5$ is the size of the second sample.

```

1. Explain why it is reasonable to assume that the results of the five individual selections are independent.
1. Compute and interpret $\IP(X=0)$.
1. Compute the probability that the first butterfly selected is tagged but the others are not.
1. Compute the probability that the last butterfly selected is tagged but the others are not.
1. Compute and interpret $\IP(X=1)$.
1. Compute and interpret $\IP(X=2)$.
1. Find the pmf of $X$.
1. Construct a table, plot, and spinner representing the distribution of $X$.
1. Make an educated guess for the long run average value of $X$.  
1. How do the results depend on $N_1$ and $N_0$?



```{solution binomial-capture-sol}
to Example \@ref(exm:binomial-capture)
```


```{asis, fold.chunk = TRUE}

1. Since the selections are made with replacement, at the time of each selection there are 52 butterflies of which 13 are tagged, regardless of the results of previous selections.  That is, for each selection the conditional probability that a butterfly is tagged is 13/52 regardless of the results of other selections.
1. Let S (for "success") represent that a selected butterfly is tagged, and F (for "failure") represent that it's not tagged.
None of the 5 butterflies are tagged if the selected sequence is FFFFF.
Since the selections are independent
$$
\IP(X = 0) = 
\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right) = \left(\frac{39}{52}\right)^5  = 0.237
$$
Repeating the process in the long run means drawing many samples of size 5: 23.7% of samples of size 5 will have 0 tagged butterflies.
1. Since the selections are independent, the probability of the outcome SFFFF is
$$
\left(\frac{13}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right)\left(\frac{39}{52}\right) = \left(\frac{13}{52}\right)\left(\frac{39}{52}\right)^4  = 0.079
$$
1. The probability of the outcome FFFFS is the same as in the previous part
$$
\left(\frac{13}{52}\right)\left(\frac{39}{52}\right)^4  = 0.079
$$
1. Each of the particular outcomes with 1 tagged butterfly has probability $\left(\frac{13}{52}\right)\left(\frac{39}{52}\right)^4=0.079$.  So we need to count how many outcomes result in 1 tagged butterfly. Since there are 5 "spots" where the 1 tagged butterfly can be, there are $\binom{5}{1}=5$ such sequences (SFFFF, FSFFF, FFSFF, FFFSF, FFFFS)
$$
\IP(X = 1) = \binom{5}{1}\left(\frac{13}{52}\right)\left(\frac{39}{52}\right)^4  = 5(0.079) = 0.3955
$$
39.6% of samples of size 5 will have exactly 1 tagged butterfly.
1. Each of the particular outcomes with 2 tagged butterflies (like SSFFF) has probability $\left(\frac{13}{52}\right)^2\left(\frac{39}{52}\right)^3=0.02637$.  Since there are 5 "spots" where the 2 tagged butterflies can be, there are $\binom{5}{2}=10$ such sequences.  
$$
\IP(X = 2) = \binom{5}{2}\left(\frac{13}{52}\right)^2\left(\frac{39}{52}\right)^3  = 10(0.02637) = 0.2637
$$
26.4% of samples of size 5 will have exactly 2 tagged butterflies.
1. Similar to the previous part. Any particular outcome with $x$ successes has $5-x$ failures.
The probability of any particular outcomes with exactly $x$ successes (and $5-x$ failures) is $(13/52)^x(39/52)^{5-x}$.
There are $\binom{5}{x}$ outcomes that result in exactly $x$ successes, so the total probability of $x$ successes is
$$
p_X(x) = \binom{5}{x}\left(\frac{13}{52}\right)^x\left(\frac{39}{52}\right)^{5-x}, \qquad x = 0, 1, 2, 3, 4, 5
$$
1. See below. Plug each possible value into the pmf from the previous part.
1. If 1/4 of the butterflies in the population are tagged, we would also expect 1/4 of the butterflies in a randomly selected sample to be tagged.
We would expect the long run average value to be $5(13/52) = 1.25$. 
1. The results only depend on $N_1$ and $N_0$ through the ratio $13/52 = N_1/(N_0+N_1)$.  That is, when the selections are made with replacement, only the population proportion is needed.
If we had only been told that 25% of the butterflies were tagged, instead of 13 out of 52, we still could have solved the problem and nothing would change.

```


The distribution of $X$ in the previous problem is called the Binomial(5, 0.25) distribution.
The parameter 5 is the size of the sample, and the parameter 0.25 is the proportion of successes in the population.



| $x$ |                             $p(x)$ |    Value |
|-----|-----------------------------------:|---------:|
| 0   | $\binom{5}{0}0.25^0(1-0.25)^{5-0}$ | 0.237305 |
| 1   | $\binom{5}{1}0.25^1(1-0.25)^{5-1}$ | 0.395508 |
| 2   | $\binom{5}{2}0.25^2(1-0.25)^{5-2}$ | 0.263672 |
| 3   | $\binom{5}{3}0.25^3(1-0.25)^{5-3}$ | 0.087891 |
| 4   | $\binom{5}{4}0.25^4(1-0.25)^{5-4}$ | 0.014648 |
| 5   | $\binom{5}{5}0.25^5(1-0.25)^{5-5}$ | 0.000977 |

Table: Table representing the Binomial(5, 0.25) probability mass function.



(ref:cap-binomial-butterfly-pmf-plot) Impulse plot representing the Binomial(5, 0.25) probability mass function.


```{r binomial-butterfly-pmf-plot, echo = FALSE, fig.cap = "(ref:cap-binomial-butterfly-pmf-plot)"}


y = 0:5
p = dbinom(y, 5, 0.25)
ggplot(data.frame(y, p),
       aes(x = y,
           xend = y,
           y = 0,
           yend = p)) +
  geom_segment(size = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 0:5) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "p(x)")

```

Figure \@ref(fig:spinner-binomial-butterfly) displays a spinner corresponding to the Binomial(5, 0.25) distribution.  To simplify the display we have lumped 4 and 5 into one "4+" category.


(ref:cap-spinner-binomial-butterfly) Spinner corresponding to the Binomial(5, 0.25) distribution.


```{r spinner-binomial-butterfly, echo=FALSE, fig.cap="(ref:cap-spinner-binomial-butterfly)"}


x = c(0:3, "4+")
p = c(dbinom(0:3, 5, 0.25), 1-pbinom(3, 5, 0.25))

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = plotp, labels=xp$x) +
  theme(axis.text.x=element_text(size=14, face="bold")) +
  # plot the probabilities as percents inside
  geom_text(aes(y = plotp,
                label = percent(p)), size=4,
            color=c(rep("black", 4), rep("white", 1))) +
  ggtitle(paste("Binomial(5, 0.25) distribution", sep=""))

spinner

```




```{definition, binomial}

A discrete random variable $X$ has a **Binomial distribution** with parameters
$n$, a nonnegative integer, and $p\in[0, 1]$ if its probability mass function is
\begin{align*}
p_{X}(x) & = \binom{n}{x} p^x (1-p)^{n-x}, & x=0, 1, 2, \ldots, n
\end{align*}
If $X$ has a Binomial($n$, $p$) distribution then
\begin{align*}
\text{Long run average value of $X$} & = np\\
\text{Variance of $X$} & = np(1-p)\\
\text{SD of $X$} & = \sqrt{np(1-p)}
\end{align*}
```

Imagine a box containing tickets with $p$ representing the proportion of tickets in the box labeled 1 ("success"); the rest are labeled 0 ("failure").  Randomly select $n$ tickets from the box *with replacement* and let $X$ be the number of tickets in the sample that are labeled 1. Then $X$ has a Binomial($n$, $p$) distribution.  Since the tickets are labeled 1 and 0, the random variable $X$ which counts the number of successes is equal to the sum of the 1/0 values on the tickets. If the selections are made with replacement, the draws are independent, so it is enough to just specify the population proportion $p$ without knowing the population size $N$.



The situation in the previous paragraph and the butterfly example involves a sequence of **Bernoulli trials**.

- There are only two possible outcomes, "success" (1) and "failure" (0), on each trial.
- The unconditional/marginal probability of success is the same on every trial, and equal to $p$
- The trials are independent.

If $X$ counts the number of successes in a *fixed number*, $n$, of Bernoulli($p$) trials then $X$ has a Binomial($n, p$) distribution.  
Careful: Don't confuse the number $p$, the probability of success on any single trial, with the probability mass function $p_X(\cdot)$ which takes as an input a number $x$ and returns as an output the probability of $x$ successes in $n$ Bernoulli($p$) trials, $p_X(x)=\IP(X=x)$. 




(ref:cap-binom-plots-n) Probability mass functions for Binomial($n$, 0.4) distributions for $n = 5, 10, 15, 20$.


```{python, echo = FALSE, fig-binom-plots-n, fig.cap = "(ref:cap-binom-plots-n)"}

p = 0.4

ns = [5, 10, 15, 20]

plt.figure()

for n in ns:
  Binomial(n, p).plot()

plt.legend(ns, title = "n")

plt.show()

```



(ref:cap-binom-plots-p) Probability mass functions for Binomial(10, $p$) distributions for $p = 0.1, 0.3, 0.5, 0.7, 0.9$.


```{python, echo = FALSE, fig-binom-plots-p, fig.cap = "(ref:cap-binom-plots-p)"}

n = 10

ps = [0.1, 0.3, 0.5, 0.7, 0.9]

plt.figure()

for p in ps:
  Binomial(n, p).plot()

plt.legend(ps, title = "p", loc = "upper center");
plt.xlim(-0.2, n + 0.2);

plt.show()
```

We will study more properties and uses of Binomial distributions later.


Now we simulate a sample of size 5 with replacement from a box model with 52 tickets, 12 labeled 1 (success) and 39 labeled 0 (failure).
With 1/0 representing S/F, we can also obtain the number of successes with `X = RV(P, sum)`.


```{python, sym-cards-binomial}
P = BoxModel({1: 13, 0: 39}, size = 5, replace = True)

X = RV(P, count_eq(1))

x = X.sim(10000)

x
```


```{python, eval = FALSE}
x.plot() # plot the simulated values and their relative frequencies

Binomial(n = 5, p = 13 / 52).plot() # plot the theoretical Binomial(5, 0.25) pmf

```

```{python, echo = FALSE}
plt.figure()
x.plot()
Binomial(n = 5, p = 13 / 52).plot()
plt.show()

```

Compare the approximate $\IP(X = 2)$ to the theoretical value.

```{python}

x.count_eq(2) / 10000, Binomial(n = 5, p = 13 / 52).pmf(2)

```


Compare the simulated average value of $X$ to the theoretical long run average value.

```{python}

x.mean(), Binomial(n = 5, p = 13 / 52).mean()

```

Compare the simulated average value of $X$ to the theoretical long run average value.


```{python}

x.var(), Binomial(n = 5, p = 13 / 52).var()

```




## Continuous random variables: Probability density functions {#pdf}







The continuous analog of a probability mass function (pmf) is a *probability density function (pdf)*.  However, while pmfs and pdfs play analogous roles, they are different in one fundamental way; namely, a pmf outputs probabilities directly, while a pdf does not.  We have seen that a pmf of a discrete random variable can be summed to find probabilities of related events.  We will see now that a pdf of a continuous random variable must be *integrated* to find probabilities of related events.

In Section \@ref(sec-linear-rescaling) we introduced histograms to summarize simulated values of a continuous random variable. In a histogram the variable axis is chopped into intervals of equal width, and the other axis is on the density scale, so that the *area* of each bar represents the relative frequency of values that lie in the interval.



We have seen examples like the Normal(30, 10) distribution where the shape of the histogram can be approximated by a smooth curve.  This curve represents an idealized model of what the histogram would look like if infinitely many values were simulated and the histogram bins were infinitesimally small.

Suppose we are interested in the random variable $X = - \log(1 - U)$ where $U$ has a Uniform(0, 1) distribution. (We will see why we might be interested in this particular transformation soon.)
Here is a histogram of 10000 simulated values of $X$.

```{python}
U = RV(Uniform(0, 1))

X = -log(1 - U)

x = X.sim(10000)

```


```{python, eval = FALSE}
x.plot()
```


```{python, echo = FALSE}
plt.figure()
x.plot()
plt.show()
```


Imagine that we

- keep simulating more and more values, and
- make the histogram bin widths smaller and smaller.

Then the "chunky" histogram would get "smoother". The following plot summarizes the results of 100,000 simulated values of $X$
in a histogram with 1000 bins, each of width on the order of 0.01. The command `Exponential(1).plot()` overlays the smooth curve modeling the theoretical shape of the distribution of $X$ (called the “Exponential(1)” distribution).  This curve is an example of a pdf.


```{python, eval = FALSE}
X.sim(100000).plot(bins=1000) # histogram of simulated values

Exponential(1).plot() # overlays the smooth curve

```


(ref:cap-exponential-smooth-histogram) A histogram of simulated values of $X = -\log(1-U)$, where $U$ has a Uniform(0, 1) distribution. With many simulated values and very fine bins, the shape of the histogram is well approximated by a smooth curve, called the "Exponential(1) density".



```{python exponential-smooth-histogram, echo = FALSE, fig.cap="(ref:cap-exponential-smooth-histogram)"}

plt.figure()
X.sim(100000).plot(bins=1000)
Exponential(1).plot() # overlays the smooth curve
plt.show()

```


A pdf represents "relative likelihood" as a function of possible values of the random variable.  Just as area represents relative frequency in a histogram, area under a pdf represents probability.

```{definition pdf}

The **probability density function (pdf)** (a.k.a.\ density) of a *continuous* RV $X$, defined on a probability space with probability measure $\IP$, is a function $f_X:\mathbb{R}\mapsto[0,\infty)$ which satisfies
\begin{align*}
\IP(a \le X \le b) & =\int_a^b f_X(x) dx, \qquad \text{for all } -\infty \le a \le b \le \infty
\end{align*}

```

For a continuous random variable $X$ with pdf $f_X$, the probability that $X$ takes a value in the interval $[a, b]$ is the *area under the pdf over the region* $[a,b]$.


A pdf assigns zero probability to intervals where the density is 0. A pdf is usually defined for all real values, but is often nonzero only for some subset of values, the possible values of the random variable.  We often write the pdf as
\[
f_X(x) =
\begin{cases}
\text{some function of $x$}, & \text{possible values of $x$}\\
0, & \text{otherwise.}
\end{cases}
\]
The "0 otherwise" part is often omitted, but be sure to specify the range of values where $f$ is positive.

The axioms of probability imply that a valid pdf must satisfy
\begin{align*}
f_X(x) & \ge 0 \qquad \text{for all } x,\\
\int_{-\infty}^\infty f_X(x) dx & = 1
\end{align*}


The total area under the pdf must be 1 to represent 100\% probability. Given a specific pdf, the generic bounds $(-\infty, \infty)$ in the above integral should be replaced by the range of possible values, that is, those values for which $f_X(x)>0$.



```{example exponential-pdf}

Let $X$ be a random variable with the "Exponential(1)" distribution, illustrated by the smooth curve in Figure \@ref(fig:exponential-smooth-histogram).  Then the pdf of $f_X$ is

\[
f_X(x) =
\begin{cases}
e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]

```


1. Verify that $f_X$ is a valid pdf.
1. Find $\IP(X\le 1)$.
1. Find $\IP(X\le 2)$.
1. Find $\IP(1 \le X< 2.5)$.
1. Compute the 25th percentile of $X$.
1. Compute the 50th percentile of $X$.
1. Compute the 75th percentile of $X$.
1. Start to construct a spinner representing the Exponential(1) distribution.

 
```{solution exponential-pdf-sol}

to Example \@ref(exm:exponential-pdf)

```

```{asis, fold.chunk = TRUE}


1. We need to check that the pdf integrates to 1: $\int_0^\infty e^{-x}dx = 1$.
1. $\IP(X\le 1) = \int_0^1 e^{-x}dx = 1-e^{-1}\approx 0.632$.  See the spinner in Figure \@ref(fig:exponential1-spinner); 63.2% of the area corresponds to $[0, 1]$. 
1. $\IP(X\le 2) = \int_0^2 e^{-x}dx = 1-e^{-2}\approx 0.865$.  See the spinner in Figure \@ref(fig:exponential1-spinner); 86.5% of the area corresponds to $[0, 2]$.
1. $\IP(1 \le X< 2.5) = \int_1^{2.5} e^{-x}dx = e^{-1}-e^{-2.5}\approx 0.286$.  See the illustration below.
1. We want to find $x>0$ such that $\IP(X \le x) =0.25$:
$$
0.25 = \IP(X \le x) = \int_0^{x} e^{-u}du = 1 - e^{-x}
$$
Solve to get $x = -\log(1-0.25) = 0.288$. 25% of values of $X$ are at most 0.288.
The value 0.288 goes at 3 o'clock on the spinner axis.
1. We want to find $x>0$ such that $\IP(X \le x) =0.5$:
$$
0.5 = \IP(X \le x) = \int_0^{x} e^{-u}du = 1 - e^{-x}
$$
Solve to get $x = -\log(1-0.5) = 0.693$. 50% of values of $X$ are at most 0.693.
The value 0.693 goes at 6 o'clock on the spinner axis.
1. We want to find $x>0$ such that $\IP(X \le x) =0.75$:
$$
0.75 = \IP(X \le x) = \int_0^{x} e^{-u}du = 1 - e^{-x}
$$
Solve to get $x = -\log(1-0.75) = 1.386$. 75% of values of $X$ are at most 1.386.
The value 1.386 goes at 9 o'clock on the spinner axis.
1. See the spinner in Figure \@ref(fig:exponential1-spinner). It's the same spinner on both sides, with different values highlighted.
Notice that intervals near 0 are stretched out, while intervals away from 0 are shrunk.

```


The shaded area under the curve below represents $\IP(1<X<2.5)$.

(ref:cap-exponential-pdf-area) Illustration of $\IP(1<X<2.5)$ for $X$ with an Exponential(1) distribution.

```{r exponential-pdf-area, echo=FALSE, fig.cap="(ref:cap-exponential-pdf-area)"}

x<-seq(0, 5, 0.001)
y<-dexp(x,1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>1 & x<2.5),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, 1), expand=c(0, 0)) +
  theme_classic() +
  xlab(expression(x)) +
  ylab(expression(f[X](x))) #+
#  ggtitle(paste("P(1<X<2.5) for X ~ Exponential(1)"))

```





(ref:cap-exponential1-spinner) An Exponential(1) spinner. The same spinner is displayed on both sides, with different features highlighted on the left and right. Only selected rounded values are displayed, but in the idealized model the spinner is infinitely precise so that any real number greater than 0 is possible. Notice that the values on the axis are *not* evenly spaced.

```{r exponential1-spinner, echo=FALSE, fig.show="hold", out.width="50%", fig.cap="(ref:cap-exponential1-spinner)"}

n = 16

xp <- data.frame(
  x = (0:(n-1))/n,
  p = rep(1/n, n)
  )

cdf = c(0, cumsum(xp$p))

plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner1 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white") + 
  coord_polar("y", start=0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = xp$x, minor_breaks = (0:99)/100, labels=c("...|0", round(qexp(xp$x[-1], rate = 1), 3))) +
  theme(axis.text.x=element_text(size=12, face="bold")) +
      annotate(geom="segment", y=(0:99)/100, yend = (0:99)/100,
             x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
  ggtitle(paste("Exponential(1) distribution", sep=""))

spinner1


x = c(0.5, 1, 1.5, 2, 2.5, "...")
p = c(pexp(0.5), pexp(1) - pexp(0.5), pexp(1.5) - pexp(1), pexp(2) - pexp(1.5), pexp(2.5) - pexp(2), 1 - pexp(2.5))

xp <- data.frame(x, p)

cdf = c(0, cumsum(xp$p))



plotp = (cdf[-1] + cdf[-length(cdf)]) / 2
  
spinner2 <- ggplot(xp, aes(x="", y=p, fill=x))+
  geom_bar(width = 1, stat = "identity", color="black", fill="white", linetype=1) + 
  # coord_polar("y", start=0) +
  coord_curvedpolar("y", start = 0) +
  blank_theme +
  # plot the possible values on the outside
  scale_y_continuous(breaks = cdf[-c(length(cdf))], labels=c("... | 0", 0.5, 1.0, 1.5, 2.0, 2.5)) +
  # theme(axis.text.x=element_text(angle=c(90-180/50*(0:49), -90-180/50*(50:99)), size=8)) +
  theme(axis.text.x=element_text(angle = 0, size=12, face = "bold")) +
  annotate(geom="segment", y=(0:19)/20+0.000, yend = (0:19)/20+0.000,
           x=1.48, xend= 1.52) +
  # plot the probabilities as percents inside
    geom_text(aes(y = plotp,
                label = percent(p)), size=4, color=c(rep("black",5), rep("black",1))) +
  ggtitle(paste("Exponential(1) distribution", sep=""))

spinner2

```




### Uniform distributions

In general, a pdf $f_X(x)$ depends on the value $x$ of the random variable $X$.
Uniform distributions are a special case where the pdf is *constant* for all possible values.

```{example meeting-pdf1}
In the meeting problem assume that Regina's arrival time $X$ (minutes after noon) follows a Uniform(0, 60) distribution.

```

1. Sketch a plot of the pdf of $X$.
1. Donny Dont says that the pdf is $f_X(x) = 1/60$.  Do you agree?  If not, specify the pdf of $X$.
1. Use the pdf to find the probability that Regina arrives before 12:15.
1. Use the pdf to find the probability that Regina arrives after 12:45.
1. Use the pdf to find the probability that Regina arrives between 12:15 and 12:45.
1. Use the pdf to find the probability that Regina arrives between 12:15:00 and 12:16:00.
1. Use the pdf to find the probability that Regina arrives between 12:15:00 and 12:15:01.
1. Use the pdf to find the probability that Regina arrives at the exact time 12:15:00 (with infinite precision).


```{solution meeting-pdf1-sol}

to Example \@ref(exm:meeting-pdf1)

```

```{asis, fold.chunk = TRUE}
Note: in a situation like this, you should use properties of Uniform distributions, sketch pictures, and use geometry to solve problems. That is, you should *not* immediately resort to calculus.
However, we show the calculus below to illustrate ideas, and because we have tackled this problem previously with Uniform probability measures and geometry in Example \@ref(exm:meeting-probspace1d).


1. We expect the height of the pdf to be constant between 0 and 60, both because her arrival time is uniform over the interval so no one value should be more likely than another, and because when we simulated values the histogram bars had roughly constant height.  See the plot below.
1. We need the area under the curve over the interval $[0, 60]$ to be 1, representing 100% probability.  If the height of the density is $c$, a constant, then the area under the curve is the area of a rectangle with base 60 (length of the interval $[0, 60]$) and height $c$.  So $c$ needs to be 1/60 for the total area to be 1.  
    However, Donny Don't hasn't specified the possible values.  It's possible that someone who sees Donny's expression would think that $f_X(100)=1/60$.  But the pdf is only 1/60 over the range $[0, 60]$; it is 0 outside of this range.  A more precise expression is
    \[
      f_X(x) =
        \begin{cases}
      1/60, & 0\le x \le 60,\\
      0, & \text{otherwise.}
      \end{cases}
    \]
    You don't necessarily always need to write "0 otherwise", but do always provide the possible values.
1. Integrate the pdf over the range $[0, 15]$.  Since the pdf has constant height, areas under the curve just correspond to areas of rectangles.
    \[
    \IP(X \le 15) = \int_0^{15} (1/60) dx = (1/60)x\Big|_{x = 0}^{x = 15} = \frac{15}{60} = 0.25
    \]
1. Integrate the pdf over the range $[45, 60]$.  
    \[
    \IP(X \ge 45) = \int_{45}^{60} (1/60) dx = (1/60)x\Big|_{x = 45}^{x = 60} = \frac{15}{60} = 1-0.75 = 0.25
    \]
1. We could use the previous parts, but we'll intergrate the pdf over the range $[15, 45]$.
    \[
    \IP(15 \le X \le 45) = \int_{15}^{45} (1/60) dx = (1/60)x\Big|_{x = 15}^{x = 45} = \frac{30}{60} = 0.75 -0.25 = 0.5
    \].
1. Integrate the pdf over the range $[15, 15 + 1]$.  
    \[
    \IP(15 \le X \le 15 + 1) = (1/60)(1) = 1/60
    \]
1. Integrate the pdf over the range $[15, 15 + 1/60]$.  
    \[
    \IP(15 \le X \le 15 + 1/60) = (1/60)(1/60) = 1/3600
    \]
1. $\IP(X = 15) = 0$. Integrate the pdf over the range $[15, 15]$.  The region under the curve at this single point corresponds to a line segment which has 0 area. 
    \[
    \IP(X = 15) = \int_{15}^{15} (1/60) dx = 0
    \]

```



(ref:cap-uniform-pdf-plot) The pdf of a Uniform(0, 60) distribution.  The blue line represents the pdf.  The shaded orange region represents the probability of the interval [15, 45].


```{r uniform-pdf-plot, echo = FALSE, fig.cap="(ref:cap-uniform-pdf-plot)"}
ggplot(data.frame(x = c(0, 1)), aes(x)) +
  stat_function(fun = dunif, color = "skyblue", size = 2) + 
  stat_function(fun = dunif, 
                xlim = c(0.25, 0.75),
                geom = "area",
                fill = "orange", alpha = 0.2) +
  scale_x_continuous(breaks = seq(0, 1, 0.25), labels = seq(0, 1, 0.25) * 60) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1),
                     breaks = seq(0, 1, 0.25), labels = round(seq(0, 1, 0.25) / 60, 4)) +
  labs(x = "x",
       y = "Density") +
  theme_classic()
```






```{example uniform-prob-zero}

Suppose that SAT Math scores follow a Uniform(200, 800) distribution.  Let $U$ be the Math score for a randomly selected student.

```

1. Identify $f_U$, the pdf of $U$.
1. Donny Dont says that the probability that $U$ is 500 is 1/600.  Do you agree?  If not, explain why not.
1. While modeling SAT Math score as a continuous random variable might be mathematically convenient, it's not entirely practical.  Suppose that the range of values $[495, 505)$ corresponds to students who actually score 500.  Find $\IP(495 \le X < 505)$.

```{solution uniform-density-prob-zero-sol}
to Example \@ref(exm:uniform-prob-zero)
```

```{asis, fold.chunk = TRUE}

1. The density still has constant height.  But now the height has to be 1/600 so that the total area under the pdf over the range of possible values $[200, 800]$ is 1.  So $f_U(u) = \frac{1}{600}, 200<u<800$ (and $f_U(u)=0$ otherwise).
1. It is true that $f_U(500)=1/600$.  However, $f_U(500)$ is NOT $\IP(U=500)$. The density (height) at a particular point is not the probability of anything.  Probabilities are determined by *integrating* the density. The "area" under the curve for the region $[500,500]$ is just a line segment, which has area 0, so $\IP(U=500)=0$.  Integrating, $\int_{500}^{500}(1/600)du=0$.  More on this point below.
1. $\IP(495 \le U < 505)=(505-495)(1/600) = 1/60$.  The integral $\int_{495}^{505}(1/600)du$ corresponds to the area of a rectangle with base $505-495$ and height 1/600, so the area is 1/60.

```


```{definition, def-uniform-pdf}
A continuous random variable $X$ has a **Uniform distribution** with parameters $a$ and $b$, with $a<b$, if its probability density function $f_X$ satisfies
\begin{align*}
f_X(x) & \propto \text{constant}, \quad & & a<x<b\\
& = \frac{1}{b-a}, \quad & &  a<x<b.
\end{align*}
If $X$ has a Uniform($a$, $b$) distribution then
\begin{align*}
\text{Long run average value of $X$} & = \frac{a+b}{2}\\
\text{Variance of $X$} & = \frac{|b-a|^2}{12}\\
\text{SD of $X$} & = \frac{|b-a|}{\sqrt{12}}
\end{align*}

```


The long run average value of a random variable that has a Uniform($a$, $b$) distribution is the midpoint of the range of possible values.
The degree of variability of a Uniform distribution is determined by the length of the interval.
Why is the standard deviation equal to the length of the interval multiplied by $0.289 \approx 1/\sqrt{12}$?
Since the deviations from the mean (midpoint) range uniformly from 0 to half the length of the interval, we might expect the average deviation to be about 0.25 times the length of the interval.
The factor $0.289 \approx 1/\sqrt{12}$, which results from the process of taking the square root of the average of the squared deviations, is not too far from 0.25.

The "standard" Uniform distribution is the Uniform(0, 1) distribution represented by the spinner in Figure \@ref(fig:uniform-spinner).
If $U$ has a Uniform(0, 1) distribution then $X = a + (b-a)U$ has a Uniform($a$, $b$) distribution.
Notice that the linear rescaling of $U$ to $a + (b-a)U$ does not change the basic shape of the distribution, just the region of possible values.
Therefore, we can construct a spinner for any Uniform distribution by starting with the Uniform(0, 1) and linearly rescaling the values on the spinner axis.


### Density is not probability

Plugging a value into the pdf of a continuous random variable does *not* provide a probability.  The pdf itself does not provide probabilities directly; instead a pdf must be integrated to find probabilities.

**The probability that a continuous random variable $X$ equals any particular value is 0.** That is, if $X$ is continuous then $\IP(X=x)=0$ for all $x$.  Therefore, for a *continuous* random variable^[The same is *not* true for discrete random variables.  For example, if $X$ is the number of heads in three flips of a fair coin then $\IP(X<1)= \IP(X=0)=1/8$ but $\IP(X \le 1)=\IP(X=0)+\IP(X=1) = 4/8$.], $\IP(X\le x) = \IP(X<x)$, etc.  A continuous random variable can take uncountably many distinct values. Simulating values of a continuous random variable corresponds to an idealized spinner with an infinitely precise needle which can land on any value in a continuous scale.

In the Uniform(0, 1) case, $0.500000000\ldots$ is different than $0.50000000010\ldots$ is different than $0.500000000000001\ldots$, etc.  Consider the spinner in Figure \@ref(fig:uniform-spinner). The spinner in the picture is only labeled in 100 increments of 0.01 each; when we spin, the probability that the needle lands closest to the 0.5 tick mark is 0.01.  But if the spinner were labeled in increments 1000 increments of 0.001, the probability of landing closest to the 0.5 tick mark is 0.001.  And with four decimal places of precision, the probability is 0.0001. And so on.  The more precise we mark the axis, the smaller the probability the spinner lands closest to the 0.5 tick mark.  The Uniform(0, 1) density represents what happens in the limit as the spinner becomes infinitely precise.  The probability of landing closest to the 0.5 tick mark gets smaller and smaller, eventually becoming 0 in the limit.




A density is an idealized mathematical model.  In practical applications, there is some acceptable degree of precision, and events like "$X$, rounded to 4 decimal places, equals 0.5" correspond to intervals that do have positive probability.  For continuous random variables, it doesn't really make sense to talk about the probability that the random value is *equal to* a particular value.  However, we can consider the probability that a random variable is *close to* a particular value.



```{example meeting-nonuniform-pdf1}
Continuing Example \@ref(exm:meeting-pdf1), we will now we assume Regina's arrival time in $[0, 1]$ has pdf

\[
f_X(x) =
        \begin{cases}
      cx, & 0\le x \le 1,\\
      0, & \text{otherwise.}
      \end{cases}
\]

where $c$ is an appropriate constant.

Note that now we're measuring arrival time in hours (i.e., fraction of the hour after noon) instead of minutes.

```

1.  Sketch a plot of the pdf.  What does this say about Regina's arival time?
1. Find the value of $c$ and specify the pdf of $X$.
1. Find the probability that Regina arrives before 12:15. 
1. Find the probability that Regina arrives after 12:45.  How does this compare to the previous part? What does that say about Regina's arrival time?
1. Find the probability that Regina arrives between 12:15 and 12:45.
1. Find the probability that Regina arrives between 12:15 and 12:16.
1. Find the probability that Regina arrives between 12:15:00 and 12:15:01.
1. Find the probability that Regina arrives at the exact time 12:15:00 (with infinite precision).
1. Find the probability that Regina arrives between 12:59:00 and 1:00:00. How does this compare to the probability for 12:15:00 to 12:16:00? What does that say about Regina's arrival time?
1. Find the probability that Regina arrives between 12:59:59 and 1:00:00. How does this compare to the probability for 12:15:00 to 12:15:01? What does that say about Regina's arrival time?
1. Find the probability that Regina arrives at the exact time 1:00:00 (with infinite precision).
1. Compare this example and your answers to Example \@ref(exm:meeting-nonuniform-probspace1).



```{solution meeting-nonuniform-pdf1-sol}

to Example \@ref(exm:meeting-nonuniform-pdf1)

```

```{asis, fold.chunk = TRUE}

You should always start problems like this by drawing the pdf and shading regions corresponding to probabilities.
In some cases, areas can be determine by simple geometry without needing calculus.
We have included the integrals below for completeness, but make sure you also see how the probabilities can be determined using geometry.

1.  The density increases linearly with $x$.
Regina is most likely to arrive closer to 1, and least likely to arrive close to noon (0).
1. $c=2$. The total area under the pdf must be 1. The region under the pdf is a triangle with area $(1/2)(1-0)(c)$, so $c$ must be 2 for the area to be 1.  Via integration
    \[
    1 = \int_0^1 cx dx = (c/2)x^2  \Big|_{x=0}^{x=1} = c / 2
    \]
1. Integrate the pdf over the region $[0, 0.25]$.  Since the pdf is linear, regions under the curve are triangles or trapezoids. The area corresponding to [0, 0.25] is a triangle with base (0.25 - 0), height $c(0.25) = 2(0.25)$, and area $0.5(0.25-0)(2(0.25)) = 0.25^2 = 0.0625$.
    \[
    \int_0^{0.25} 2x dx = x^2 \Bigg|_{x=0}^{x=0.25} = 0.25^2 = (1/2)(0.25 - 0)(2(0.25)) = 0.0625
    \]
1. Integrate the pdf over the region $[0.75, 1]$.
    \[
    \int_{0.75}^1 2x dx = x^2 \Bigg|_{x=0.75}^{x=1} = 1 - 0.75^2 = 0.4375
    \]
    So Regina is 7 times more likely to arrive within 15 minutes of 1 than within 15 minutes of noon.
1. 0.5. We could integrate the pdf from 0.25 to 0.75, or just use the previous results and properties of probabilities.
1. Similar to the previous parts, the probability  is $(0.25 + 1/60)^2 - 0.25^2 = 0.0086$.
(This probability is less than what it was in the uniform case.)
1. Similar to the previous part, $(0.25 + 1/3600)^2 - 0.25^2 = 0.00014$. (This probability is less than what it was in the uniform case.)
1. The exact time 12:15:00 represents a single point the sample space, an interval of length 0. The probability that Regina arrives at the exact time 12:15:00 (with infinite precision) is 0.
1. Similar to previous parts, $1^2 - (1-1/60)^2 = 0.0331$. Notice that this one minute interval around 1:00 has a probability that is about 3.85 times larger than a one minute interval around 12:15.
1. $1^2 - (1-1/3600)^2 = 0.00056$. Notice that this one second interval around 1:00 has a probability that is about 4 times higher than a one second interval around 12:15, though both probabilities are small.
1. The exact time 1:00:00 represents a single point the sample space, an interval of length 0.  The probability that Regina arrives at the exact time 1:00:00 (with infinite precision) is 0.
1. The results are the same as those in Example \@ref(exm:meeting-nonuniform-probspace1).
In that example, the probability that Regina arrives in the interval $[0, x]$ was $x^2$, which can be obtained by integrating the pdf in this example from 0 to $x$.
Careful when you integrate; since $x$ is in the bounds of the integral you need a different dummy variable to use in $f_X$.
\[
  \IP(X \le x) = \int_0^x f_X(u) du = \int_0^x 2u du = u^2 \Bigg|_{u=x}^{u=0} = x^2
\]
We will see soon that such a function is called a *cumulative distribution function (cdf)*.
```


(ref:cap-meeting-nonuniform-pdf) The probability density function from Example \@ref(exm:meeting-nonuniform-pdf1).


```{python, meeting-nonuniform-pdf-plot, echo=FALSE, fig.cap="(ref:cap-meeting-nonuniform-pdf)"}

Beta(2, 1).plot()
plt.show()

```



In the previous example, we specified the general shape of the pdf, then found the constant that made the total area under the curve equal to 1.
In general, a pdf is often defined only up to some multiplicative constant $c$, for example $f_X(x) = c\times(\text{some function of }x)$, or $f_X(x) \propto \text{some function of }x$.  The constant $c$ does not affect the shape of the distribution as a function of $x$, only the scale on the density axis.  The absolute scaling on the density axis is somewhat irrelevant; it is whatever it needs to be to provide the proper *area*.  In particular, the total area under the pdf must be 1.  The scaling constant is determined by the requirement that $\int_{-\infty}^\infty f_X(x) dx = 1$.  

What's more important about the pdf is *relative* heights.  In the previous example the density at 1, $f_X(1) = c$, was 4 times greater than than density at 0.25, $f_X(0.25) = 0.25c$.  This was the reason why the probability of arriving close to 1 was about 4 times greater than the probability of arriving close to 12:15 (time 0.25).  The ratio of the densities at these two points could be computed without knowing the value of $c$.

Compare the pdf in Example \@ref(fig:meeting-nonuniform-pdf-plot) with the probabilities under the non-uniform measure in Figure \@ref(fig:arrival-time-probmeasure).
The pdf at a particular possible value $x$ is related to the probability that the random value takes a value close to that value $x$. 


```{example exponential-pdf-closeto}

Let $X$ be a random variable with the "Exponential(1)" distribution, illustrated by the smooth curve in Figure \@ref(fig:exponential-smooth-histogram).  Then the pdf of $f_X$ is

\[
f_X(x) =
\begin{cases}
e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]

```



1. Compute $\IP(X = 1)$.
1. Without integrating, approximate the probability that $X$ rounded to two decimal places is 1.
1. Without integrating, approximate the probability that $X$ rounded to two decimal places is 1.7.
1. Find and interpret the ratio of the probabilities from the two previous parts.  How could we have obtained this ratio from the pdf?
 
```{solution exponential-pdf-closeto-sol}

to Example \@ref(exm:exponential-pdf-closeto)

```

```{asis, fold.chunk = TRUE}


1. $\IP(X = 1)=0$, since $X$ is continuous.
1. Over a short region around 1, the area under the curve can be approximated by the area of a rectangle with height $f_X(1)$:
    \[
      \IP(0.995<X<1.005)\approx f_X(1)(1.005 - 0.995)=e^{-1}(0.01)\approx 0.00367879.
      \]
    See the illustration below. This provides a pretty good approximation of the true integral^[Reporting so many decimal places is unnecessary, and provides a false sense of precision.  All of these idealized mathematical models are at best approximately true in practice.  However, we provide the extra decimal places here to compare the approximation with the "exact" calculation.] $\int_{0.995}^{1.005} e^{-x}dx = e^{-0.995}-e^{-1.005}\approx 0.00367881$.
1. Over a short region around 1.7, the area under the curve can be approximated by the area of a rectangle with height $f_X(1.7)$:
    \[
    \IP(1.695<X<1.705)\approx f_X(1.7)(1.705 - 1.695)=e^{-1.7}(0.01)\approx 0.001826835.
    \]
    This provides a pretty good approximation of the integral  $\int_{1.695}^{1.705} e^{-x}dx = e^{-1.695}-e^{-1.705}\approx 0.001826843$.
1. Compare the rectangle-based approximations 
    \[
    \frac{\IP(1 - 0.005 <X < 1 + 0.005)}{\IP(1.7 - 0.005 <X < 1.7 + 0.005)} \approx 2.01 \approx \frac{e^{-1}(0.01)}{e^{-1.7}(0.01)} = \frac{e^{-1}}{e^{-1.7}} = \frac{f_X(1)}{f_X(1.7)}  
    \]
    The probability that $X$ is "close to" 1 is about 2 times greater than the probability that $X$ is "close to" 1.7.  This ratio is determined by the ratio of the densities at 1 and 1.7.

```


(ref:cap-exponential-pdf-area2) Illustration of $\IP(0.995<X<1.005)$ (orange) and $\IP(1.695<X<1.705)$ (blue) for $X$ with an Exponential(1) distribution. The plot illustrates how the probability that $X$ is "close to" $x$ can be approximated by the area of a rectangle with height equal to the density at $x$, $f_X(x)$. The density height at $x = 1$ is roughly twice as large than the density height at $x = 1.7$ so the probability that $X$ is "close to" 1 is (roughly) twice as large as the probability that $X$ is "close to" 1.7.

```{r exponential-pdf-area2, echo=FALSE, fig.cap="(ref:cap-exponential-pdf-area2)"}

x<-seq(0, 5, 0.001)
y<-dexp(x,1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, 1), expand=c(0, 0)) +
  theme_classic() +
  xlab(expression(x)) +
  ylab(expression(f[X](x))) +
#  ggtitle(paste("P(1<X<2.5) for X ~ Exponential(1)")) +
  geom_rect(data=xddf, mapping=aes(xmin=1-0.1/2, xmax=1+0.1/2,
                                   ymin=0, ymax=dexp(1, 1)),
            color="orange", fill = "orange") +
    geom_rect(data=xddf, mapping=aes(xmin=1.7-0.1/2, xmax=1.7+0.1/2,
                                   ymin=0, ymax=dexp(1.7, 1)),
            color="skyblue", fill = "skyblue") 

```



A density is an idealized mathematical model. In practical applications, there is some acceptable degree of precision, and events like "$X$, rounded to 4 decimal places, equals 0.5" correspond to intervals that do have positive probability. For continuous random variables, it doesn't really make sense to talk about the probability that the random value equals a particular value. However, we can consider the probability that a random variable is close to a particular value.




To emphasize: The density $f_X(x)$ at value $x$ is *not* a probability. Rather, the density $f_X(x)$ at value $x$ is related to the probability that the RV $X$ takes a value "close to $x$" in the following sense^[This is true because an integral can be approximated by a sum of the areas of many rectangles with narrow bases.  Over a small interval of values surrounding $x$, the density shouldn't change that much, so we can estimate the area under the curve by the area of the rectangle with height $f_X(x)$ and base each equal to the length of the small interval of interest.].
\[
\IP\left(x-\frac{\epsilon}{2} \le X \le x+\frac{\epsilon}{2}\right) \approx f_X(x)\epsilon, \qquad \text{for small $\epsilon$}
\]
The quantity $\epsilon$ is a small number that represents the desired degree of precision.  For example, rounding to two decimal places corresponds to $\epsilon=0.01$.

Technically, any particular $x$ occurs with probability 0, so it doesn't really make sense to say that some values are more likely than others. However, a RV $X$ is more likely to take values *close to* those values that have greater density.  As we said previously,  what's important about a pdf is *relative* heights.  For example, if $f_X(x_2)= 2f_X(x_1)$ then $X$ is roughly "twice as likely to be near $x_2$ than to be near $x_1$" in the above sense.
\[
\frac{f_X(x_2)}{f_X(x_1)} = \frac{f_X(x_2)\epsilon}{f_X(x_1)\epsilon} \approx  \frac{\IP\left(x_2-\frac{\epsilon}{2} \le X \le x_2+\frac{\epsilon}{2}\right)}{\IP\left(x_1-\frac{\epsilon}{2} \le X \le x_1+\frac{\epsilon}{2}\right)}
\]


### Exponential distributions



Exponential distributions are often used to model the *waiting times* between events in a random process that occurs continuously over time.


```{example, exponential-quakes}
Suppose that we model the waiting time, measured continuously in hours, from now until the next earthquake (of any magnitude) occurs in southern CA  as a continuous random variable $X$ with pdf
\[
f_X(x) = 2 e^{-2x}, \; x \ge0
\]
This is the pdf of the "Exponential(2)" distribution.
```

1. Sketch the pdf of $X$.  What does this tell you about waiting times?
1. Without doing any integration, approximate the probability that $X$ rounded to the nearest minute is 0.5 hours.
1. Without doing any integration determine how much more likely that $X$ rounded to the nearest minute is to be 0.5 than 1.5.
1. Compute and interpret $\IP(X > 0.25)$.
1. Compute and interpret $\IP(X \le 3)$.
1. Compute and interpret the 25th percentile of $X$.
1. Compute and interpret the 50th percentile of $X$.
1. Compute and interpret the 75th percentile of $X$.
1. How do the values from the three previous parts compare to the percentiles from the Exponential(1) distribution depicted in \@ref(fig:exponential1-spinner)?
Suggest a method for simulating values of $X$ using the Exponential(1) spinner.
1. Use simulation to approximate the long run average value of $X$.
Interpret this value.
At what *rate* do earthquakes tend to occur?
1. Use simulation to approximate the standard deviation of $X$.
What do you notice?


```{solution, exponential-quakes-sol}
to Example \@ref(exm:exponential-quakes)
```

```{asis, fold.chunk = TRUE}

1. See simulation below for plots.  Waiting times near 0 are most likely, and density decreases exponentially as waiting time increases.
1. Remember, the density at $x=0.5$ is not a probability, but it is related to the probability that $X$ takes a value close to $x=0.5$. The approximate probability that $X$ rounded to the nearest minute is 0.5 hours is 
\[
f_X(0.5)(1/60) = 2e^{-2(0.5)}(1/60) =  0.0123 
\]
1. Find the ratio of the densities at 0.5 and 1.5:
\[
\frac{f_X(0.5)}{f_X(1.5)} = \frac{2e^{-2(0.5)}}{2e^{-2(1.5)}} = \frac{e^{-2(0.5)}}{e^{-2(1.5)}} \approx 7.4.
\]
$X$ rounded to the nearest minute is about 7.4 times more likely to be 0.5 than 1.5.
A waiting time close to half an hour is about 7.4 times more likely than a waiting time close to 1.5 hours.
1. $\IP(X > 0.25) = \int_{0.25}^\infty 2e^{-2x}dx = e^{-2(0.25)}=0.606$.
Careful: this is *not* $2e^{-2(0.25)}$.
According to this model, the waiting time between earthquakes is more than 15 minutes for about 60% of earthquakes.
1. $\IP(X \le 3) = \int_0^3 2e^{-2x}dx = 1-e^{-2(3)}=0.9975$.  While any value greater than 0 is possible in principle, the probability that $X$ takes a really large value is small.  About 99.75% of earthquakes happen within 3 hours of the previous earthquake.
1. We want to find $x>0$ such that $\IP(X \le x) =0.25$:
$$
0.25 = \IP(X \le x) = \int_0^{x} 2e^{-2u}du = 1 - e^{-2x}
$$
Solve to get $x = -0.5\log(1-0.25) = 0.5(0.288)=0.144$. For 25% of earthquakes, the next earthquake happens within 0.144 hours (8.6 minutes).
Constructing a spinner, the value $0.5(0.288)=0.144$ goes at 3 o'clock on the spinner axis.
1. We want to find $x>0$ such that $\IP(X \le x) =0.5$:
$$
0.5 = \IP(X \le x) = \int_0^{x} 2e^{-2u}du = 1 - e^{-2x}
$$
Solve to get $x = -0.5\log(1-0.5) = 0.5(0.693)=0.347$. For 50% of earthquakes, the next earthquake happens within 0.347 hours (20.8 minutes).
Constructing a spinner, the value $0.5(0.693)=0.347$ goes at 6 o'clock on the spinner axis.
1. We want to find $x>0$ such that $\IP(X \le x) =0.75$:
$$
0.75 = \IP(X \le x) = \int_0^{x} 2e^{-2u}du = 1 - e^{-2x}
$$
Solve to get $x = -0.5\log(1-0.75) = 0.5(1.386)=0.693$. For 75% of earthquakes, the next earthquake happens within 0.693 hours (41.6 minutes).
Constructing a spinner, the value $0.5(1.386)=0.693$ goes at 9 o'clock on the spinner axis.
1. We could construct a spinner corresponding to an Exponential(2) distribution.
But the previous parts suggest that the values on the axis of the Exponential(2) are the values on the Exponential(1) spinner multiplied by 1/2.
For example, for an Exponential(1) distribution 25% of values are less than 0.288, while for an Exponential(2) distribution 25% of values are less than $0.5(0.288) = 0.144$.
Therefore, to simulate a value from an Exponential(2) distribution we can simulate a value from an Exponential(1) distribution and multiply the result by 1/2.
1. See simulation results below.
The simulated average is about 0.5 hours.
According to this model, the average waiting time between earthquakes is 0.5 hours.
That is, earthquakes tend to occur at rate 2 earthquakes per hour on average; this is what the parameter 2 in the pdf represents.
1. The simulated standard deviation is also about 0.5, the same as the long run average value.


```

Below we simulate values from an Exponential distribution with rate parameter 2 and compare the simulated values to the theoretical results.
In Symbulate `.cdf()` returns $\le$ probabilities; for example `Exponential(2).cdf(3)` returns the probability that a random variable with an Exponential(2) distribution takes a value $\le 3$.
(We will discuss cdfs in more detail in the next section.)


```{python}

X = RV(Exponential(rate = 2))

x = X.sim(10000)

x

```

```{python, eval = FALSE}

x.plot() # plot the simulated values

Exponential(rate = 2).plot() # plot the theoretical Exponential(2) pdf

```


```{python, echo = FALSE}
plt.figure()
x.plot()

Exponential(rate = 2).plot()
plt.show()

```

```{python}

x.count_gt(0.25) / x.count(), 1 - Exponential(rate = 2).cdf(0.25)

```


```{python}

x.count_lt(3) / x.count(), Exponential(rate = 2).cdf(3)

```

```{python}

x.mean(), Exponential(rate = 2).mean()

```

```{python}

x.sd(), Exponential(rate = 2).sd()

```



```{definition, exponential-pdf-def}

A continuous random variable $X$ has an **Exponential distribution** with *rate* parameter^[Exponential distributions are sometimes parametrized directly by their mean $1/\lambda$, instead of the rate parameter $\lambda$.  The mean $1/\lambda$ is sometimes called the scale parameter.] $\lambda>0$ if its pdf is
\[
f_X(x) =
\begin{cases}\lambda e^{-\lambda x}, & x \ge 0,\\
0, & \text{otherwise}
\end{cases}
\]
If $X$ has an Exponential($\lambda$) distribution then
\begin{align*}
\IP(X>x)  & = e^{-\lambda x}, \quad x\ge 0\\
\text{Long run average of $X$} & = \frac{1}{\lambda}\\
\text{Standard deviation of $X$} & = \frac{1}{\lambda}
\end{align*}

```



Exponential distributions are often used to model the *waiting time* in a random process until some event occurs.

- $\lambda$ is the average *rate* at which events occur over time (e.g., 2 per hour)
- $1/\lambda$ is the mean time between events (e.g., 1/2 hour)

An Exponential density has a peak at 0 and then decreases exponentially as $x$ increases.
The function $e^{-\lambda x}$ defines the shape of the density and the rate at which the density decreases.
The constant $\lambda$, which defines the density at $x=0$, simply rescales the vertical axis so that the total area under the pdf is 1.

(ref:exponential-densities-caption) Exponential densities with rate parameter $\lambda$. 

```{python, exponential-densities, echo = FALSE, fig.cap='(ref:exponential-densities-caption)'}
plt.figure()
rates = [0.5, 1, 2]
for rate in rates:
    Exponential(rate).plot()
    
plt.legend(['$\lambda=$' + str(i) for i in rates]);
plt.xlim(0, 8);
plt.show()
```


The "standard" Exponential distribution is the Exponential(1) distribution, with rate parameter 1 and long run average 1.
If $X$ has an Exponential(1) distribution and $\lambda>0$ is a constant then $X/\lambda$ has an Exponential($\lambda$) distribution.
Recall that an Exponential($\lambda$) distribution has long run average value $1/\lambda$.
Values from an Exponential(1) distribution will have long run average value 1, so if we multiply all the values by $1/\lambda$ then the transformed values will have long run average value $1/\lambda$.
Multiplying by the constant $1/\lambda$ does not change the shape of the distribution; it just relabels the values on the variable axis.


```{python}
U = RV(Exponential(1))

X = (1 / 2) * U
```


```{python, eval = FALSE}
X.sim(10000).plot()  # simulated distribution of X

Exponential(rate = 1 / 2).plot() # theoretical distribution

```


```{python, echo = FALSE}
plt.figure()
X.sim(10000).plot()  # simulated distribution of X

Exponential(rate = 2).plot() # theoretical distribution
plt.show()

```


## Cumulative distribution functions



While pmfs and pdfs play analogous roles for discrete and continuous random variables, respectively, they do behave differently; pmfs provide probabilities directly, but pdfs do not.  It is convenient to have one object that describes a distribution in the same way, regardless of the type of variable, and which returns probabilities directly.  This object is called the *cumulative distribution function (cdf)*.  While the definition might seem strange at first, you have probably already had experience with cumulative distribution functions.


```{example height-percentile}

Maggie and Seamus are babies who have just turned one.
At their one-year visits to their pediatrician:

- Maggie is 76cm tall and in the [75th percentile of height for girls](https://www.cdc.gov/growthcharts/data/set1clinical/cj41l018.pdf).
- Seamus is 72cm tall and in the [10th percentile of height for boys](https://www.cdc.gov/growthcharts/data/set1clinical/cj41l017.pdf).

Explain what these percentiles mean.

```

```{solution height-percentile-sol}
to Example \@ref(exm:height-percentile)
```


```{asis, fold.chunk = TRUE}

- 75% of one-year-old girls (in the U.S.) are less than 76cm tall, and 25% are more than 76cm tall. So Maggie is taller than 75% of one-year-old girls.
- 10% of one-year-old boys (in the U.S.) are less than 72cm tall, and 90% are more than 72cm tall. So the wee baby Seamus is taller than 10% of one-year-old boys.

```


<!-- ```{example sat-percentile} -->

<!-- According to [data on students who took the SAT in 2018-2019](https://collegereadiness.collegeboard.org/pdf/understanding-sat-scores.pdf), 1400 was the 94th percentile of SAT scores, while 1000 was the 40th percentile.  How do you interpret these percentiles? -->

<!-- ``` -->



<!-- ```{solution sat-percentile-sol} -->
<!-- to Example \@ref(exm:sat-percentile) -->
<!-- ``` -->


<!-- ```{asis, fold.chunk = TRUE} -->


<!-- Interpretation: 94% of SAT takers scores at or below 1400, and 6% of SAT takers score greater than 1400.  Similarly, 40% of SAT takers scores at or below 1000, and 60% of SAT takers score greater than 1000.  -->

<!-- ``` -->


Roughly, the value $x$ is the $p$th percentile of a distribution of a random variable $X$ if $p$ percent of values of the variable are less than or equal to $x$: $\IP(X\le x) = p$. The *cumulative distribution function (cdf)* of a random variable fills in the blank
for any given $x$: $x$ is the (blank) percentile. That is, for an input $x$, the cdf outputs $\IP(X\le x)$.




```{definition, cdf}

The **cumulative distribution function (cdf)** (of a random variable $X$ defined on a probability space with probability measure $\IP$)
is the function, $F_X: \mathbb{R}\mapsto[0,1]$, defined by
$F_X(x) = \IP(X\le x)$. A cdf is defined for all real numbers $x$
regardless of whether $x$ is a possible value of $X$.

```


```{example sat-percentile2}

According to [data on students who took the SAT in 2018-2019](https://collegereadiness.collegeboard.org/pdf/understanding-sat-scores.pdf), 1400 was the 94th percentile of SAT scores, while 1000 was the 40th percentile.
Let $X$ be the SAT score of a randomly selected
student (from this cohort), and let $F_X$ be the cdf of $X$.
Evaluate the cdf for each of the following.
For the purposes of this exercise, interpret these quantities in terms of actual SAT scores, which take values in 400, 410, 420, $\ldots$, 1590, 1600. 

```

1. $F_X(1400)$
1. $F_X(1405)$
1. $F_X(1000)$
1. $F_X(1003.7)$
1. $F_X(-3.1)$
1. $F_X(390)$
1. $F_X(399.5)$
1. $F_X(1600)$
1. $F_X(1610)$
1. $F_X(2307.4)$
1. $F_X(1400)-F_X(1000)$


```{solution sat-percentile2-sol}
to Example \@ref(exm:sat-percentile2)
```


```{asis, fold.chunk = TRUE}

1. $F_X(1400)=0.94$. We are told that $\IP(X \le 1400) = 0.94$.
1. $F_X(1405) = 0.94$. In terms of reall SAT scores, $\IP(X \le 1405) = \IP(X\le 1400)$.
1. $F_X(1000) = 0.40$. We are told that $\IP(X \le 1000) = 0.40$.
1. $F_X(1003.7) = 0.40$. In terms of reall SAT scores, $\IP(X \le 1003.7) = \IP(X\le 1000)$.
1. $F_X(-3.1)=0$. The smallest possible score is 400.
1. $F_X(390)=0$. The smallest possible score is 400.
1. $F_X(399.5)= 0$. The smallest possible score is 400.
1. $F_X(1600) = 1$. The largest possible score is 1600, so 100% of students score no more than 1600.
1. $F_X(1610) = 1$. The largest possible score is 1600.
1. $F_X(2307.4) = 1$. The largest possible score is 1600.
1. $0.54 = F_X(1400)-F_X(1000)=\IP(X\le 1400) - \IP(X \le 1000) = \IP(1000 < X \le 1400)$.  54% of SAT takers score greater than 1000 but at most 1400.

```


To understand a cdf, imagine a spinner for a particular distribution.  Suppose a "second hand" starts at the smallest possible value ("12:00") and sweeps clockwise around the spinner.  The second hand sweeps out area as it goes; when the second hand is pointing at $x$, the area that it has swept through represents $\IP(X\le x)$.  The cdf records the values of $F_X(x) = \IP(X\le x)$ as the second hand moves along and points to different values of $x$.

While a cdf is defined the same way for both discrete and continuous random variables, it is probably best understood in terms of continuous random variables.  Remember that for a continuous random variable, $\IP(X\le x)$ is the area under the density curve over the interval $(-\infty, x]$ (remember the density might be 0 for some values in this range).  Imagine plotting a density curve and adding a vertical line at $x$; $\IP(X\le x)$ is the area under the curve to left of this line.  The cdf is constructed by moving the vertical line from left to right, from smaller to larger values of $x$, and recording  the area under the curve to the left of the line, $F_X(x) = \IP(X\le x)$, as $x$ varies.







<!-- See Figure \@ref(fig:cdf-illustration) for an illustration. -->

See the figure below for an illustration.
The shaded area in the plot on the left represents $F_X(x)=\IP(X\le x)$, which is about 0.6 in this example. This area is represented by the $(x, F_X(x))$ point in the cdf plot in the middle. The cdf plot in the middle represents the result of recording the area in the plot on the left for all values of $x$. The plot on the right displays the spinner corresponding to the pdf on the left.



<!-- (ref:cap-cdf-picture) Illustration of a pdf (left) and the corresponding cdf (middle). The cdf at $x$ is highlighted in the plots and shaded spinner (right). -->



```{r cdf-illustration, echo=FALSE, fig.show="hold", out.width="33%"}


x0 = qgamma(0.6, shape=4, rate=1)
x<-seq(0, 15, 0.001)
y<-dgamma(x,shape=4, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>0 & x<x0),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0)) +
  theme_classic() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  scale_x_continuous(breaks = x0, labels = expression(x)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(f[X](x)))
#  ggtitle(paste("Shaded area is ", expression(F(x)),", the cdf of X evaluated at x", sep=""))

y<-pgamma(x,shape=4, rate=1)
y0 = pgamma(x0,shape=4, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0.01)) +
  theme_classic() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_x_continuous(breaks = x0, labels = expression(x)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(F[X](x))) +
#  ggtitle(paste("Shaded area is ", expression(F(x)),", the cdf of X evaluated at x", sep="")) +
  geom_segment(x=x0, xend=x0, y=0, yend=y0, size = 2, linetype=2, colour="orange") +
  geom_segment(x=0, xend=x0, y=y0, yend=y0, size = 2, linetype=2, colour="orange")



p = 0.4

df <- data.frame(
  group = c(">x", "<= x"),
  value = c(1-p, p)
  )

cdfpie <- ggplot(df, aes(x="", y=value, fill=group)) +
  geom_bar(width = 1, stat = "identity", color='black') +
    blank_theme +
  scale_y_continuous(breaks=c(1-p, p), labels=c("x", "")) +
  coord_polar("y") +
  scale_fill_manual(values=c("white", "orange")) +
  geom_text(aes(y = value/2 + c(0, cumsum(value)[-length(value)]), 
            label = c("P(X <= x)", "")), size=5) +
  theme(legend.position = "none")

cdfpie



```



```{example exponential-cdf-calcs}

Let $X$ be a random variable with the Exponential(1) distribution.  Then the pdf of $f_X$ is

\[
f_X(x) =
\begin{cases}
e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]

```

1. Find the cdf of $X$, and sketch a plot of it.
1. Evaluate and interpret $F_X(1)$, and draw a picture depicting it.
1. Evaluate and interpret $F_X(2)-F_X(1)$, and draw a picture depicting it.
1. Evaluate and interpret $F_X(2)$, and draw a picture depicting it.
1. Find $\IP(1 < X < 2.5)$ without integrating again.
1. Suppose we had been given the cdf instead of the pdf.  How could we find the pdf?


```{solution exponential-cdf-calcs-sol}
to Example \@ref(exm:exponential-cdf-calcs)
```

```{asis, fold.chunk = TRUE}

1. $F_X(x)=0$ for $x<0$. For $x>0$ we integrate the density^[Here $x$ represents a particular value of interest, so we use a different dummy variable, $u$,  in the integrand.]
\[
F_X(x) = \IP(X \le x) = \int_0^x e^{-u} du = 1 - e^{-x}
\]
So the cdf of $X$ is
\[
F_X(x) =
\begin{cases}
1 - e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]
1. $F_X(1)=\IP(X\le 1) = 1-e^{-1}\approx 0.632$.  This is represented by the area under the Exponential(1) density curve from 0 to 1, 63.2%.
1. This is represented by the area under the Exponential(1) density curve from 1 to 2, 23.3%.
\[
F_X(2)- F_X(1)=\IP(X\le 2) - \IP(X \le 1) =\IP(1<X\le 2)= (1-e^{-2})-(1-e^{-1})=e^{-1}-e^{-2}\approx 0.233
\]
1. $F_X(2)=\IP(X\le 2) = 1-e^{-2}\approx 0.865$.  This is represented by the area under the Exponential(1) density curve from 0 to 1, 63.2%+23.3% = 86.5%.
1.
\[
\IP(1 < X < 2.5) = \IP(X\le 2.5) - \IP(X \le 1) = F_X(2.5) - F_X(1) =  (1-e^{-2.5})-(1-e^{-1})=e^{-1}-e^{-2.5}\approx 0.286
\]
1. Since the cdf is obtained by integrating the pdf, the pdf if obtained by differentiating the cdf.  Differentiate the cdf $F_X(x)=1-e^{-x},\ x>0$ with respect to its argument $x$ to obtain the pdf $f_X(x) = e^{-x},\ x>0$.

```


(ref:cap-exponential-cdf-illustration) Illustration of the pdf (left) and the cdf (right) for the Exponential(1) distribution represented by the spinner in Figure \@ref(fig:exponential1-spinner). The shaded area in the plot on the left represents $F_X(1)=\IP(X\le 1)$, which is $1-e^{-1}\approx0.632$.  This area is represented by the $(1, F_X(1))$ point in the cdf plot on the right, and in the region from 0 to 1 in the spinner in Figure \@ref(fig:exponential1-spinner).

```{r exponential-cdf-illustration, echo=FALSE, fig.cap="(ref:cap-exponential-cdf-illustration)", out.width='50%', fig.show='hold'}
x0 = 1
xmax = 6
x<-seq(0, xmax, 0.001)
y<-dgamma(x,shape=1, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>0 & x<x0),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0)) +
  theme_classic() +
  scale_x_continuous(limits=c(0, xmax), breaks=0:xmax, expand=c(0, 0)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(f[X](x)))


y<-pgamma(x,shape=1, rate=1)
y0 = pgamma(x0,shape=1, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0.01)) +
  theme_classic() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_x_continuous(limits=c(0, xmax), breaks=0:xmax, expand=c(0, 0)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(F[X](x))) +
#  ggtitle(paste("Shaded area is ", expression(F(x)),", the cdf of X evaluated at x", sep="")) +
  geom_segment(x=x0, xend=x0, y=0, yend=y0, size = 2, linetype=2, colour="orange") +
  geom_segment(x=0, xend=x0, y=y0, yend=y0, size = 2, linetype=2, colour="orange")

```

For named distributions, we can evaluate the cdf in Symbulate using the `.cdf()` method.

```{python}

Exponential(1).cdf(1)

```

```{python}

Exponential(1).cdf([-1, 2, 2.5])

```



**For continuous random variables, think of the cdf as a "generic integral".**
Rather than integrating from scratch to find $\IP(X < 1)$, $\IP(X < 2)$, $\IP(1 < X< 2)$, etc, the integral is computed once for a generic $x$ and then evaluated to find probabilities for specific values of $x$, $F_X(1)$, $F_X(2)$, $F_X(2)-F_X(1)$, etc.

We integrate the pdf to find the cdf, and we differentiate the cdf to find the pdf.
If $X$ is a continuous random variable with cdf $F_X$ then its pdf if $f_X = F'_X$.

```{example, meeting-nonuniform-cdf1}
Recall Example \@ref(exm:meeting-nonuniform-probspace1).  Let $X$ be Regina's arrival time.

```

1. Find the cdf of $X$.
1. Find the pdf of $X$.

```{solution meeting-nonuniform-cdf1-sol}

to Example \@ref(exm:meeting-nonuniform-cdf1)

```

```{asis, fold.chunk = TRUE}

1. The cdf is provided by the setup: $F_X(x) = x^2, 0<x<1$.  (And $F_X(x) = 0, x <0$, $F_X(x)=1, x > 1$)
1.  Differentiate the cdf with respect to $x$.  $f_X(x) = F'_X(x) = 2x$, $0<x<1$.  This is the pdf in Example \@ref(exm:meeting-nonuniform-pdf1).

```

For any random variable $X$ with cdf $F_X$
\[
F_X(b) - F_X(a) = \IP(a<X \le b)
\]
Note that whether the inequalities in the above event are strict ($<$) or not ($\le$) matters for discrete random variables, but not for continuous.


```{example binomial-cdf}

Let $X$ be the number of heads in 3 flips of a fair coin.  

```

1. Find the cdf of $X$ and sketch a plot of it.
1. Let $Y$ be the number of tails in 3 flips of a fair coin.  Find the cdf of $Y$.

```{solution binomial-cdf-sol}
to Example \@ref(exm:binomial-cdf)
```

```{asis, fold.chunk = TRUE}

1. See Figure \@ref(fig:binomial-cdf-illustration). $X$ takes values 0, 1, 2, 3, with respective probabilities 1/8, 3/8, 3/8, 1/8.  We sum these probabilities to find the cdf.  For example, $F_X(0) = \IP(X\le 0) = 1/8$, $F_X(1)=\IP(X\le 1) = \IP(X=0) + \IP(X=1) = 1/8+3/8 = 0.5$.  Remember that $x$ is defined for any value of $x$, for example $F_X(1.5) = \IP(X\le 1.5)= \IP(X=0) + \IP(X=1)=0.5$. The cdf is a step function, which is flat for impossible values of $x$ and jumps at possible values $x$ with the jump size at $x$ equal to the value of the pmf at $x$.
\[
F_X(x) =
\begin{cases}
0, & x<0,\\
1/8, & 0\le x<1,\\
4/8, & 1\le x<2,\\
7/8, & 2\le x<3,\\
1, & x\ge 3.
\end{cases}
\]
1. The cdf describes a distribution.  Since $X$ and $Y$ have the same distribution, they will have the same cdf.  The only difference would be labeling; we would call the cdf of $Y$, $F_Y$, and the argument of this function would typically (but not necessarily) be denoted $y$.

```


(ref:cap-binomial-cdf-illustration) Illustration of the pmf (left) and the cdf (right) of $X$, the number of heads in 3 flips of a fair coin.  The possible values of $X$ are 0, 1, 2, 3.  The pmf on the left displays the probabilities of these values, $p_X(x) = \IP(X=x)$. The cdf on the right displays $F_X(x)=\IP(X\le x)$. The cdf is flat between possible values, and jumps at the possible values, with the jumps sizes given by the pmf. (The corresponding distribution is the "Binomial(3, 0.5)" distribution.)




```{r binomial-cdf-illustration, echo=FALSE, fig.cap="(ref:cap-binomial-cdf-illustration)", out.width='50%', fig.show='hold'}

n = 3
p = 0.5
x = 0:3
xmin = -0.5
xmax = 3.5
px = dbinom(x, n, p)
Fx = pbinom(x, n, p)

plot(x, px, type='h', ylim=c(0, 1), xlim=c(xmin, xmax), ylab=expression(p[X](x)), yaxt='n', col="orange", lwd=2)
axis(2, (0:8)/8, cex.axis = 0.7)


plot(x, Fx, type='n', ylim=c(0, 1), xlim=c(xmin, xmax), ylab=expression(F[X](x)), yaxt='n')
axis(2, (0:8)/8, cex.axis = 0.7)
points(x, Fx, pch=19)
segments(x0=c(xmin, x), x1=c(x, xmax), y0=c(0, Fx), y1 = c(0, Fx))
segments(x0=x, x1=x, y0=c(0, Fx[-length(Fx)]), y1=Fx, lty=2, lwd = 2, col="orange") 
```



```{python}

Binomial(3, 0.5).cdf(2)

```

```{python}

Binomial(3, 0.5).cdf([-1, 0, 0.5, 0.99, 1, 1.1, 2.4, 2.9, 3, 3.1, 3.9999, 4, 10])

```


A few properties of cdfs

- A cdf is defined for all values of $x$, regardless if $x$ is a possible value of the RV.
- A cdf is a non-decreasing function^[This follows from the subset rule, since if $x_1\le x_2$ then $\{X\le x_1\}\subseteq\{X\le x_2\}$]: if $x_1 \le x_2$ then $F_X(x_1)\le F_X(x_2)$.
- A cdf approaches 0 as the input approaches $-\infty$: $\lim_{x\to-\infty}F_X(x) = 0$
- A cdf approaches 1 as the input approaches $\infty$: $\lim_{x\to\infty}F_X(x) = 1$
- The cdf of a *discrete* random variable is a step function.
  - The steps occur at the possible values of the random variable.
  - The height of a particular step corresponds to the probability of that value, given by the pmf.
- The cdf of a *continuous* random variable is a continuous function.
  - The cdf of a *continuous* random variable is obtained by integrating the pdf, so
  - The pdf of a *continuous* random variable is obtained by differentiating the cdf
  \[
  F_X' = f_X \qquad \text{if $X$ is continuous}
  \]
- For any random variable $X$ with cdf $F_X$
\[
F_X(b) - F_X(a) = \IP(a<X \le b)
\]
Whether the inequalities in the above event are strict ($<$) or not ($\le$) matters for discrete random variables, but not for continuous.

One advantage to using cdfs is that they are defined the same way ($F_X(x) = \IP(X\le x)$) for both continuous and discrete random variables.  So results stated in terms of cdfs apply for both discrete and continuous random variables.  This is a little more convenient than having two versions of every definition/result/proof: a statement for discrete RVs in terms of pmfs and a separate statement for continuous RVs in terms of pdfs.  The following definition is an example.



```{definition, samedist-def}

Random variables $X$ and $Y$ **have the same distribution** if their cdfs are the same, that is, if $F_X(u) = F_Y(u)$ for all^[Note that $u$ just represents a dummy variable, the argument of the two functions.  While we generally think of $x$ as the argument of $F_X$, that is just a convenient labeling.  Here we are checking for equality of two *functions*, so we need to use the same input for both.  That is, something like "$F_X(x) = F_Y(y)$" makes no sense because $x$ and $y$ represent different inputs.] $u\in\mathbb{R}$.

```

That is, two random variables have the same distribution if all the percentiles are the same.  While we generally think of two discrete random variables having the same distribution if they have the same pmf, and two continuous random variables having the same distribution if they have the same pdf, the above definition provides a consistent criteria for any two random variables to have the same distribution, regardless of type.



```{example mixed-cdf}
Randomly select a car insurance policy and let $X$ be the amount of claims in a year for the policy, measured in thousands of dollars, which could be 0
    if the policy has no claims. Suppose the cdf of $X$ is
$$
F_X(x) = 1-0.06e^{-x / 4.3}, \quad x\ge 0
$$
```


1.  Compute and interpret $\IP(X \le 2)$.
1.  Compute and interpret $\IP(X > 2)$.
1.  Compute $\IP(X \le -0.001)$
1.  Compute $\IP(X \le 0)$.
1.  Compute and interpret $\IP(X = 0)$.
Be careful!
(Hint: see the two previous parts.
Draw the cdf, starting from $x < 0$ and see what happens at $x = 0$.)
1.  Compute and interpret $\IP(X > 0)$.  Be careful!
1.  Compute and interpret $\IP(0 < X \le 2)$.  Be careful!
1.  Is $X$ discrete, continuous, or neither?  Explain.
1.  Compute and interpret $\IP(X > 2 | X > 0)$.
1.  Find $\IP(X > x | X > 0)$ for $x > 0$.
1.  Identify by name the conditional distribution of $X$ given $X>0$, including the values of relevant parameters.
1.  Describe a "two-stage" process for simulating values of $X$.

```{solution mixed-cdf-sol}
to Example \@ref(exm:mixed-cdf)
```

```{asis, fold.chunk = TRUE}

1.  $\IP(X \le 2) = F_X(2) = 1 - 0.06e^{-2/4.3} = 0.962$. About 96.2% of policies have claims no larger than 2 thousand dollars.
1.  $\IP(X > 2) = 1 - \IP(X \le 2) = 1 - F_X(2) = 0.06e^{-2/4.3} = 0.038$. About 3.8% of policies have claims greater than 2 thousand dollars.
1.  $\IP(X \le -0.001) = F_X(-0.001) = 0$
1.  $\IP(X \le 0) = F_X(0) = 1 - 0.06e^{-0/4.3} = 1 - 0.06 = 0.94$.
1.  $F(x) = 0$ for all $x<0$ so $\IP(X < 0) = 0$. But $\IP(X\le 0) = 0.94.$ So $\IP(X = 0) = \IP(X\le 0) - \IP(X < 0) = 0.94 - 0$. About 94% of policies have no claims.
1.  $\IP(X> 0) = 1 - \IP(X \le 0) = 1 - F_X(0) = 0.06e^{-0/4.3} = 0.06.$ About 6% of policies have non-zero claims.
1.  $\IP(0 < X \le 2) = F_X(2) - F_X(0) = (1-0.06e^{-2/4.3}) - (1-0.06e^{-0/4.3}) = 0.962 - 0.94 = 0.022$.  About 2.2% of policies have non-zero claims less than 2 thousand dollars.
1.  $X$ is neither discrete or continuous; it's a mixture of both. We see that $\IP(X=0) > 0$, so $X$ is not continuous. But also any value $X \ge 0$ is possible, so it's not discrete either. Here $X$ is mixed discrete and continuous. 94% of policies have no claims, so there is a spike at 0. But among the policies that do have claims, the amounts follow a continuous distribution (here it's Exponential). If you plot the cdf, there is a jump at $x = 0$ (from 0 to 0.94) but then it is continuous for $x>0$.
1.  Use the definition of conditional probability
$$
\IP(X > 2 | X > 0) = \frac{\IP(X> 2, X>0)}{\IP(X > 0)} = \frac{\IP(X> 2)}{\IP(X > 0)} = \frac{0.06e^{-2 / 4.3}}{0.06} = e^{-2 / 4.3} = 0.628.
$$
About 62.8% *of policies that have non-zero claims* have claims greater than 2 thousand dollars.
1.  Similar to the previous part for a generic $x>0$
$$
\IP(X > x | X > 0) = \frac{\IP(X> x, X>0)}{\IP(X > 0)} = \frac{\IP(X> x)}{\IP(X > 0)} = \frac{0.06e^{-x / 4.3}}{0.06} = e^{-x / 4.3}.
$$
1.  The key is to recognize that $e^{-x/4.3}$ is the one minus  the cdf of the Exponential(1/4.3) distribution. Therefore, the conditional cdf of $X$ given $X>0$ is the cdf of the Exponential(1/4.3) distribution. That is, the conditional distribution of $X$ given $X>0$ is the Exponential(1/4.3) distribution, with rate parameter 1/4.3 and long run average 4.3 thousand dollars.
*Among policies with non-zero claims*, the distribution of claim amounts follows an Exponential(1/4.3) distribution. *Among policies with non-zero claims*, the average claim amount is 4.3 thousand dollars.
1.  First, simulate whether or not the policy has a non-zero claim: construct a spinner than lands on "no claim" with probability 0.94 and "claim" with probability 0.06. If this spinner lands on "no claim" set $X = 0$. Otherwise, simulate a value from an Exponential(1/4.3) distribution --- for example, by simulating a value from the Exponential(1) spinner and multiplying the result by 4.3 --- and let $X$ be the simulated value.
```


The random variable in the previous example is a mixed discrete and continuous random variable.
The cdf has both discrete (jumps) and continuous features.

(ref:cap-mixed-cdf-plot) Illustration of the cdf of $X$ (left) and the conditional cdf of $X$ given $X>0$ (right) in Example \@ref(exm:mixed-cdf).


```{r mixed-cdf-plot, echo=FALSE, fig.cap="(ref:cap-mixed-cdf-plot)", out.width='50%', fig.show='hold'}

x2 = seq(0.01, 20, 0.01)


plot(x2, 1 - 0.06 * exp(- x2 / 4.3), type='l', ylim=c(0, 1), xlim=c(-2, 20), ylab=expression(F[X](x)), yaxt='n', xlab = "x", main = "cdf of X", lwd = 2)
axis(2, (0:8)/8, cex.axis = 0.7)
points(0, 0, cex = 1.5)
points(0, 0.94, pch = 19, cex = 1.5)
segments(x0 = -5, x1= -0.05, y0 = 0, y1 = 0, lwd = 2)

plot(x2, 1 - exp(- x2 / 4.3), type='l', ylim=c(0, 1), xlim=c(-2, 20), ylab="P(X <= x | X > 0)", yaxt='n', xlab = expression(x), main = "Conditional cdf of X given X > 0", lwd = 2)
axis(2, (0:8)/8, cex.axis = 0.7)
segments(x0 = -5, x1= 0, y0 = 0, y1 = 0, lwd = 2)

```

The code below implements the two-stage method for simulating values of $X$ discussed in the example.
$I$ indicates if there is a claim ($I=1$) or not $(I=0)$.
$Y$ is the output of the Exponential(1/4.3) spinner.
Defining $X = IY$ reflects that if $I= 0$ then $X=0$, otherwise $X = Y$.

```{python}
I, Y = RV(BoxModel([0, 1], probs = [0.94, 0.06]) * Exponential(rate = 1 / 4.3))

X = I * Y

x = X.sim(10000)

x
```

The histogram is obscured by the large proportion of zeros.
For mixed discrete and continuous random variables like this, it is often better to summarize the discrete and continuous parts separately.

```{python, eval = FALSE}
x.plot()

```

```{python, echo = FALSE}
plt.figure()
x.plot()
plt.show()

```

```{python}
x.count_eq(0) / x.count()
```


```{python}
x.count_gt(2) / x.count()
```

```{python}
x.mean()
```

Now we condition on policies that have non-zero claims.
Among the policies with non-zero claims, the claim amounts follow an Exponential(1/4.3) distribution.

```{python}
x_given_not0 = (X | (X > 0) ).sim(10000)

x_given_not0
```

```{python, eval = FALSE}
x_given_not0.plot() # plot the simulated values

Exponential(rate = 1 / 4.3).plot() # plot the theoretical pdf
```

```{python, echo = FALSE}
plt.figure()
x_given_not0.plot()
Exponential(rate = 1 / 4.3).plot()
plt.show()

```

```{python}
x_given_not0.count_gt(2) / x_given_not0.count()
```


```{python}
x_given_not0.mean()
```