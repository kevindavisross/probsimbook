
# Distributions {#distchap}

<!-- \newcommand{\IP}{\textrm{P}} -->
<!-- \newcommand{\IQ}{\textrm{Q}} -->
<!-- \newcommand{\E}{\textrm{E}} -->
<!-- \newcommand{\Var}{\textrm{Var}} -->
<!-- \newcommand{\SD}{\textrm{SD}} -->
<!-- \newcommand{\Cov}{\textrm{Cov}} -->
<!-- \newcommand{\Corr}{\textrm{Corr}} -->
<!-- \newcommand{\Xbar}{\bar{X}} -->
<!-- \newcommand{\Ybar}{\bar{X}} -->
<!-- \newcommand{\xbar}{\bar{x}} -->
<!-- \newcommand{\ybar}{\bar{y}} -->
<!-- \newcommand{\ind}{\textrm{I}} -->
<!-- \newcommand{\dd}{\text{DDWDDD}} -->
<!-- \newcommand{\ep}{\epsilon} -->
<!-- \newcommand{\reals}{\mathbb{R}} -->

Random variables can potentially take many different values, usually with some values or intervals of values more likely than others.  We have used simulation to investigate the pattern of variability of random variables.  Plots and summary statistics like the ones encountered in the previous chapters summarize distributions of random variables.

The **(probability) distribution** of a random variable specifies the possible values
of the random variable and a way of determining corresponding probabilities.  We will see several ways to describe distributions, some of which depend on the number and types of the random variables under investigation.

Commonly encountered random variables can be classified as discrete or
continuous (or a mixture of the two^[There is another type of weird random variable which has a "singular" distribution, like the [Cantor distribution](https://en.wikipedia.org/wiki/Cantor_distribution), but we're counting these random variables as not commonly encountered.]).

- A **discrete** random variable can take on only countably many isolated points on a
number line. These are often counting type variables.  Note that "countably many" includes  the case of countably infinite, such as $\{0, 1, 2, \ldots\}$.
- A **continuous** random variable can take any value within some uncountable interval, such as $[0, 1]$, $[0,\infty)$, or $(-\infty, \infty)$. These are
often measurement type variables.




We will see a few ways of specifying a distribution.

- A well labeled *plot*
- A *table* of possible values and corresponding probabilities for discrete random variables.  This could be a two-way table for the joint distribution of two discrete random variables. 
- A *probability mass function* for discrete random variables or a *probability density function* for continuous random variables which maps possible values $x$ --- or $(x, y)$ pairs, etc --- to their respective probability (for discrete) or density (for continuous).
- A *cumulative distribution function*, which provides all the percentiles of the distribution
- By *name, including values of relevant parameters*, e.g., "Exponential(1)", "Normal(500, 100)", "Binomial(5, 0.3)", "BivariateNormal(500, 500, 100, 100, 0.8)". Some probabilistic situations are so common that the corresponding distributions have special names.  Always be sure to specify values of relevant parameters, e.g., "Normal(500, 100) distribution" rather than just "Normal distribution".  Note that different named distributions have different identifying parameters. For example, the parameters 0 and 1 mean something different for the  Uniform(0, 1) distribution than for the Normal(0, 1) distribution.



## Do not confuse a random variable with its distribution {#distribution}



Heed the title.  A random variable measures a numerical quantity which depends on the outcome of a random phenomenon. The distribution of a random variable specifies the long run pattern of variation of values of the random variable over many repetitions of the underlying random phenomenon. The distribution of a random variable can be approximated by simulating an outcome of the random process, observing the value of the random variable for that outcome, repeating this process many times, and summarizing the results.  But a random variable is not its distribution.

A distribution is determined by:

- The underlying probability measure $\IP$, which represents all the assumptions about the random phenomenon.
- The random variable $X$ itself, that is, the function which maps sample space outcomes to numbers.

Changing either the probability measure or the random variable itself can change the distribution of the random variable. For example, consider the sample space of two rolls of a fair four-sided die.  In each of the following scenarios, the random variable $X$ has a different distribution.

1. The die is fair and $X$ is the sum of the two rolls
1. The die is fair and $X$ is the larger of the two rolls
1. The die is weighted to land on 1 with probability 0.1 and 4 with probability 0.4 and $X$ is the sum of the two rolls.

In particular, in (1) $X$ takes the value 2 with probability $1/16$, in (2) $X$ takes the value 2 with probability $3/16$, and in (3) $X$ takes the value 2 with probability 0.01. In (1) and (2), the probability measure is the same (fair die) but the function defining the random variable is different (sum versus max).  In (1) and (3), the function defining the random variable is the same, and the sample space of outcomes is the same, but the probability measure is different.

We often specify the distribution of a random variable directly without explicit mention of the underlying probability space or function defining the random variable.  For example, in Section \@ref(sim-normal) we assumed the SAT Math score of a randomly selected student had a Normal distribution with mean 500 and standard deviation 100. In situations like this, you can think of the probability space as being the distribution of the random variable and the function defining the random variable to be the identity function.  This idea corresponds to the "simulate from the distribution method": construct a spinner corresponding to the distribution of the random variable and spin it to simulate a value of the random variable.


```{example dd-same-distribution}

Donny Dont is thoroughly confused about the distinction between a random variable and its distribution.  Help him understand by by providing a concrete example of *two different random variables* $X$ and $Y$ that have the *same distribution*. Can you think of $X$ and $Y$ for which $\IP(X = Y) = 0$?  How about a discrete example and a continuous example?  (Hint: we have already seen some examples that you could use.)

```



```{solution dd-same-distribution-sol}
to Example \@ref(exm:dd-same-distribution)

```

```{asis, fold.chunk = TRUE}

Flip a fair coin 3 times and let $X$ be the number of heads and $Y$ be the number of tails.  Then $X$ and $Y$ have the same distribution, but they are not the same random variable; they are measuring different things.  In this case $\IP(X = Y)=0$; in an odd number of flips it's not possible to have the same number of heads and tails on any single outcome.  See Example \@ref(exm:coin-HT-same-dist).

In Example \@ref(exm:meeting-probspace2), the distribution of Regina's arrival time is Uniform(0, 1) and the distribution of Cady's arrival time is Uniform(0, 1).  But these are two random variables; one measures Regina's arrival time and one measure Cady's.  If Regina and Cady met every day for a year, then the day-to-day pattern of Regina's arrival times would look like the day-to-day pattern of Cady's arrival times.  But on any given day, their arrival times would not be the same.  (Under the assumptions of Example \@ref(exm:meeting-probspace2), $\IP(R = Y) = 0$.)

In Section \@ref(sec-example-sat-both) we assumed slightly different distributions for SAT Math and Reading scores.  But suppose instead we assume that SAT Math scores follow a Normal(500, 100) and that SAT Reading scores follow a Normal(500, 100) distribution.  Then the distribution of SAT Math scores is the same as the distribution of reading scores --- each corresponding to the spinner in \@ref(fig:sat-normal-spinner) --- but these are two different random variables.  One measures SAT Math score and one measures SAT Reading score; they are not the same thing.

```



A distribution, like a spinner, is a blueprint for simulating values of the random variable.  If two random variables have the same distribution, you could use the same spinner to simulate values of either random variable.  But a distribution is not the random variable itself. (In other words, "the map is not the territory.")




Two random variables can have the same (long run) distribution, even if the values of the two random variables are never equal on any particular repetition (outcome). If $X$ and $Y$ have the same distribution, then the spinner used to simulate $X$ values can also be used to simulate $Y$ values; in the long run the patterns would be the same.

At the other extreme, two random variables $X$ and $Y$ are the same random variable only if for every outcome of the random phenomenon the resulting values of $X$ and $Y$ are the same. That is, $X$ and $Y$ are the same random variable only if they are the same *function*: $X(\omega)=Y(\omega)$ for all $\omega\in\Omega$.  It is possible to have two random variables for which $\IP(X=Y)$ is large, but $X$ and $Y$ have different distributions.



Many commonly encountered distributions have special names. For example, the distribution of $X$,  the number of heads in 3 flips of a fair coin (and also of $Y$, the number of tails) in Example \@ref(exm:coin-HT-same-dist) is called the "Binomial(3, 0.5)" distribution.  If a random variable has a Binomial(3, 0.5) distribution then it takes the possible values 0, 1, 2, 3, with respective probability 1/8, 3/8, 3/8, 1/8. The random variable in each of the following situations has a Binomial(3, 0.5) distribution.

- $X$ is the number of Heads in three flips of a fair coin
- $Y$ is the number of Tails in three flips of a fair coin
- $Z$ is the number of even numbers rolled in three rolls of a fair six-sided die
- $W$ is the number of boys in a random sample of three births (assuming boys and girls are equally likely^[[Which isn't quite true](https://www.npr.org/sections/health-shots/2015/03/30/396384911/why-are-more-baby-boys-born-than-girls).])

Each of these situations involves a different sample space of outcomes (coins, dice, births) with a random variable which counts different things (Heads, Tails, evens, boys). But all the scenarios have some general features in common:

- There are 3 "trials" (3 flips, 3 rolls, 3 babies)
- Each trial can be classified as "success" or not (Heads, Tails, even, boy)
- Each trial is equally likely to result in success or not (fair coin, fair die, assuming boys and girls are equally likely)
- The trials are independent (more on this later)
- The random variable counts the number of successes in the 3 trials

These examples illustrate that knowledge that a random variable has a specific distribution (e.g., Binomial(3, 0.5)) does not necessarily convey any information about the underlying outcomes or random variable (function) being measured.  (We will study Binomial distributions in more detail later.) 

The scenarios involving $W, X, Y, Z$ illustrate that two random variables do not have to be defined on the same sample space in order to determine if they have the same distribution.  This is in contrast to computing quantities like $\IP(X=Y)$: $\{X=Y\}$ is an event which cannot be investigated unless $X$ and $Y$ are defined for the same outcomes.  For example, you could not estimate the probability that a student has the same score on both SAT Math and Reading exams unless you measured pairs of scores for each student in a sample.  However, you could collect SAT Math scores for one set of students to estimate the marginal distribution of Math scores, and collect Reading scores for a separate set of students to estimate the marginal distribution of Reading scores.

A random variable can be defined explicitly as a function on a probability space, or implicitly through its distribution.
The distribution of a random variable is often assumed or specified directly,
without mention of the underlying probabilty space or the function
defining the random variable. For example, a problem might state "let $Y$ have a Binomial(3, 0.5) distribution" or "let $Y$ have an Exponential(1) distribution". But remember, such statements do not necessarily convey any
information about the underlying sample space outcomes or random variable (function) being
measured. In Symbulate the `RV` command can also be used to define a RV implicitly via its distribution. A definition like `X = RV(Binomial(3, 0.5))` effectively defines a
random variable `X` on an unspecified probability space via an
unspecified function.


```{python}

W = RV(Binomial(3, 0.5))

W.sim(10000).plot()
plt.show()

```



```{example dd-same-joint-distribution}

Suppose that $X$, $Y$, and $Z$ all have the same distribution.  Donny Dont says

1. The pair $(X, Y)$ has the same joint distribution as the pair $(X, Z)$.
1. $X+Y$ has the same distribution as $X+Z$.
1. $X+Y$ has the same distribution as $X+X=2X$.

Determine if each of Donny's statements is correct.  If not, explain why not using a simple example.


```



```{solution dd-same-joint-distibution-sol}
to Example \@ref(exm:dd-same-joint-distribution)
```


```{asis, fold.chunk = TRUE}

First of all, Donny's statements wouldn't even make sense unless the random variables were all defined on the same probability space.  For example, if $X$ is SAT Math score and $Y$ is SAT reading score it doesn't makes sense to consider $X+Y$ unless $(X, Y)$ pairs are measured for the same students.  But even assuming the random variables are defined on the same probability space, we can find counterexamples to Donny's statements.

As just one example, flip a fair coin 4 times and let

- $X$ be the number of heads in flips 1 through 3
- $Y$ be the number of tails in flips 1 through 3
- $Z$ be the number of heads in flips 2 through 4.

1. The joint distribution of $(X, Y)$ is not the same as the joint distribution of $(X, Z)$. For example, $(X, Y)$ takes the pair $(3, 3)$ with probability 0, but $(X, Z)$ takes the pair $(3, 3)$ with nonzero probability (1/16).
1. The distribution of $X+Y$ is not the same as the distribution of $X+Z$; $X+Y$ is 3 with probability 1, but the probability that $X+Z$ is 3 is less than 1 (4/16). 
1. The distribution of $X+Y$ is not the same as the distribution of $2X$; $X+Y$ is 3 with probability 1, but $2X$ takes values 0, 2, 4, 6 with nonzero probability. 



```


Remember that a joint distribution is a probability distribution on pairs of values.
Just because $X_1$ and $X_2$ have the same marginal distribution, and $Y_1$ and $Y_2$ have the same marginal distribution, doesn't necessary imply that $(X_1, Y_1)$ and $(X_2, Y_2)$ have the same joint distributions.  In general, information about the marginal distributions alone is not enough to determine information about the joint distribution.  We saw a related two-way table example in Section \@ref(dist-intro).  Just because two two-way tables have the same totals, they don't necessarily have the same interior cells.

The distribution of any random variable obtained via a transformation of multiple random variables will depend on the joint distribution of the random variables involved; for example, the distribution of $X+Y$ depends on the joint distribution of $X$ and $Y$.





```{example uniform-same-dist}

Consider the probability space corresponding to two spins of the Uniform(0, 1) spinner and let $U_1$ be the result of the first spin and $U_2$ the result of the second.  For each of the following pairs of random variables, determine whether or not they have the same distribution as each other.  No calculations necessary; just think conceptually.

```

1. $U_1$ and $U_2$
1. $U_1$ and $1-U_1$
1. $U_1$ and $1+U_1$
1. $U_1$ and $U_1^2$
1. $U_1+U_2$ and $2U_1$
1. $U_1$ and $1-U_2$
1. Is the joint distribution of $(U_1, 1-U_1)$ the same as the joint distribution of $(U_1, 1 - U_2)$?


```{solution uniform-same-dist-sol}
to Example \@ref(exm:uniform-same-dist)
```


```{asis, fold.chunk = TRUE}

1. Yes, each has a Uniform(0, 1) distribution.
1. Yes, each has a Uniform(0, 1) distribution.  For $u\in[0, 1]$, $1-u\in[0, 1]$, so $U_1$ and $1-U_1$ have the same possible values, and as discussed in Section \@ref(sec-linear-rescaling) a linear rescaling does not change the shape of the distribution.
1. No, the two variables do not have the same possible values.  The shapes would be similar though; $1+U_1$ has a Uniform(1, 2) distribution.
1. No, a non-linear rescaling generally changes the shape of the distribution.  For example, $\IP(U_1\le0.49) = 0.49$, but $\IP(U_1^2 \le 0.49) = \IP(U_1 \le 0.7) = 0.7$  Squaring a number in [0, 1] makes the number even smaller, so the distribution of $U_1^2$ places higher density on smaller values than $U_1$ does.
1. No, $U_1+U_2$ has a triangular shaped distribution on (0, 2) with a peak at 1. (The shape is similar to that of the distribution of $X$ in Section \@ref(sim-transform-joint), but the possible values are (0, 2) rather than (2, 8).) But $2U_1$ has a Uniform(0, 2) distribution.  Do not confuse a random variable with its distribution.  Just because $U_1$ and $U_2$ have the same distribution, you cannot replace $U_2$ with $U_1$ in transformations. The random variable $U_1+U_2$ is not the same random variable as $2U_1$; spinning a spinner and adding the spins will not necessarily produce the same value as spinner a spinner once and multiplying the value by 2.
1. Yes, just like $U_1$ and $1-U_2$ have the same distribution.
1. No. The marginal distributions are the same, but the joint distribution of $(U_1, 1-U_1)$ places all density along a line, while the joint density of $(U_1, 1-U_2)$ is distributed over the whole two-dimensional region $[0, 1]\times[0,1]$.

```


Do not confuse a random variable with its distribution. This is probably getting repetitive by now, but we're emphasizing this point for a reason.  Many common mistakes in probability problems involve confusing a random variable with its distribution.  For example, we will soon that if a continuous random variable $X$ has probability density function $f(x)$, then the probability density function of $X^2$ is NOT $f(x^2)$ nor $(f(x))^2$.  Mistakes like these, which are very common, essentially involve confusing a random variable with its distribution.  Understanding the fundamental difference between a random variable and its distribution will help you avoid many common mistakes, especially in problems involving a lot of calculus or mathematical symbols.



## Discrete random variables: Probability mass functions


Discrete random variables take at most countably many possible values (e.g. $0, 1, 2, \ldots$).  They are often, but not always, counting variables (e.g., $X$ is the number of Heads in 10 coin flips).  We have seen in several examples that the distribution of a discrete random variable can be specified via a table listing the possible values of $x$ and the corresponding probability $\IP(X=x)$.  Always be sure to specify the possible values of $X$.





In some cases, the distribution has a "formulaic" shape and  $\IP(X=x)$ can be written explicitly as a function of $x$. For example, let $X$ be the sum of two rolls of a fair four-sided die.  The distribution of $X$ is displayed in Table \@ref(tab:dice-rv-sol-table) and the plot below.  The probabilities of the possible $x$ values follow a clear triangular pattern as a function of $x$.

```{python}

P = BoxModel([1, 2, 3, 4], size = 2)
X = RV(P, sum)

x = X.sim(10000)
x.plot()
x.plot('density')
plt.show()

```

For each possible value $x$ of the random variable $X$, $\IP(X=x)$ can be obtained from the following formula

\[
p(x) =
\begin{cases}
\frac{4-|x-5|}{16}, & x = 2, 3, 4, 5, 6,7, 8,\\
0, & \text{otherwise.}
\end{cases}
\]

That is, $\IP(X = x) = p(x)$ for all $x$. For example, $\IP(X = 2) = 1/16 = p(2)$; $\IP(X=5)=4/16=p(5)$; $\IP(X=7.5)=0=p(7.5)$.  To specify the distribution of $X$ we could provide Table \@ref(tab:dice-rv-sol-table), or we could just provide the function $p(x)$ above.  Notice that part of the specification of $p(x)$ involves the possible values of $x$; $p(x)$ is only nonzero for $x=2,3, \ldots, 8$.  Think of $p(x)$ as a compact way of representing Table \@ref(tab:dice-rv-sol-table).  The function $p(x)$ is the *probability mass function* of the random variable $X$.



```{definition pmf}

The **probability mass function (pmf)** (a.k.a., density (pdf)^[We use "pmf" for discrete distributions and reserve "pdf" for continuous probability density functions.  The terms "pdf" and "density" are sometimes used in both discrete and continuous situations even though the objects the terms represent differ between the two situations (probability versus density).  In particular, in R the `d` commands (`dbinom`, `dnorm`, etc) are used for both discrete and continuous distributions. In Symbulate, you can use `.pmf()` for discrete distributions.]) of a *discrete* RV $X$, defined on a probability space with probability measure $\IP$, is a function $p_X:\mathbb{R}\mapsto[0,1]$ which specifies each possible value of the RV and the probability that the RV takes that particular value: $p_X(x)=\IP(X=x)$ for each possible value of $x$.

```


The axioms of probability imply that a valid pmf must satisfy
\begin{align*}
p_X(x) & \ge 0 \quad \text{for all $x$}\\
p_X(x) & >0 \quad \text{for at most countably many $x$ (the possible values, i.e., support)}\\
\sum_x p_X(x) & = 1
\end{align*}

The countable set of possible values of a discrete random variable $X$, $\{x: \IP(X=x)>0\}$, is called its **support**.

The pmf of a discrete random variable provides the probability of "equal to" events: $\IP(X = x)$.  Probabilities for other general events, e.g., $\IP(X \le x)$ can be obtained by summing the pmf over the range of values of interest.



We have seen that a distribution of a discrete random variable can be represented in a table, with a corresponding spinner.  Think of a pmf as providing a compact formula for constructing the table/spinner.



```{example, pmf-dice-max}

Let $Y$ be the larger of two rolls of a fair four-sided die.  Find the probability mass function of $Y$.

```



```{solution pmf-dice-max-sol}
to Example \@ref(exm:pmf-dice-max)
```





```{asis, fold.chunk = TRUE}

See Table \@ref(tab:dice-max-dist-table) and the plot below. As a function of $y=1, 2, 3, 4$, $\IP(Y=y)$ is linear with slope 2/16 passing through the point (1, 1/16).  The pmf of $Y$ is

\[
p_Y(y) =
\begin{cases}
\frac{2y-1}{16}, & y = 1, 2, 3, 4, \\
0, & \text{otherwise.}
\end{cases}
\]

For any $y$, $\IP(Y=y) = p_Y(y)$.  For example, $\IP(Y=2) = 3/16 = p_Y(2)$ and $\IP(Y = 3.3) = 0 = p_Y(3.3)$.

```



```{python}

P = BoxModel([1, 2, 3, 4], size = 2)
Y = RV(P, max)

y = Y.sim(10000)
y.plot()
y.plot('density')
plt.show()

```

```{example dd-pmf-label}

Donny Dont provides two answers to Example \@ref(exm:pmf-dice-max).  Are his answers correct?  If not, why not?
  
1. $p_Y(y) = \frac{2y-1}{16}$
1. $p_Y(x) = \frac{2x-1}{16},\; x = 1, 2, 3, 4$, and $p_Y(x)= 0$ otherwise.


```


```{solution dd-pmf-label-sol}
to Example \@ref(exm:dd-pmf-label)
```


```{asis, fold.chunk = TRUE}

1. Donny's solution is incomplete; he forgot to specify the possible values. It's possible that someone who sees Donny's expression would think that $p_Y(2.5)=4/16$.  You don't necessarily always need to write "0 otherwise", but do always provide the possible values.
1. Donny's answer is actually correct, though maybe a little confusing.  The important place to put a $Y$ is the subscript of $p$: $p_Y$ identifies this function as the pmf of the random variable $Y$, as opposed to any other random variable that might be of interest.  The argument of $p_Y$ is just a dummy variable that defines the function.  As an analogy, $g(u)=u^2$ is the same function as $g(x)=x^2$; it doesn't matter which symbol defines the argument.  It is convenient to represent the argument of the pmf of $Y$ as $y$, and the argument of the pmf of $X$ as $x$, but this is not necessary.  Donny's answer does provide a way of constructing Table \@ref(tab:dice-max-dist-table). 

```


When there are multiple discrete random variables of interest, we usually identify their pmfs with subscripts: $p_X, p_Y, p_Z$, etc.

<!-- For example, if $X$ is the number of heads in three flips of a fair coin, then $\IP(X=x)$ can be written in terms of the following formula^[This is an example of a Binomial probability mass function.  We will see the details behind this formula in Section \@ref(sec-binomial).]. -->
<!-- \[ -->
<!-- \IP(X =x) = \frac{3!}{x!(3-x)!}(0.5)^{x}(0.5)^{3-x}, \qquad x = 0, 1, 2, 3. -->
<!-- \] -->
<!-- For example^[Recall factorial notation: if $k$ is a positive integer, then $k!=k(k-1)(k-2)\cdots (2)(1)$, e.g., $5! = 5(4)(3)(2)(1) = 120$.  Remember, $0!=1$ by definition.], $\IP(X=2) = \frac{3!}{2!(3-2)!}(0.5)^{2}(0.5)^{3-2}=3/8$. The above formula is an example of a *probability mass function*. -->


### Benford's law

We often specify the distribution of a random variable directly by providing its pmf.
Certain common distributions have special names.

```{example benford}

Randomly select a county in the U.S. Let $X$ be the leading digit in the county's population.  [For example](https://en.wikipedia.org/wiki/List_of_counties_in_California), if the county's population is 10,040,000 (Los Angeles County) then $X=1$; if 3,170,000 (Orange County) then $X=3$; if 283,000 (SLO County) then $X=2$; if 30,600 (Lassen County) then $X=3$.  The possible values of $X$ are $1, 2, \ldots, 9$.  You might think that $X$ is equally likely  to be any of its possible values.  However, a more appropriate model^[In a wide variety of data sets, the leading digit follows Benford's law.  This [Shiny app](http://shiny.calpoly.sh/BenfordData/) has a few examples.  Benford's law is often used in fraud detection.  In particular, if the leading digits in a series of values follows a distribution other than Benford's law, such as discrete uniform, then there is evidence that the values might have been fudged.  Benford's law has been used recently to test [reliability of reported COVID-19 cases and deaths](https://www.nature.com/articles/d41586-020-01565-5).] is to assume that  $X$ has pmf
\[
p_X(x) = 
\begin{cases}
\log_{10}(1+\frac{1}{x}), & x = 1, 2, \ldots, 9,\\
0, & \text{otherwise}
\end{cases}
\]
This distribution is known as [Benford's law](https://en.wikipedia.org/wiki/Benford's_law).

```

1. Construct a table specifying the distribution of $X$, and the corresponding spinner.
1. Find $\IP(X \ge 3)$



```{solution benford-sol}
to Example \@ref(exm:benford)
```

1. Table \@ref(tab:benford-table) and the spinner in Figure \@ref(fig:spinner-benford) below specify the distribution of $X$.
1. We can add the corresponding values from the pmf. $\IP(X \ge 3) = 1 - \IP(X <3) = 1 - (0.301 + 0.176) = 0.523$.

(ref:cap-benford-table) Benford's law.

```{r, benford-table, echo = FALSE}
y = 1:9
p = log(1 + 1 / y, base = 10)

knitr::kable(
  data.frame(y, p),
  col.names = c("x", "p(x)"),
  booktabs = TRUE,
  caption = "(ref:cap-benford-table)",
  digits = 3
)

```  


(ref:cap-spinner-benford) Spinner corresponding to Benford's law.

```{r spinner-benford, echo=FALSE, fig.cap="(ref:cap-spinner-benford)"}

knitr::include_graphics("_graphics/spinner-benford-all.png")

```



### Poisson distributions


Some probability mass functions assign nonzero probability to countably infinitely many values.




```{example, homerun-poisson}

Let $X$ be the number of home runs hit (in total by both teams) in a randomly selected Major League Baseball game.  Technically, there is no fixed upper bound on what $X$ can be, so mathematically it is convenient to consider $0, 1, 2, \ldots$ as the possible values of $X$. Assume that the pmf of $X$ is

\[
p_X(x) =
\begin{cases}
e^{-2.3} \frac{2.3^x}{x!}, & x = 0, 1, 2, \ldots\\
0, & \text{otherwise.}
\end{cases}
\]

This is known as the Poisson(2.3) distribution.  (We will cover Poisson distributions in more detail later.)

```

1. Verify that $p_X$ is a valid pmf.
1. Compute $\IP(X = 3)$, and interpret the value as a long run relative frequency.
1. Construct a table and spinner corresponding to the distribution of $X$.
1. Find $\IP(X \le 13)$, and interpret the value as a long run relative frequency.  ([The most home runs ever hit in a baseball game is 13](https://www.mlb.com/news/d-backs-and-phillies-set-mlb-home-run-record).)
1. Find and interpret the ratio of $\IP(X = 5)$ to $\IP(X = 3)$.  Does the value $e^{-2.3}$ affect this ratio?
1. Use simulation to find the long run average value of $X$, and interpret this value.
1. Use simulation to find the variance and standard deviation of $X$.

```{solution homerun-poisson-sol}
to Example \@ref(exm:homerun-poisson)
```

```{asis, fold.chunk = TRUE}

1. We need to verify that the probabilities in the pmf sum to 1.  Use the [Taylor series expansion of $e^u$.](https://www.mathsisfun.com/algebra/taylor-series.html)
    \[
    \sum_{x=0}^\infty e^{-2.3} \frac{2.3^x}{x!} = e^{-2.3} \sum_{x=0}^\infty \frac{2.3^x}{x!} = e^{-2.3}e^{2.3} = 1   
    \]
1.  Just plug $x=3$ into the pmf: $\IP(X=3)=p_X(3)=e^{-2.3}2.3^3/3! = 0.203$.  In the long run, 20.3% of baseball games have 3 home runs. 
1. See below.  Plug each value of $x$ into the pmf.  For example, $\IP(X = 5) =p_X(5)=e^{-2.3}2.3^5/5! = 0.054$.
1. Just sum the values of the pmf corresponding to the values $x = 0, 1, \ldots, 13$: $\IP(X \le 13) = \sum_{x=0}^{13} p_X(x)=0.9999998$.  There isn't any shorter way to do it. In the long run, almost all MLB games have at most 13 home runs. Even though the pmf assigns nonzero probability to all values 0, 1, 2, $\ldots$, the probability that $X$ takes a value greater than 13 is extremely small.
1. The ratio is
    \[
    \frac{\IP(X=3)}{\IP(X=5)} = \frac{p_X(3)}{p_X(5)} = \frac{e^{-2.3}2.3^3/3!}{e^{-2.3}2.3^5/5!} = \frac{2.3^3/3!}{2.3^5/5!} = 3.78
    \]
    Games with 3 home runs occur about 3.8 times more frequently than games with 5 home runs.  The constant $e^{-2.3}$ does not affect this ratio; see below for further discussion.
1. The simulation results suggest that the long run average value of $X$ is equal to the parameter 2.3. Over many baseball games there are a total of 2.3 home runs per game on average.
1. The simulation results also suggest that variance of $X$ is equal to 2.3, and the standard deviation of $X$ is equal to $\sqrt{2.3}\approx 1.52$.

```


In Symbulate we can define a random variable with a Poisson(2.3) distribution and simulate values. 


```{python}
X =  RV(Poisson(2.3))

x = X.sim(10000)
x
```

The spikes in the plot below correspond to simulated relative frequencies.  The connecting dots displayed by `Poisson(2.3).plot()` are determined by the pmf in Example \@ref(exm:homerun-poisson).

```{python}

x.plot()
Poisson(2.3).plot()
plt.show()

```

The simulated approximate long run average value and variance are both about equal to the parameter 2.3.

```{python}

x.mean(), x.var(), x.sd()
```


The Symbulate `pmf` method can be used to compute the pmf for named distributions.  The following compares the simulated relative frequency of $\{X = 3\}$ to the theoretical probability $p_X(3)$.

```{python}

x.count_eq(3) / x.count(), Poisson(2.3).pmf(3)

```

You can evaluate the pmf at multiple values.

```{python}

xs = list(range(5)) # the values 0, 1, ..., 4

Poisson(2.3).pmf(xs)

```

Below we use the Python package `tabulate` to construct a somewhat nicer table.  Don't get this tabulate confused with `.tabulate()` in Symbulate.

```{python}

xs = list(range(14)) # the values 0, 1, ..., 13

from tabulate import tabulate

print(tabulate({'x': xs,
                'p_X(x)': [Poisson(2.3).pmf(u) for u in xs]},
               headers = 'keys', floatfmt=".6f"))

```

We can obtain $\IP(X \le 13)$ by summing the corresponding probabilities from the pmf. We can also find $\IP(X \le 13)$ by evaluating the `cdf` at 13.  (We will see more about *cumulative distribution functions* (cdfs) soon.)

```{python}

Poisson(2.3).pmf(xs).sum(), Poisson(2.3).cdf(13)

```

Figure \@ref(fig:spinner-poisson-hr) displays a spinner corresponding to the Poisson(2.3) distribution.  To simplify the display we have lumped all values $6, 7, \ldots$ into one "6+" category.

(ref:cap-spinner-poisson-hr) Spinner corresponding to the Poisson(2.3) distribution.

```{r spinner-poisson-hr, echo=FALSE, fig.cap="(ref:cap-spinner-poisson-hr)"}

knitr::include_graphics("_graphics/spinner-poisson-hr.png")

```



The constant $e^{-2.3}$ doesn't affect the shape of the probability mass function.  Rather, the constant $e^{-2.3}$ is what ensures that the probabilities sum to 1.  We could have written the pmf as

\[
p_X(x) \propto
\begin{cases}
\frac{2.3^x}{x!}, & x = 0, 1, 2, \ldots\\
0, & \text{otherwise.}
\end{cases}
\]

The symbol $\propto$ means "is proportional to".  This specification is enough to determine the shape of the distribution and relative likelihoods.  For example, the above is enough to determine that the probability that $X$ takes the value 3 is 3.78 times greater than the probability that $X$ takes the value 5. Once we have the shape of the distribution, we can "renormalize" by multiplying all values by a constant, in this case $e^{-2.3}$, so that the values sum to 1. We saw a similar idea in Example \@ref(exm:worldseries-proportional).  The constant is whatever it needs to be so that the values sum to 1; what's important is the relative shape.  The "is proportional to" specification defines the shape of the plot; the constant just rescales the values on the probability axis.

Why might we assume this particular Poisson(2.3) distribution for the number of home runs per game?  We'll discuss this point in more detail later.  For now we'll just present Figure \@ref(fig:poisson-hr-data) which displays the actual distribution of home runs over the 2431 games in the 2018 MLB season.  The spikes represent the observed relative frequencies; the connecting dots represent the theoretical Poisson(2.3) pmf.  We can see that the Poisson(2.3) distribution models the data reasonably well.


(ref:cap-poisson-hr-data) Data on home runs per game in the 2018 MLB season, compared with the Poisson(2.3) distribution.

```{r poisson-hr-data, echo=FALSE, fig.cap="(ref:cap-poisson-hr-data)"}

knitr::include_graphics("_graphics/poisson-hr-data.png")

```


A general Poisson distribution is defined by a single parameter $\mu>0$.  (In the home run example, $\mu=2.3$.)  

```{definition, def-poisson}
A discrete random variable $X$ has a **Poisson distribution** with parameter^[The parameter for a Poisson distribution is often denoted $\lambda$.  However, we use $\mu$ to denote the parameter of a Poisson distribution, and reserve $\lambda$  to denote the rate parameter of a *Poisson process* (which has mean $\lambda t$ at time $t$).] $\mu>0$ if its probability mass function $p_X$ satisfies
\begin{align*}
p_X(x) & \propto \frac{\mu^x}{x!}, \;\qquad x=0,1,2,\ldots\\
& = \frac{e^{-\mu}\mu^x}{x!}, \quad x=0,1,2,\ldots
\end{align*}
The function $\mu^x / x!$ defines the shape of the pmf.  The constant $e^{-\mu}$ ensures that the probabilities sum to 1.


If $X$ has a Poisson($\mu$) distribution then
\begin{align*}
\text{Long run average value of $X$} & = \mu\\
\text{Variance of $X$} & = \mu\\
\text{SD of $X$} & = \sqrt{\mu}
\end{align*}

```


### Joint probability mass functions

Most interesting problems involve two or more^[We mostly focus on the case of two random variables, but analogous definitions and concepts apply for more than two (though the notation can get a bit messier).] random variables defined on the same probability space.  In these situations, we can consider how the variables vary together, or jointly, and study their relationship. The *joint distribution* of random variables $X$ and $Y$ (defined on the same probability space) is a probability distribution on $(x, y)$ *pairs*.  In this context, the distribution of one of the variables alone is called a *marginal distribution*.

Think of a joint distribution of two discrete RVs as a spinner that returns pairs of values, like the one on the right in Figure \@ref(fig:dice-spinners-sum-max).



```{example mscoin-joint}

Recall Example \@ref(exm:mscoin-rv).  Flip  a *fair* coin 4 times.  If the outcome is TTTT or TTTH discard the repetition and try again. Let:

- $Z$  be the number of flips immediately following H.
- $Y$  be the number of flips immediately following H that result in H.
- $W = Y / Z$  be the proportion of flips immediately following H that result in H.


```

1. Make a two-way table specifying the joint probability mass function of $Y$ and $Z$.
1. Make a table specifying the pmf of $W$.

```{solution mscoin-joint-sol}
to Example \@ref(exm:mscoin-joint)
```

```{asis, fold.chunk = TRUE}

1. We basically just need to reorganize Table \@ref(tab:mscoin). For example, $p_{Y,Z}(1,2) = \IP(Y=1, Z=2) = \IP(\{HHTH, HTHH, HHTT, THHT\})=4/14$.

    | $y, z$ |    1 |    2 |    3 |
    |--------|-----:|-----:|-----:|
    | 0      | 5/14 | 1/14 |    0 |
    | 1      | 1/14 | 4/14 |    0 |
    | 2      |    0 | 1/14 | 1/14 |
    | 3      |    0 |    0 | 1/14 |

1.  For each $(y, z)$ pair, find the value of $y/z$; see Table \@ref(tab:mscoin). 

    | $w$ | $p_W(w)$ |
    |----:|---------:|
    |   0 |     6/14 |
    | 1/2 |     4/14 |
    | 2/3 |     1/14 |
    |   1 |     3/14 |
  
```





```{definition joint-pmf}

The **joint probability mass function (pmf)** of two *discrete* random variables $(X,Y)$ defined on a probability space with probability measure $\IP$ is the function $p_{X,Y}:\mathbb{R}^2\mapsto[0,1]$ defined by
\[
p_{X,Y}(x,y) = \IP(X= x, Y= y) \qquad \text{ for all } x,y
\]

```

The axioms of probability imply that a valid joint pmf must satisfy

\begin{align*}
p_{X,Y}(x,y) & \ge 0 \quad \text{for all $x, y$}\\
p_{X,Y}(x,y) & >0 \quad \text{for at most countably many $(x,y)$ pairs (the possible values, i.e., support)}\\
\sum_x \sum_y p_{X,Y}(x,y) & = 1
\end{align*}

Remember to specify the possible $(x, y)$ pairs when defining a joint pmf.


```{example, dice-joint-pmf}
Let $X$ be the sum and $Y$ the larger of two rolls of a fair four-sided die.  Find the joint pmf of $X$ and $Y$.
```



```{solution dice-joint-sol}
to Example \@ref(exm:dice-joint-pmf)
```

```{asis, fold.chunk = TRUE}

The joint pmf is tabled in Table \@ref(tab:dice-joint-dist-twoway).  We need to find a function that can construct the table.  We basically just need a compact way to express the possible pairs.

\[
p_{X, Y}(x, y)
=
\begin{cases}
2/16, & y = 2, 3, 4, \qquad x = y+1, \ldots, 2y-1\\
1/16, & y = 1, 2, 3, 4, \quad x = 2y\\
0, & \text{otherwise}
\end{cases}
\]

```


```{example, poisson-hr-joint}

Let $X$ be the number of home runs hit by the home team, and $Y$ the number of home runs hit by the away team in a randomly selected MLB game.  Suppose that $X$ and $Y$ have joint pmf

\[
p_{X, Y}(x, y)
=
\begin{cases}
e^{-2.3}\frac{1.2^{x}1.1^{y}}{x!y!}, & x = 0, 1, 2, \ldots; y = 0, 1, 2, \ldots,\\
0, & \text{otherwise.}
\end{cases}
\]

```


1. Find the probability that the home teams hits 2 home runs and the away team hits 1 home run.
1. Construct a two-way table representation of the joint pmf.
1. Find the probability that the home team hits 2 home runs.
1. Find the marginal pmf of $X$.
1. Find the probability that the away team hits 1 home run.
1. Find the marginal pmf of $Y$.
1. Find the probability that both teams combine to hit a total of 3 home runs. 
1. Find the probability that the home team and the away team hit the same number of home runs.




```{solution poisson-hr-joint-sol}
to Example \@ref(exm:poisson-hr-joint)
```

```{asis, fold.chunk = TRUE}

1. Plug $x=2, y=1$ into the joint pmf: $\IP(X = 2, Y = 1) = p_{X, Y}(2, 1) = e^{-2.3}\frac{1.2^2 1.1^1}{2!1!}= 0.0794$.
1. See Table \@ref(tab:poisson-hr-joint-table) below. The possible values of $x$ are 0, 1, 2, $\ldots$, and similarly for $y$, and any $(x, y)$ pair of these values is possible.  Plug each $(x, y)$ pair into $p_{X,Y}$ like in the previous part.
1. Sum over the values of $y$
    \[
    \IP(X = 2) = \sum_{y=0}^\infty p_{X, Y}(2, y) = \sum_{y=0}^\infty   e^{-2.3}\frac{1.2^2 1.1^y}{2!y!} = e^{-2.3}\frac{1.2^2}{2!}\sum_{y=0}^\infty   \frac{1.1^y}{y!} = e^{-2.3}\frac{1.2^2}{2!}\left(e^{1.1}\right) = e^{-1.2}\frac{1.2^2}{2!} = 0.217.
    \]
1. See the row sums in the table below. Repeat the calculation from the previous part but for a generic value of $x=0, 1, 2\ldots$
    \[
    p_X(x) = \sum_{y=0}^\infty p_{X, Y}(x, y) = \sum_{y=0}^\infty   e^{-2.3}\frac{1.2^x 1.1^y}{x!y!} = e^{-2.3}\frac{1.2^x}{x!}\sum_{y=0}^\infty   \frac{1.1^y}{y!} = e^{-2.3}\frac{1.2^x}{x!}\left(e^{1.1}\right) = e^{-1.2}\frac{1.2^x}{x!}.
    \]
    The marginal distribution of $X$ is the Poisson(1.2) distribution. Notice that the marginal pmf of $X$ is a function of values of $X$ alone.
1. Sum over the values of $x$
    \[
    \IP(Y = 1) = \sum_{x=0}^\infty p_{X, Y}(x, 1) = \sum_{x=0}^\infty   e^{-2.3}\frac{1.2^x 1.1^1}{x!1!} = e^{-2.3}\frac{1.1^1}{1!}\sum_{x=0}^\infty   \frac{1.2^x}{x!} = e^{-2.3}\frac{1.1^1}{1!}\left(e^{1.2}\right) = e^{-1.1}\frac{1.1^1}{1!} = 0.366.
    \]
1. See the row sums in the table below. Repeat the calculation from the previous part but for a generic value of $y=0, 1, 2\ldots$
    \[
    \IP(Y = y) = \sum_{x=0}^\infty p_{X, Y}(x, y) = \sum_{x=0}^\infty   e^{-2.3}\frac{1.2^x 1.1^y}{x!y!} = e^{-2.3}\frac{1.1^y}{y!}\sum_{x=0}^\infty   \frac{1.2^x}{x!} = e^{-2.3}\frac{1.1^y}{y!}\left(e^{1.2}\right) = e^{-1.1}\frac{1.1^y}{y!}.
    \]
    The marginal distribution of $Y$ is the Poisson(1.1) distribution. Notice that the marginal pmf of $Y$ is a function of values of $Y$ alone.
1. The corresponding $(x, y)$ pairs are (0, 3), (1, 2), (2, 1), (3, 0).
    \[
     \IP(X + Y = 3) = p_{X, Y}(0, 3) +  p_{X, Y}(1, 2)+p_{X, Y}(2, 1)+p_{X, Y}(3, 0) = 0.022 + 0.073 + 0.079 + 0.029 = 0.203
    \]
    We'll see an easier way to do this later.
1. Sum over values where $x=y$, the diagonal cells of the table.
    \[
    \IP(X = Y) = \sum_{x = 0}^\infty e^{-2.3}\frac{1.2^x 1.1^x}{x!x!} = 0.283.
    \]


```


(ref:cap-poisson-hr-joint-table) Two-way table representing the joint distribution of $X$ and $Y$, in Example \@ref(exm:poisson-hr-joint).

```{r, poisson-hr-joint-table, echo = FALSE}

table2 <- NULL

for (x in 0:6){
  table2 <- rbind(table2, dpois(x, 1.2) * dpois(0:6, 1.1))
}

table2 <- rbind(table2,
                rbind(c(rep(NA, 7)),
                      dpois(0:6, 1.1)))

table2 <- cbind(table2,
                rep(NA, 9),
                c(dpois(0:6, 1.2), NA, 1))

options(knitr.kable.NA = '')


knitr::kable(
  data.frame(c(0:6, "...", "Total"), table2),
  col.names = c("x, y", 0:6, "...", "Total"),
  booktabs = TRUE,
  caption = "(ref:cap-poisson-hr-joint-table)",
  digits = 5
)

```

The code below simulates $(X, Y)$ pairs from the joint distribution in Example \@ref(exm:poisson-hr-joint).
Figure \@ref(fig:poisson-joint-ggplot) summarizes the simulated pairs.
We will discuss the code syntax in more detail later^[It turns out that in this case $X$ and $Y$ are *independent*, so their joint distribution is the the product of their marginal distributions, hence the `*` syntax.].


(ref:cap-poisson-joint-ggplot) Tile plot of $(X, Y)$ pairs simulated from the joint distribution  in Example \@ref(exm:poisson-hr-joint).


```{python, poisson-joint-ggplot, fig.cap="(ref:cap-poisson-joint-ggplot)"}

X, Y = RV(Poisson(1.2) * Poisson(1.1))

(X & Y).sim(100000).plot('tile')
plt.show()

```


```{r, include = FALSE, eval = FALSE, echo = FALSE}

n_reps = 1000000

x = rpois(n_reps, 1.2)
y = rpois(n_reps, 1.1)

df = data.frame(x, y)

count_table = df %>%
  count(x, y, name = "freq") %>%
  mutate(rel_freq = freq / n_reps)


ggplot(count_table, aes(x, y, fill = rel_freq)) +
   geom_tile() +
   scale_fill_viridis_c() +
   guides(fill = guide_legend(title="Relative frequency"))

```


The *marginal pmfs* are determined by the joint pmf by the law of total probability.  If we imagine a plot with blocks whose heights represent the joint probabilities, the marginal probability of a particular value of one variable can be obtained by "stacking" all the blocks corresponding to that value.  In terms of a two-way table, a marginal distribution can be obtained by "collapsing" the table by summing rows or columns.

\begin{align*}
p_X(x) & = \sum_y p_{X,Y}(x,y) & & \text{a function of $x$ only}
\\
p_Y(y) & = \sum_x p_{X,Y}(x,y) & & \text{a function of $y$ only}
\\
\end{align*}




## Continuous random variables: Probability density functions {#pdf}







The continuous analog of a probability mass function (pmf) is a *probability density function (pdf)*.  However, while pmfs and pdfs play analogous roles, they are different in one fundamental way; namely, a pmf outputs probabilities directly, while a pdf does not.  We have seen that a pmf of a discrete random variable can be summed to find probabilities of related events.  We will see now that a pdf of a continuous random variable must be *integrated* to find probabilities of related events.

In Section \@ref(sec-linear-rescaling) we introduced histograms to summarize simulated values of a continuous random variable. In a histogram the variable axis is chopped into intervals of equal width, and the other axis is on the density scale, so that the *area* of each bar represents the relative frequency of values that lie in the interval.



We have seen examples (e,g., Figures \@ref(fig:normal-sat-density), \@ref(fig:log-uniform-density)) where the shape of the histogram can be approximated by a smooth curve.  This curve represents an idealized model of what the histogram would look like if infinitely many values were simulated and the histogram bins were infinitesimally small.

Consider Figure \@ref(fig:log-uniform-density) which summarizes 10000 simulated values of the random variable $X = - \log(1 - U)$ where $U$ has a Uniform(0, 1) distribution.  Imagine that we

- keep simulating more and more values, and
- make the histogram bin widths smaller and smaller.

Then the "chunky" histogram would get "smoother". The following plot summarizes the results of 100,000 simulated values of $X$
in a histogram with 1000 bins, each of width on the order of 0.01. The command `Exponential(1).plot()` overlays the smooth curve modeling the theoretical shape of the distribution of $X$ (called the “Exponential(1)” distribution).  This curve is an example of a pdf.


```{python}

U = RV(Uniform(0, 1))

X = -log(1 - U)

X.sim(100000).plot(bins=1000)
Exponential(1).plot() # overlays the smooth curve
plt.show()

```


A pdf represents "relative likelihood" as a function of possible values of the random variable.  Just as area represents relative frequency in a histogram, area under a pdf represents probability.

```{definition pdf}

The **probability density function (pdf)** (a.k.a.\ density) of a *continuous* RV $X$, defined on a probability space with probability measure $\IP$, is a function $f_X:\mathbb{R}\mapsto[0,\infty)$ which satisfies
\begin{align*}
\IP(a \le X \le b) & =\int_a^b f_X(x) dx, \qquad \text{for all } -\infty \le a \le b \le \infty
\end{align*}

```

For a continuous random variable $X$ with pdf $f_X$, the probability that $X$ takes a value in the interval $[a, b]$ is the *area under the pdf over the region* $[a,b]$.


A pdf assigns zero probability to intervals where the density is 0. A pdf is usually defined for all real values, but is often nonzero only for some subset of values, the possible values of the random variable.  We often write the pdf as
\[
f_X(x) =
\begin{cases}
\text{some function of $x$}, & \text{possible values of $x$}\\
0, & \text{otherwise.}
\end{cases}
\]


The axioms of probability imply that a valid pdf must satisfy
\begin{align*}
f_X(x) & \ge 0 \qquad \text{for all } x,\\
\int_{-\infty}^\infty f_X(x) dx & = 1
\end{align*}


The total area under the pdf must be 1 to represent 100\% probability. Given a specific pdf, the generic bounds $(-\infty, \infty)$ in the above integral should be replaced by the range of possible values, that is, those values for which $f_X(x)>0$.




### Uniform densities

In general, a pdf $f_X(x)$ depends on the value $x$ of the random variable $X$.
Uniform distributions are a special case where the pdf is *constant* for all possible values.

```{example meeting-pdf1}
Recall Example \@ref(exm:meeting-probspace1). Regina and Cady plan to meet for lunch between noon and 1 but they are not sure of their arrival times.  We'll consider only Regina's arrival time for now. (We'll get back to Cady soon.) Assume that Regina arrives at a time chosen uniformly at random between noon and 1.  We can model Regina's arrival with the sample space $[0, 1]$ and a uniform probability measure.  Let $X$ be Regina's arrival time in $[0, 1]$.

```

1. Sketch a plot of the pdf of $X$.
1. Donny Dont says that the pdf is $f_X(x) = 1$.  Do you agree?  If not, specify the pdf of $X$.
1. Use the pdf to find the probability that Regina arrives before 12:15.
1. Use the pdf to find the probability that Regina arrives after 12:45.
1. Use the pdf to find the probability that Regina arrives between 12:15 and 12:45.
1. Use the pdf to find the probability that Regina arrives between 12:15:00 and 12:16:00.
1. Use the pdf to find the probability that Regina arrives between 12:15:00 and 12:15:01.
1. Use the pdf to find the probability that Regina arrives at the exact time 12:15:00 (with infinite precision).


```{solution meeting-pdf1-sol}

to Example \@ref(exm:meeting-pdf1)

```

```{asis, fold.chunk = TRUE}
1. We expect the height of the pdf to be constant between 0 and 1, both because her arrival time is uniform over the interval so no one value should be more likely than another, and because when we simulated values the histogram bars had roughly constant height.  See the plot below.
1. We need the area under the curve over the interval $[0, 1]$ to be 1, representing 100% probability.  If the height of the density is $c$, a constant, then the area under the curve is the area of a rectangle with base 1 (length of the interval $[0, 1]$) and height $c$.  So $c$ needs to be 1 for the total area to be 1.  
    However, Donny Don't hasn't specified the possible values.  It's possible that someone who sees Donny's expression would think that $f_X(2.5)=1$.  But the pdf is only 1 over the range $[0, 1]$; it is 0 outside of this range.  A more precise expression is
    \[
      f_X(x) =
        \begin{cases}
      1, & 0\le x \le 1,\\
      0, & \text{otherwise.}
      \end{cases}
    \]
    You don't necessarily always need to write "0 otherwise", but do always provide the possible values.
1. Integrate the pdf over the range $[0, 0.25]$.  Since the pdf has constant height, areas under the curve just correspond to areas of rectangles.
    \[
    \IP(X \le 0.25) = \int_0^{0.25} 1 dx = x\Big|_{x = 0}^{x = 0.25} = 0.25
    \]
1. Integrate the pdf over the range $[0.75, 1]$.  
    \[
    \IP(X \ge 0.75) = \int_{0.75}^1 1 dx = x\Big|_{x = 0.75}^{x = 1} = 1-0.75 = 0.25
    \]
1. We could use the previous parts, but we'll intergrate the pdf over the range $[0.25, 0.75]$.
    \[
    \IP(0.25 \le X \le 0.75) = \int_{0.25}^{0.75} 1 dx = x\Big|_{x = 0.25}^{x = 0.75} = 0.75 -0.25 = 0.5
    \].
1. Integrate the pdf over the range $[0.25, 0.25 + 1/60]$.  
    \[
    \IP(0.25 \le X \le 0.25 + 1/60) = 1/60
    \]
1. Integrate the pdf over the range $[0.25, 0.25 + 1/3600]$.  
    \[
    \IP(0.25 \le X \le 0.25 + 1/3600) = 1/3600
    \]
1. $\IP(X = 0.25) = 0$. Integrate the pdf over the range $[0.25, 0.25]$.  The region under the curve at this single point corresponds to a line segment which has 0 area. 
    \[
    \IP(X = 0.25) = \int_{0.25}^{0.25} 1 dx = 0
    \]

```



(ref:cap-uniform-pdf-plot) The pdf of a Uniform(0, 1) distribution.  The blue line represents the pdf.  The shaded orange region represents the probability of the interval [0.25, 0.75].

```{python uniform-pdf-plot, fig.cap="(ref:cap-uniform-pdf-plot)"}

Uniform(0, 1).plot()

plt.fill_between([0.25, 0.75], 1, 0, color='orange', alpha=0.2) # shade the region

plt.show()

```





```{example uniform-prob-zero}

Suppose that SAT Math scores follow a Uniform(200, 800) distribution.  Let $U$ be the Math score for a randomly selected student.

```

1. Identify $f_U$, the pdf of $U$.
1. Donny Dont says that the probability that $U$ is 500 is 1/600.  Do you agree?  If not, explain why not.
1. While modeling SAT Math score as a continuous random variable might be mathematically convenient, it's not entirely practical.  Suppose that the range of values $[495, 505)$ corresponds to students who actually score 500.  Find $\IP(495 \le X < 505)$.

```{solution uniform-density-prob-zero-sol}
to Example \@ref(exm:uniform-prob-zero)
```

```{asis, fold.chunk = TRUE}

1. The density still has constant height.  But now the height has to be 1/600 so that the total area under the pdf over the range of possible values $[200, 800]$ is 1.  So $f_U(u) = \frac{1}{600}, 200<u<800$ (and $f_U(u)=0$ otherwise).
1. It is true that $f_U(500)=1/600$.  However, $f_U(500)$ is NOT $\IP(U=500)$. The density (height) at a particular point is not the probability of anything.  Probabilities are determined by *integrating* the density. The "area" under the curve for the region $[500,500]$ is just a line segment, which has area 0, so $\IP(U=500)=0$.  Integrating, $\int_{500}^{500}(1/600)du=0$.  More on this point below.
1. $\IP(495 \le U < 505)=(505-495)(1/600) = 1/60$.  The integral $\int_{495}^{505}(1/600)du$ corresponds to the area of a rectangle with base $505-495$ and height 1/600, so the area is 1/60.

```


```{definition, def-uniform-pdf}
A continuous random variable $X$ has a **Uniform distribution** with parameters $a$ and $b$, with $a<b$, if its probability density function $f_X$ satisfies
\begin{align*}
f_X(x) & \propto \text{constant}, \quad & & a<x<b\\
& = \frac{1}{b-a}, \quad & &  a<x<b.
\end{align*}
If $X$ has a Uniform($a$, $b$) distribution then
\begin{align*}
\text{Long run average value of $X$} & = \frac{a+b}{2}\\
\text{Variance of $X$} & = \frac{|b-a|^2}{12}\\
\text{SD of $X$} & = \frac{|b-a|}{\sqrt{12}}
\end{align*}

```


The long run average value of a random variable that has a Uniform($a$, $b$) distribution is the midpoint of the range of possible values.
The degree of variability of a Uniform distribution is determined by the length of the interval.
Why is the standard deviation equal to the length of the interval multiplied by $0.289 \approx 1/\sqrt{12}$?
Since the deviations from the mean (midpoint) range uniformly from 0 to half the length of the interval, we might expect the average deviation to be about 0.25 times the length of the interval.
The factor $0.289 \approx 1/\sqrt{12}$, which results from the process of taking the square root of the average of the squared deviations, is not too far from 0.25.

### Density is not probability

Plugging a value into the pdf of a continuous random variable does *not* provide a probability.  The pdf itself does not provide probabilities directly; instead a pdf must be integrated to find probabilities.

**The probability that a continuous random variable $X$ equals any particular value is 0.** That is, if $X$ is continuous then $\IP(X=x)=0$ for all $x$.  Therefore, for a *continuous* random variable^[The same is *not* true for discrete random variables.  For example, if $X$ is the number of heads in three flips of a fair coin then $\IP(X<1)= \IP(X=0)=1/8$ but $\IP(X \le 1)=\IP(X=0)+\IP(X=1) = 4/8$.], $\IP(X\le x) = \IP(X<x)$, etc.  A continuous random variable can take uncountably many distinct values. Simulating values of a continuous random variable corresponds to an idealized spinner with an infinitely precise needle which can land on any value in a continuous scale.

In the Uniform(0, 1) case, $0.500000000\ldots$ is different than $0.50000000010\ldots$ is different than $0.500000000000001\ldots$, etc.  Consider the spinner in Figure \@ref(fig:uniform-spinner). The spinner in the picture is only labeled in 100 increments of 0.01 each; when we spin, the probability that the needle lands closest to the 0.5 tick mark is 0.01.  But if the spinner were labeled in increments 1000 increments of 0.001, the probability of landing closest to the 0.5 tick mark is 0.001.  And with four decimal places of precision, the probability is 0.0001. And so on.  The more precise we mark the axis, the smaller the probability the spinner lands closest to the 0.5 tick mark.  The Uniform(0, 1) density represents what happens in the limit as the spinner becomes infinitely precise.  The probability of landing closest to the 0.5 tick mark gets smaller and smaller, eventually becoming 0 in the limit.




A density is an idealized mathematical model.  In practical applications, there is some acceptable degree of precision, and events like "$X$, rounded to 4 decimal places, equals 0.5" correspond to intervals that do have positive probability.  For continuous random variables, it doesn't really make sense to talk about the probability that the random value is *equal to* a particular value.  However, we can consider the probability that a random variable is *close to* a particular value.



```{example meeting-nonuniform-pdf1}
Continuing Example \@ref(exm:meeting-pdf1), we will now we assume Regina's arrival time in $[0, 1]$ has pdf

\[
f_X(x) =
        \begin{cases}
      cx, & 0\le x \le 1,\\
      0, & \text{otherwise.}
      \end{cases}
\]

where $c$ is an appropriate constant.

```

1.  Sketch a plot of the pdf.  What does this say about Regina's arival time?
1. Find the value of $c$ and specify the pdf of $X$.
1. Find the probability that Regina arrives before 12:15. 
1. Find the probability that Regina arrives after 12:45.  How does this compare to the previous part? What does that say about Regina's arrival time?
1. Find the probability that Regina arrives between 12:15 and 12:45.
1. Find the probability that Regina arrives between 12:15:00 and 12:15:01.
1. Find the probability that Regina arrives at the exact time 12:15:00 (with infinite precision).
1. Find the probability that Regina arrives between 12:59:00 and 1:00:00. How does this compare to the probability for 12:15:00 to 12:16:00? What does that say about Regina's arrival time?
1. Find the probability that Regina arrives between 12:59:59 and 1:00:00. How does this compare to the probability for 12:15:00 to 12:15:01? What does that say about Regina's arrival time?
1. Find the probability that Regina arrives at the exact time 1:00:00 (with infinite precision).
1. Compare this example and your answers to Example \@ref(exm:meeting-nonuniform-probspace1).



```{solution meeting-nonuniform-pdf1-sol}

to Example \@ref(exm:meeting-nonuniform-pdf1)

```

```{asis, fold.chunk = TRUE}

1.  The density increases linearly with $x$.
Regina is most likely to arrive closer to 1, and least likely to arrive close to noon (0).
1. $c=2$. The total area under the pdf must be 1. The region under the pdf is a triangle with area $(1/2)(1-0)(c)$, so $c$ must be 2 for the area to be 1.  Via integration
    \[
    1 = \int_0^1 cx dx = (c/2)x^2  \Big|_{x=0}^{x=1} = c / 2
    \]
1. Integrate the pdf over the region $[0, 0.25]$.  Since the pdf is linear, regions under the curve are triangles or trapezoids.
    \[
    \int_0^{0.25} 2x dx = x^2 \Bigg|_{x=0}^{x=0.25} = 0.25^2 = (1/2)(0.25 - 0)(2(0.25)) = 0.0625
    \]
1. Integrate the pdf over the region $[0.75, 1]$.
    \[
    \int_{0.75}^1 2x dx = x^2 \Bigg|_{x=0.75}^{x=1} = 1 - 0.75^2 = 0.4375
    \]
    So Regina is 7 times more likely to arrive within 15 minutes of 1 than within 15 minutes of noon.
1. 0.5. We could integrate the pdf from 0.25 to 0.75, or just use the previous results and properties of probabilities.
1. Similar to the previous parts, the probability  is $(0.25 + 1/60)^2 - 0.25^2 = 0.0086$.
(This probability is less than what it was in the uniform case.)
1. Similar to the previous part, $(0.25 + 1/3600)^2 - 0.25^2 = 0.00014$. (This probability is less than what it was in the uniform case.)
1. The exact time 12:15:00 represents a single point the sample space, an interval of length 0. 1. The probability that Regina arrives at the exact time 12:15:00 (with infinite precision) is 0.
1. Similar to previous parts, $1^2 - (1-1/60)^2 = 0.0331$. Notice that this one minute interval around 1:00 has a probability that is about 3.85 times larger than a one minute interval around 12:15.
1. $1^2 - (1-1/3600)^2 = 0.00056$. Notice that this one second interval around 1:00 has a probability that is about 4 times higher than a one second interval around 12:15, though both probabilities are small.
1. The exact time 1:00:00 represents a single point the sample space, an interval of length 0.  The probability that Regina arrives at the exact time 1:00:00 (with infinite precision) is 0.
1. The results are the same as those in Example \@ref(exm:meeting-nonuniform-probspace1).
In that example, the probability that Regina arrives in the interval $[0, x]$ was $x^2$, which can be obtained by integrating the pdf in this example from 0 to $x$.
Careful when you integrate; since $x$ is in the bounds of the integral you need a different dummy variable to use in $f_X$.
\[
  \IP(X \le x) = \int_0^x f_X(u) du = \int_0^x 2u du = u^2 \Bigg|_{u=x}^{u=0} = x^2
\]
We will see soon that such a function is called a cumulative distribution function (cdf).
    Notice how the pdf in REVISE 
```


(ref:cap-meeting-nonuniform-pdf) The probability density function from Example \@ref(exm:meeting-nonuniform-pdf1).


```{python, meeting-nonuniform-pdf-plot, echo=FALSE, fig.cap="(ref:cap-meeting-nonuniform-pdf)"}

Beta(2, 1).plot()
plt.show()

```



In the previous example, we specified the general shape of the pdf, then found the constant that made the total area under the curve equal to 1.
In general, a pdf is often defined only up to some multiplicative constant $c$, for example $f_X(x) = c\times(\text{some function of x})$, or $f_X(x) \propto \text{some function of x}$.  The constant $c$ does not affect the shape of the distribution as a function of $x$, only the scale on the density axis.  The absolute scaling on the density axis is somewhat irrelevant; it is whatever it needs to be to provide the proper *area*.  In particular, the total area under the pdf must be 1.  The scaling constant is determined by the requirement that $\int_{-\infty}^\infty f_X(x) dx = 1$.  

What's more important about the pdf is *relative* heights.  In the previous example the density at 1, $f_X(1) = c$, was 4 times greater than than density at 0.25, $f_X(0.25) = 0.25c$.  This was the reason why the probability of arriving close to 1 was about 4 times greater than the probability of arriving close to 12:15 (time 0.25).  The ratio of the densities at these two points could be computed without knowing the value of $c$.

Compare the pdf in Example \@ref(fig:meeting-nonuniform-pdf-plot) with the probabilities under the non-uniform measure in Figure \@ref(fig:arrival-time-probmeasure).
The pdf at a particular possible value $x$ is related to the probability that the random value takes a value close to that value $x$. 


```{example exponential-pdf}

Let $X$ be a random variable with the "Exponential(1)" distribution, illustrated by the smooth curve in Figure \@ref(fig:log-uniform-density), and represented by the spinner in Figure \@ref(fig:exponential-spinner).  Then the pdf of $f_X$ is

\[
f_X(x) =
\begin{cases}
e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]

```


1. Verify that $f_X$ is a valid pdf.
1. Find $\IP(X\le 1)$.
1. Find $\IP(X\le 2)$.
1. Find $\IP(1 \le X< 2.5)$.
1. Compute $\IP(X = 1)$.
1. Without integrating, approximate the probability that $X$ rounded to two decimal places is 1.
1. Without integrating, approximate the probability that $X$ rounded to two decimal places is 2.
1. Find and interpret the ratio of the probabilities from the two previous parts.  How could we have obtained this ratio from the pdf?
 
```{solution exponential-pdf-sol}

to Example \@ref(exm:exponential-pdf)

```

```{asis, fold.chunk = TRUE}


1. We need to check that the pdf integrates to 1: $\int_0^\infty e^{-x}dx = 1$.
1. $\IP(X\le 1) = \int_0^1 e^{-x}dx = 1-e^{-1}\approx 0.632$.  Recall the corresponding spinner in Figure \@ref(fig:exponential-spinner); 63.2% of the area corresponds to $[0, 1]$. 
1. $\IP(X\le 2) = \int_0^2 e^{-x}dx = 1-e^{-2}\approx 0.865$.  Recall the corresponding spinner in Figure \@ref(fig:exponential-spinner); 86.5% of the area corresponds to $[0, 2]$. 
1. $\IP(1 \le X< 2.5) = \int_1^{2.5} e^{-x}dx = e^{-1}-e^{-2.5}\approx 0.286$.  See the illustration below.
1. $\IP(X = 1)=0$, since $X$ is continuous.
1. Over a short region around 1, the area under the curve can be approximated by the area of a rectangle with height $f_X(1)$:
    \[
      \IP(0.995<X<1.005)\approx f_X(1)(1.005 - 0.995)=e^{-1}(0.01)\approx 0.00367879.
      \]
    See the illustration below. This provides a pretty good approximation of the true integral^[Reporting so many decimal places is unnecessary, and provides a false sense of precision.  All of these idealized mathematical models are at best approximately true in practice.  However, we provide the extra decimal places here to compare the approximation with the "exact" calculation.] $\int_{0.995}^{1.005} e^{-x}dx = e^{-0.995}-e^{-1.005}\approx 0.00367881$.
1. Over a short region around 2, the area under the curve can be approximated by the area of a rectangle with height $f_X(2)$:
    \[
    \IP(1.995<X<2.005)\approx f_X(2)(2.005 - 1.995)=e^{-2}(0.01)\approx 0.00135335.
    \]
    This provides a pretty good approximation of the integral  $\int_{1.995}^{2.005} e^{-x}dx = e^{-1.995}-e^{-2.005}\approx 0.00135336$.
1. Compare the rectangle-based approximations 
    \[
    \frac{\IP(1 - 0.005 <X < 1 + 0.005)}{\IP(2 - 0.005 <X < 2 + 0.005)} \approx 2.718 \approx \frac{e^{-1}(0.01)}{e^{-2}(0.01)} = \frac{e^{-1}}{e^{-2}} = \frac{f_X(1)}{f_X(2)}  
    \]
    The probability that $X$ is "close to" 1 is about 2.718 times greater than the probability that $X$ is "close to" 2.  This ratio is determined by the ratio of the densities at 1 and 2.

```

(ref:cap-exponential-pdf-area) Illustration of $\IP(1<X<2.5)$ for $X$ with an Exponential(1) distribution, corresponding to the spinner in Figure \@ref(fig:log-uniform-density).

```{r exponential-pdf-area, echo=FALSE, fig.cap="(ref:cap-exponential-pdf-area)"}

x<-seq(0, 5, 0.001)
y<-dexp(x,1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>1 & x<2.5),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, 1), expand=c(0, 0)) +
  theme_classic() +
  xlab(expression(x)) +
  ylab(expression(f[X](x))) #+
#  ggtitle(paste("P(1<X<2.5) for X ~ Exponential(1)"))

```


(ref:cap-exponential-pdf-area2) Illustration of $\IP(0.995<X<1.005)$ for $X$ with an Exponential(1) distribution, corresponding to the spinner in Figure \@ref(fig:log-uniform-density). The plot illustrates how the probability that $X$ is "close to" $x$ can be approximated by the area of a rectangle with height equal to the density at $x$, $f_X(x)$.

```{r exponential-pdf-area2, echo=FALSE, fig.cap="(ref:cap-exponential-pdf-area2)", out.width='50%', fig.show='hold'}

x<-seq(0, 5, 0.001)
y<-dexp(x,1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, 1), expand=c(0, 0)) +
  theme_classic() +
  xlab(expression(x)) +
  ylab(expression(f[X](x))) +
#  ggtitle(paste("P(1<X<2.5) for X ~ Exponential(1)")) +
  geom_rect(data=xddf, mapping=aes(xmin=1-0.1/2, xmax=1+0.1/2,
                                   ymin=0, ymax=dexp(1, 1)),
            color="orange", fill = "orange") 

```



A density is an idealized mathematical model. In practical applications, there is some acceptable degree of precision, and events like "$X$, rounded to 4 decimal places, equals 0.5" correspond to intervals that do have positive probability. For continuous random variables, it doesn't really make sense to talk about the probability that the random value equals a particular value. However, we can consider the probability that a random variable is close to a particular value.




To emphasize: The density $f_X(x)$ at value $x$ is *not* a probability. Rather, the density $f_X(x)$ at value $x$ is related to the probability that the RV $X$ takes a value "close to $x$" in the following sense^[This is true because an integral can be approximated by a sum of the areas of many rectangles with narrow bases.  Over a small interval of values surrounding $x$, the density shouldn't change that much, so we can estimate the area under the curve by the area of the rectangle with height $f_X(x)$ and base each equal to the length of the small interval of interest.].
\[
\IP\left(x-\frac{\ep}{2} \le X \le x+\frac{\ep}{2}\right) \approx f_X(x)\ep, \qquad \text{for small $\ep$}
\]
The quantity $\ep$ is a small number that represents the desired degree of precision.  For example, rounding to two decimal places corresponds to $\ep=0.01$.

Technically, any particular $x$ occurs with probability 0, so it doesn't really make sense to say that some values are more likely than others. However, a RV $X$ is more likely to take values *close to* those values that have greater density.  As we said previously,  what's important about a pdf is *relative* heights.  For example, if $f_X(\tilde{x})= 2f_X(x)$ then $X$ is roughly "twice as likely to be near $\tilde{x}$ than to be near $x$" in the above sense.
\[
\frac{f_X(\tilde{x})}{f_X(x)} = \frac{f_X(\tilde{x})\ep}{f_X(x)\ep} \approx  \frac{\IP\left(\tilde{x}-\frac{\ep}{2} \le X \le \tilde{x}+\frac{\ep}{2}\right)}{\IP\left(x-\frac{\ep}{2} \le X \le x+\frac{\ep}{2}\right)}
\]


### Exponential distributions



Exponential distributions are often used to model the *waiting times* between events in a random process that occurs continuously over time.


```{example, exponential-quakes}
Suppose that we model the waiting time, measured continuously in hours, from now until the next earthquake (of any magnitude) occurs in southern CA  as a continuous random variable $X$ with pdf
\[
f_X(x) = 2 e^{-2x}, \; x \ge0
\]
This is the pdf of the "Exponential(2)" distribution.
```

1. Sketch the pdf of $X$.  What does this tell you about waiting times?
1. Without doing any integration, approximate the probability that $X$ rounded to the nearest minute is 0.5 hours.
1. Without doing any integration determine how much more likely that $X$ rounded to the nearest minute is to be 0.5 than 1.5.
1. Compute and interpret $\IP(X > 0.25)$.
1. Compute and interpret $\IP(X \le 3)$.
1. Use simulation to approximate the long run average value of $X$.
Interpret this value.
At what *rate* do earthquakes tend to occur?
1. Use simulation to approximate the standard deviation of $X$.
What do you notice?


```{solution, exponential-quakes-sol}
to Example \@ref(exm:exponential-quakes)
```

```{asis, fold.chunk = TRUE}

1. See simulation below for plots.  Waiting times near 0 are most likely, and density decreases exponentially as waiting time increases.
1. Remember, the density at $x=0.5$ is not a probability, but it is related to the probability that $X$ takes a value close to $x=0.5$. The approximate probability that $X$ rounded to the nearest minute is 0.5 hours is 
\[
f_X(0.5)(1/60) = 2e^{-2(0.5)}(1/60) =  0.0123 
\]
1. Find the ratio of the densities at 0.5 and 1.5:
\[
\frac{f_X(0.5)}{f_X(1.5)} = \frac{2e^{-2(0.5)}}{2e^{-2(1.5)}} = \frac{e^{-2(0.5)}}{e^{-2(1.5)}} \approx 7.4.
\]
$X$ rounded to the nearest minute is about 7.4 times more likely to be 0.5 than 1.5.
A waiting time close to half an hour is about 7.4 times more likely than a waiting time close to 1.5 hours.
1. $\IP(X > 0.25) = \int_{0.25}^\infty 2e^{-2x}dx = e^{-2(0.25)}=0.606$.
Careful: this is *not* $2e^{-2(0.25)}$.
According to this model, the waiting time between earthquakes is more than 15 minutes for about 60% of earthquakes.
1. $\IP(X \le 3) = \int_0^3 2e^{-2x}dx = 1-e^{-2(3)}=0.9975$.  While any value greater than 0 is possible in principle, the probability that $X$ takes a really large value is small.  About 99.75% of earthquakes happen within 3 hours of the previous earthquake.
1. See simulation results below.
The simulated average is about 0.5 hours.
According to this model, the average waiting time between earthquakes is 0.5 hours.
That is, earthquakes tend to occur at rate 2 earthquakes per hour on average; this is what the 2 in the pdf represents.
1. The simulated standard deviation is also about 0.5, the same as the long run average value.


```

Below we simulate values from an Exponential distribution with rate parameter 2 and compare the simulated values to the theoretical results.


```{python}

X = RV(Exponential(rate = 2))

x = X.sim(10000)

x.plot()

Exponential(rate = 2).plot()
plt.show()

```

```{python}

x.count_gt(0.25) / x.count(), 1 - Exponential(rate = 2).cdf(0.25)

```


```{python}

x.count_lt(3) / x.count(), Exponential(rate = 2).cdf(3)

```

```{python}

x.mean(), Exponential(rate = 2).mean()

```

```{python}

x.sd(), Exponential(rate = 2).sd()

```



```{definition, exponential-pdf-def}

A continuous random variable $X$ has an **Exponential distribution** with *rate* parameter^[Exponential distributions are sometimes parametrized directly by their mean $1/\lambda$, instead of the rate parameter $\lambda$.  The mean $1/\lambda$ is called the scale parameter.] $\lambda>0$ if its pdf is
\[
f_X(x) =
\begin{cases}\lambda e^{-\lambda x}, & x \ge 0,\\
0, & \text{otherwise}
\end{cases}
\]
If $X$ has an Exponential($\lambda$) distribution then
\begin{align*}
\IP(X>x)  & = e^{-\lambda x}, \quad x\ge 0\\
\text{Long run average of $X$} & = \frac{1}{\lambda}\\
\text{Standard deviation of $X$} & = \frac{1}{\lambda}
\end{align*}

```



Exponential distributions are often used to model the *waiting time* in a random process until some event occurs.

- $\lambda$ is the average *rate* at which events occur over time (e.g., 2 per hour)
- $1/\lambda$ is the mean time between events (e.g., 1/2 hour)

An Exponential density has a peak at 0 and then decreases exponentially as $x$ increases.
The function $e^{-\lambda x}$ defines the shape of the density and the rate at which the density decreases.
The constant $\lambda$, which defines the density at $x=0$, simply rescales the vertical axis so that the total area under the pdf is 1.

(ref:exponential-densities-caption) Exponential densities with rate parameter $\lambda$. 

```{python, exponential-densities-caption, fig.cap='(ref:exponential-densities-caption)'}
plt.figure()
rates = [0.5, 1, 2]
for rate in rates:
    Exponential(rate).plot()
    
plt.legend(['$\lambda=$' + str(i) for i in rates]);
plt.xlim(0, 8);
plt.show()
```





### Joint probability density fuctions

Recall that the *joint distribution* of random variables $X$ and $Y$ (defined on the same probability space) is a probability distribution on $(x, y)$ *pairs*.  The joint distribution of two continuous random variables can be specified by a joint pdf, a surface specifying the density of $(x, y)$ pairs. The probability that the $(X,Y)$ pair of random variables lies is some region is the *volume* under the pdf surface over the region.


```{example uniform-sum-max-pdf}

Recall the example in Section \@ref(sim-transform-joint). Let $\IP$ be the probability space corresponding to two spins of the Uniform(1, 4) spinner, and let $X$ be the sum of the two spins, and $Y$ the larger spin (or the common value if a tie). Review the results of the simulation in Section \@ref(sim-transform-joint) before proceeding.

```

1. Find the joint pdf of $(X, Y)$.  Hint: see Figure \@ref(fig:dice-continuous-sum-max-joint).
1. Use geometry to find $\IP(X <4, Y > 2.5)$.
1. Suggest an expression for the marginal pdf of $Y$.  Hint: see Figure \@ref(fig:dice-continuous-max-marginal).
1. Use calculus to derive $f_Y(2.5)$, the marginal pdf of $Y$ evaluated at $y=2.5$.
1. Use calculus to derive $f_Y$, the marginal pdf of $Y$.
1. Find $\IP(Y > 2.5)$.
1. Suggest an expression for the marginal pdf of $X$.  Hint: see Figure \@ref(fig:dice-continuous-sum-marginal).
1. Use calculus to derive $f_X(4)$, the marginal pdf of $X$ evaluated at $x=4$.
1. Use calculus to derive $f_X(6.5)$, the marginal pdf of $X$ evaluated at $x=6.5$.
1. Use calculus to derive $f_X$, the marginal pdf of $X$. Hint: consider $x<5$ and $x>5$ separately.
1. Find $\IP(X < 4)$.
1. Find $\IP(X < 6.5)$.



```{solution uniform-sum-max-pdf-sol}
to Example \@ref(exm:uniform-sum-max-pdf)
```


```{asis, fold.chunk = TRUE}


1. Recall the discussion in Section \@ref(sim-transform-joint). While marginally $X$ takes values in (2, 8) and marginally $Y$ takes values in (1, 4), not every pair in $(2,8)\times(1,4)$ is possible.  Rather, the possible values of $(X, Y)$ lie in $\{(x, y): 2<x<8, 1<y<4, x/2<y<x-1\}$. The joint density of $(X, Y)$ is constant over the range of possible values.  The region $\{(x, y): 2<x<8, 1<y<4, x/2<y<x-1\}$ is a triangle with area 4.5.  The joint pdf is a surface of constant height floating above this triangle.  The volume under the density surface is the volume of this triangular "wedge".  If the constant height is $1/4.5 = 2/9\approx 0.222$, then the volume under the surface will be 1.  Therefore, the joint pdf of $(X, Y)$ is
    \[
    f_{X, Y}(x, y) =
      \begin{cases}
    2/9, & 2<x<8,\; 1<y<4,\; x/2<y<x-1,\\
    0, & \text{otherwise}
    \end{cases}
    \]
1. $\IP(X <4, Y > 2.5) = 1/36$. The probability is the volume under the pdf over the region of interest.  The base of the triangular wedge is $\{(x, y): 3.5<x<4, 2.5<y<3, x/2<y\}$, a region which has area $(1/2)(4-3.5)(3-2.5) = 1/8$.  Therefore, the volume of the triangular wedge that has constant height 2/9 is $(2/9)(1/8) = 1/36$
1. The density starts at 0 at $y=1$ and then increases linearly until $y=4$.  So we might guess
    \[
     f_Y(y) = 
       \begin{cases}
     c(y - 1), & 1<y<4,\\
     0, & \text{otherwise.}
          \end{cases}
    \]
    Then find $c$ to make the total area under the pdf equal 1.  The area under the pdf is the area of a triangle with base 4-1 and height $c(4-1)$ so setting $1=(1/2)(4-1)(c(4-1))$ yields $c=2/9$.
1. We find the marginal pdf of $Y$ evaluated at $y=2.5$ by "stacking" the density at each pair $(x, 2.5)$ over the possible $x$ values.  For discrete variables, the stacking is achieved by summing the joint pmf over the possible $x$ values. For continuous random variables we integrate the joint pdf over the possible $x$ values corresponding to $y=2.5$.  If $y=2.5$ then $3.5 < x< 5$.  "Integrate out the $x$'s" by computing a $dx$ integral: 
    \[
      f_Y(0.25) = \int_{3.5}^5 (2/9)\, dx = (2/9)x \Bigg|_{x=3.5}^{x=5} = 1/3
    \]
    This agrees with the result of plugging in $y=2.5$ in the expression in the previous part (with $c=2/9$).
1. To find the marginal pdf of $Y$ we repeat the calculation from the previous part for each possible value of $y$.  Fix a $1<y<4$, and replace 2.5 in the previous part with a generic $y$.  The possible values of $x$ corresponding to a given $y$ are $y + 1 < x < 2y$. "Integrate out the $x$'s" by computing a $dx$ integral.  Within the $dx$ integral, $y$ is treated like a constant.
    \[
      f_Y(y) = \int_{y+1}^{2y} (2/9)\, dx = (2/9)x \Bigg|_{x=y+1}^{x=2y} = (2/9)(y-1), \qquad 1<y<4
    \]
1. If we have already derived the marginal pdf of $Y$, we can treat this just like a one variable problem.  Integrate the pdf of $Y$ over $(2.5, 4)$
    \[
      \IP(Y > 2.5) = \int_{2.5}^4 (2/9)(y-1)\, dy = (1/9)(y-1)^2\Bigg|_{y=2.5}^{y=4} = 0.25
    \]
1. We see that the density is 0 at $x=2$ and $x=8$ and has a triangular shape with a peak at $x=5$.  If $c$ is the density at $x=5$, then $1 = (1/2)(8-2)c$ implies $c=1/3$. We might guess
    \[
     f_X(x) = 
       \begin{cases}
       (1/9)(x-2), & 2 < x< 5,\\
       (1/9)(8-x), & 5<x<8,\\
       0, & \text{otherwise.}
       \end{cases}
    \]
    We could also write this as $f_X(x) = 1/3 - (1/9)|x - 5|, 2<x<8$.
1. We find the marginal pdf of $X$ evaluated at $x=4$ by "stacking" the density at each pair $(4, y)$ over the possible $y$ values.  For discrete variables, the stacking is achieved by summing the joint pmf over the possible $y$ values. For continuous random variables we integrate the joint pdf over the possible $y$ values corresponding to $x=4$.  If $x=4$ then $2 < y< 3$.  "Integrate out the $y$'s" by computing a $dy$ integral: 
    \[
      f_X(4) = \int_{2}^3 (2/9)\, dy = (2/9)y \Bigg|_{y=2}^{y=3} = 2/9 \approx 0.222
    \]
    This agrees with the result of plugging in $x=4$ in the expression in the previous part.
1. This is similar to the previous part, but for $x=6.5$, we hit the upper bound of 4 on $y$ values; that is, the range isn't just from $x/2$ to $x-1$, but rather $x/2$ to 4.  If $x=6.5$ then $3.25 < y< 4$.  "Integrate out the $y$'s" by computing a $dy$ integral: 
    \[
      f_X(6.5) = \int_{3.25}^4 (2/9)\, dy = (2/9)y \Bigg|_{y=3.25}^{y=4} = 1/6 \approx 0.167
    \]
    This agrees with the result of plugging in $x=6.5$ in the expression in two parts ago.
1. For $2<x<5$ the bounds on possible $y$ values are $x/2$ to $x-1$
    \[
      f_X(x) = \int_{x/2}^{x-1} (2/9)\, dy = (2/9)y \Bigg|_{y=x/2}^{y=x-1} = (1/9)(x - 2), \qquad 2<x<5.
    \]
    For $5<x<8$ the bounds on possible $y$ values are $x/2$ to $4$
    \[
      f_X(x) = \int_{x/2}^{4} (2/9)\, dy = (2/9)y \Bigg|_{y=x/2}^{y=4} = (1/9)(8 - x), \qquad 5<x<8.
    \]
    So the calculus matches what we did a few parts ago.
1. If we have already derived the marginal pdf of $X$, we can treat this just like a one variable problem.  Integrate the pdf of $X$ over $(2, 4)$
    \[
      \IP(X  < 4) = \int_{2}^4 (1/9)(x-2)\, dx = (1/18)(x-2)^2\Bigg|_{x=2}^{x=4} = 2/9 \approx 0.222
    \]
1. If we have already derived the marginal pdf of $X$, we can treat this just like a one variable problem.  It's easiest to use the complement rule and integrate the pdf of $X$ over $(6.5, 8)$
    \[
      \IP(X  < 6.5) = 1 - \IP(X > 6.5) = 1 - \int_{6.5}^8 (1/9)(8-x)\, dx =1 -  (-1/18)(8-x)^2\Bigg|_{x=6.5}^{x=8} = 1 - 1/8 = 7/8  = 0.875
    \]

```



```{definition joint-pdf}

The **joint probability density function (pdf)** of two *continuous* random variables $(X,Y)$ defined on a probability space with probability measure $\IP$ is the function $f_{X,Y}:\reals^2\mapsto[0,\infty)$ which satisfies, for any $S\subseteq \reals^2$,
\[
 \IP[(X,Y)\in S] = \iint\limits_{A}  f_{X,Y}(x,y)\, dx dy 
\]


```


A joint pdf is a surface with height $f_{X,Y}(x,y)$ at $(x, y)$. The probability that the $(X,Y)$ pair of random variables lies in the region $A$ is the *volume* under the pdf surface over the region $A$

A joint pdf is a probability distribution on $(x, y)$ *pairs*. The height of the density surface at a particular $(x,y)$ pair is related to the probability that $(X, Y)$ takes a value "close to^[You can have different precisions for $X$ and $Y$, e.g., $\ep_x, \ep_y$, but using one $\ep$ makes the notation a little simpler.]" $(x, y)$
\[
\IP(x-\ep/2<X <  x+\ep/2,\; y-\ep/2<Y < y+\ep/2) = \ep^2 f_{X, Y}(x, y) \qquad \text{for small $\ep$}
\]

A valid joint pdf must satisfy
\begin{align*}
f_{X,Y}(x,y) & \ge 0\\
\int_{-\infty}^\infty\int_{-\infty}^\infty f_{X,Y}(x,y)\, dx dy &  = 1
\end{align*}
Given a specific pdf, the generic bounds $(-\infty, \infty)\times(-\infty, \infty)$ in the above integral should be replaced by the range of possible pairs of values, that is, those $(x, y)$ pairs for which $f_{X, Y}(x, y)>0$.


The marginal pdfs can be obtained from the joint pdf by the law of total probability.  In the discrete case, to find the marginal probability that $X$ is equal to $x$, sum the joint pmf $p_{X, Y}(x, y)$ over all possible $y$ values.  The continuous analog is to integrate the joint pdf $f_{X,Y}(x,y)$ over all possible $y$ values to find the marginal density of $X$ at $x$.  This can be thought of as "stacking" or "collapsing" the joint pdf.

\begin{align*}
f_X(x) & = \int_{-\infty}^\infty f_{X,Y}(x,y) dy & & \text{a function of $x$ only}
\\
f_Y(y) & = \int_{-\infty}^\infty f_{X,Y}(x,y) dx & & \text{a function of $y$ only}
\end{align*}

The marginal distribution of $X$ is a distribution on $x$ values only.  For example, the pdf of $X$ is a function of $x$ only (and not $y$).  (Similarly the pdf of $Y$ is a function of $y$ only and not $x$.)

In general the marginal distributions do not determine the joint distribution, unless the RVs are independent.  In terms of a table: you can get the totals from the interior cells, but in general you can't get the interior cells from the totals.



## Cumulative distribution functions



While pmfs and pdfs play analogous roles for discrete and continuous random variables, respectively, they do behave differently; pmfs provide probabilities directly, but pdfs do not.  It is convenient to have one object that describes a distribution in the same way, regardless of the type of variable, and which returns probabilities directly.  This object is called the *cumulative distribution function (cdf)*.  While the definition might seem strange at first, you have probably already had experience with cumulative distribution functions.


```{example height-percentile}

Maggie and Seamus are babies who have just turned one.
At their one-year visits to their pediatrician:

- Maggie is 76cm tall and in the [75th percentile of height for girls](https://www.cdc.gov/growthcharts/data/set1clinical/cj41l018.pdf).
- Seamus is 72cm tall and in the [10th percentile of height for boys](https://www.cdc.gov/growthcharts/data/set1clinical/cj41l017.pdf).

Explain what these percentiles mean.

```

```{solution height-percentile-sol}
to Example \@ref(exm:height-percentile)
```


```{asis, fold.chunk = TRUE}

- 75% of one-year-old girls (in the U.S.) are less than 76cm tall, and 25% are more than 76cm tall. So Maggie is taller than 75% of one-year-old girls.
- 10% of one-year-old boys (in the U.S.) are less than 72cm tall, and 90% are more than 72cm tall. So the wee baby Seamus is taller than 10% of one-year-old boys.

```


<!-- ```{example sat-percentile} -->

<!-- According to [data on students who took the SAT in 2018-2019](https://collegereadiness.collegeboard.org/pdf/understanding-sat-scores.pdf), 1400 was the 94th percentile of SAT scores, while 1000 was the 40th percentile.  How do you interpret these percentiles? -->

<!-- ``` -->



<!-- ```{solution sat-percentile-sol} -->
<!-- to Example \@ref(exm:sat-percentile) -->
<!-- ``` -->


<!-- ```{asis, fold.chunk = TRUE} -->


<!-- Interpretation: 94% of SAT takers scores at or below 1400, and 6% of SAT takers score greater than 1400.  Similarly, 40% of SAT takers scores at or below 1000, and 60% of SAT takers score greater than 1000.  -->

<!-- ``` -->


Roughly, the value $x$ is the $p$th percentile of a distribution of a random variable $X$ if $p$ percent of values of the variable are less than or equal to $x$: $\IP(X\le x) = p$. The *cumulative distribution function (cdf)* of a random variable fills in the blank
for any given $x$: $x$ is the (blank) percentile. That is, for an input $x$, the cdf outputs $\IP(X\le x)$.




```{definition, cdf}

The **cumulative distribution function (cdf)** (of a random variable $X$ defined on a probability space with probability measure $\IP$)
is the function, $F_X: \mathbb{R}\mapsto[0,1]$, defined by
$F_X(x) = \IP(X\le x)$. A cdf is defined for all real numbers $x$
regardless of whether $x$ is a possible value of $X$.

```


```{example sat-percentile2}

According to [data on students who took the SAT in 2018-2019](https://collegereadiness.collegeboard.org/pdf/understanding-sat-scores.pdf), 1400 was the 94th percentile of SAT scores, while 1000 was the 40th percentile.
Let $X$ be the SAT score of a randomly selected
student (from this cohort), and let $F_X$ be the cdf of $X$.
Evaluate the cdf for each of the following.
For the purposes of this exercise, interpret these quantities in terms of actual SAT scores, which take values in 400, 410, 420, $\ldots$, 1590, 1600. 

```

1. $F_X(1400)$
1. $F_X(1405)$
1. $F_X(1000)$
1. $F_X(1003.7)$
1. $F_X(-3.1)$
1. $F_X(390)$
1. $F_X(399.5)$
1. $F_X(1600)$
1. $F_X(1610)$
1. $F_X(2307.4)$
1. $F_X(1400)-F_X(1000)$


```{solution sat-percentile2-sol}
to Example \@ref(exm:sat-percentile2)
```


```{asis, fold.chunk = TRUE}

1. $F_X(1400)=0.94$. We are told that $\IP(X \le 1400) = 0.94$.
1. $F_X(1405) = 0.94$. In terms of reall SAT scores, $\IP(X \le 1405) = \IP(X\le 1400)$.
1. $F_X(1000) = 0.40$. We are told that $\IP(X \le 1000) = 0.40$.
1. $F_X(1003.7) = 0.40$. In terms of reall SAT scores, $\IP(X \le 1003.7) = \IP(X\le 1000)$.
1. $F_X(-3.1)=0$. The smallest possible score is 400.
1. $F_X(390)=0$. The smallest possible score is 400.
1. $F_X(399.5)= 0$. The smallest possible score is 400.
1. $F_X(1600) = 1$. The largest possible score is 1600, so 100% of students score no more than 1600.
1. $F_X(1610) = 1$. The largest possible score is 1600.
1. $F_X(2307.4) = 1$. The largest possible score is 1600.
1. $0.54 = F_X(1400)-F_X(1000)=\IP(X\le 1400) - \IP(X \le 1000) = \IP(1000 < X \le 1400)$.  54% of SAT takers score greater than 1000 but at most 1400.

```


To understand a cdf, imagine a spinner for a particular distribution.  Suppose a "second hand" starts at the smallest possible value ("12:00") and sweeps clockwise around the spinner.  The second hand sweeps out area as it goes; when the second hand is pointing at $x$, the area that it has swept through represents $\IP(X\le x)$.  The cdf records the values of $F_X(x) = \IP(X\le x)$ as the second hand moves along and points to different values of $x$.

While a cdf is defined the same way for both discrete and continuous random variables, it is probably best understood in terms of continuous random variables.  Remember that for a continuous random variable, $\IP(X\le x)$ is the area under the density curve over the interval $(-\infty, x]$ (remember the density might be 0 for some values in this range).  Imagine plotting a density curve and adding a vertical line at $x$; $\IP(X\le x)$ is the area under the curve to left of this line.  The cdf is constructed by moving the vertical line from left to right, from smaller to larger values of $x$, and recording  the area under the curve to the left of the line, $F_X(x) = \IP(X\le x)$, as $x$ varies.







<!-- See Figure \@ref(fig:cdf-illustration) for an illustration. -->

See the figure below for an illustration.
The shaded area in the plot on the left represents $F_X(x)=\IP(X\le x)$, which is about 0.6 in this example. This area is represented by the $(x, F_X(x))$ point in the cdf plot in the middle. The cdf plot in the middle represents the result of recording the area in the plot on the left for all values of $x$. The plot on the right displays the spinner corresponding to the pdf on the left.



<!-- (ref:cap-cdf-picture) Illustration of a pdf (left) and the corresponding cdf (middle). The cdf at $x$ is highlighted in the plots and shaded spinner (right). -->



```{r cdf-illustration, echo=FALSE, fig.show="hold", out.width="33%"}


x0 = qgamma(0.6, shape=4, rate=1)
x<-seq(0, 15, 0.001)
y<-dgamma(x,shape=4, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>0 & x<x0),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0)) +
  theme_classic() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  scale_x_continuous(breaks = x0, labels = expression(x)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(f[X](x)))
#  ggtitle(paste("Shaded area is ", expression(F(x)),", the cdf of X evaluated at x", sep=""))

y<-pgamma(x,shape=4, rate=1)
y0 = pgamma(x0,shape=4, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0.01)) +
  theme_classic() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_x_continuous(breaks = x0, labels = expression(x)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(F[X](x))) +
#  ggtitle(paste("Shaded area is ", expression(F(x)),", the cdf of X evaluated at x", sep="")) +
  geom_segment(x=x0, xend=x0, y=0, yend=y0, size = 2, linetype=2, colour="orange") +
  geom_segment(x=0, xend=x0, y=y0, yend=y0, size = 2, linetype=2, colour="orange")



p = 0.4

df <- data.frame(
  group = c(">x", "<= x"),
  value = c(1-p, p)
  )

cdfpie <- ggplot(df, aes(x="", y=value, fill=group)) +
  geom_bar(width = 1, stat = "identity", color='black') +
    blank_theme +
  scale_y_continuous(breaks=c(1-p, p), labels=c("x", "")) +
  coord_polar("y") +
  scale_fill_manual(values=c("white", "orange")) +
  geom_text(aes(y = value/2 + c(0, cumsum(value)[-length(value)]), 
            label = c("P(X <= x)", "")), size=5) +
  theme(legend.position = "none")

cdfpie



```



```{example exponential-cdf-calcs}

Let $X$ be a random variable with the "Exponential(1)" distribution, illustrated by the smooth curve in Figure \@ref(fig:log-uniform-density).  Then the pdf of $f_X$ is

\[
f_X(x) =
\begin{cases}
e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]

```

1. Find the cdf of $X$, and sketch a plot of it.
1. Evaluate and interpret $F_X(1)$.  How does this appear in Figure \@ref(fig:log-uniform-density)?
1. Evaluate and interpret $F_X(2)-F_X(1)$.  How does this appear in Figure \@ref(fig:log-uniform-density)?
1. Evaluate and interpret $F_X(2)$.  How does this appear in Figure \@ref(fig:log-uniform-density)?
1. Find $\IP(1 < X < 2.5)$ without integrating again.
1. Suppose we had been given the cdf instead of the pdf.  How could we find the pdf?


```{solution exponential-cdf-calcs-sol}
to Example \@ref(exm:exponential-cdf-calcs)
```

```{asis, fold.chunk = TRUE}

1. $F_X(x)=0$ for $x<0$. For $x>0$ we integrate the density^[Here $x$ represents a particular value of interest, so we use a different dummy variable, $u$,  in the integrand.]
\[
F_X(x) = \IP(X \le x) = \int_0^x e^{-u} du = 1 - e^{-x}
\]
So the cdf of $X$ is
\[
F_X(x) =
\begin{cases}
1 - e^{-x}, & x>0,\\
0, & \text{otherwise.}
\end{cases}
\]
1. $F_X(1)=\IP(X\le 1) = 1-e^{-1}\approx 0.632$.  This is represented in Figure \@ref(fig:log-uniform-density) by the area of the region from 0 to 1, 63.2%.
1. This is represented in Figure \@ref(fig:log-uniform-density) by the area of the region from 1 to 2, 23.3%.
\[
F_X(2)- F_X(1)=\IP(X\le 2) - \IP(X \le 1) =\IP(1<X\le 2)= (1-e^{-2})-(1-e^{-1})=e^{-1}-e^{-2}\approx 0.233
\]
1. $F_X(2)=\IP(X\le 2) = 1-e^{-2}\approx 0.865$.  This is represented in Figure \@ref(fig:log-uniform-density) by the area of the region from 0 to 2, 63.2%+23.3% = 86.5%.
1.
\[
\IP(1 < X < 2.5) = \IP(X\le 2.5) - \IP(X \le 1) = (1-e^{-2.5})-(1-e^{-1})=e^{-1}-e^{-2.5}\approx 0.286
\]
1. Since the cdf is obtained by integrating the pdf, the pdf if obtained by differentiating the cdf.  Differentiate the cdf $F_X(x)=1-e^{-x},\ x>0$ with respect to its argument $x$ to obtain the pdf $f_X(x) = e^{-x},\ x>0$.

```


(ref:cap-exponential-cdf-illustration) Illustration of the pdf (left) and the cdf (right) for the Exponential(1) distribution represented by the spinner in Figure \@ref(fig:log-uniform-density). The shaded area in the plot on the left represents $F_X(1)=\IP(X\le 1)$, which is $1-e^{-1}\approx0.632$.  This area is represented by the $(1, F_X(1))$ point in the cdf plot on the right, and in the region from 0 to 1 in the spinner in Figure \@ref(fig:log-uniform-density).

```{r exponential-cdf-illustration, echo=FALSE, fig.cap="(ref:cap-exponential-cdf-illustration)", out.width='50%', fig.show='hold'}
x0 = 1
xmax = 6
x<-seq(0, xmax, 0.001)
y<-dgamma(x,shape=1, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>0 & x<x0),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0)) +
  theme_classic() +
  scale_x_continuous(limits=c(0, xmax), breaks=0:xmax, expand=c(0, 0)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(f[X](x)))


y<-pgamma(x,shape=1, rate=1)
y0 = pgamma(x0,shape=1, rate=1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, max(y)), expand=c(0, 0.01)) +
  theme_classic() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  scale_x_continuous(limits=c(0, xmax), breaks=0:xmax, expand=c(0, 0)) +
  theme(axis.text.x = element_text(color = c("black")),
        axis.ticks.x = element_line(color = c("black"))) +
  ylab(expression(F[X](x))) +
#  ggtitle(paste("Shaded area is ", expression(F(x)),", the cdf of X evaluated at x", sep="")) +
  geom_segment(x=x0, xend=x0, y=0, yend=y0, size = 2, linetype=2, colour="orange") +
  geom_segment(x=0, xend=x0, y=y0, yend=y0, size = 2, linetype=2, colour="orange")

```

For named distributions, we can evaluate the cdf in Symbulate using the `.cdf()` method.

```{python}

Exponential(1).cdf(1)

```

```{python}

Exponential(1).cdf([-1, 2, 2.5])

```



**For continuous random variables, think of the cdf as a "generic integral".**
Rather than integrating from scratch to find $\IP(X < 1)$, $\IP(X < 2)$, $\IP(1 < X< 2)$, etc, the integral is computed once for a generic $x$ and then evaluated to find probabilities for specific values of $x$, $F_X(1)$, $F_X(2)$, $F_X(2)-F_X(1)$, etc.

We integrate the pdf to find the cdf, and we differentiate the cdf to find the pdf.
If $X$ is a continuous random variable with cdf $F_X$ then its pdf if $f_X = F'_X$.

```{example, meeting-nonuniform-cdf1}
Recall Example \@ref(exm:meeting-nonuniform-probspace1).  Let $X$ be Regina's arrival time.

```

1. Find the cdf of $X$.
1. Find the pdf of $X$.

```{solution meeting-nonuniform-cdf1-sol}

to Example \@ref(exm:meeting-nonuniform-cdf1)

```

```{asis, fold.chunk = TRUE}

1. The cdf is provided by the setup: $F_X(x) = x^2, 0<x<1$.  (And $F_X(x) = 0, x <0$, $F_X(x)=1, x > 1$)
1.  Differentiate the cdf with respect to $x$.  $f_X(x) = F'_X(x) = 2x$, $0<x<1$.  This is the pdf in Example \@ref(exm:meeting-nonuniform-pdf1).

```

For any random variable $X$ with cdf $F_X$
\[
F_X(b) - F_X(a) = \IP(a<X \le b)
\]
Note that whether the inequalities in the above event are strict ($<$) or not ($\le$) matters for discrete random variables, but not for continuous.


```{example binomial-cdf}

Let $X$ be the number of heads in 3 flips of a fair coin.  

```

1. Find the cdf of $X$ and sketch a plot of it.
1. Let $Y$ be the number of tails in 3 flips of a fair coin.  Find the cdf of $Y$.

```{solution binomial-cdf-sol}
to Example \@ref(exm:binomial-cdf)
```

```{asis, fold.chunk = TRUE}

1. See Figure \@ref(fig:binomial-cdf-illustration). $X$ takes values 0, 1, 2, 3, with respective probabilities 1/8, 3/8, 3/8, 1/8.  We sum these probabilities to find the cdf.  For example, $F_X(0) = \IP(X\le 0) = 1/8$, $F_X(1)=\IP(X\le 1) = \IP(X=0) + \IP(X=1) = 1/8+3/8 = 0.5$.  Remember that $x$ is defined for any value of $x$, for example $F_X(1.5) = \IP(X\le 1.5)= \IP(X=0) + \IP(X=1)=0.5$. The cdf is a step function, which is flat for impossible values of $x$ and jumps at possible values $x$ with the jump size at $x$ equal to the value of the pmf at $x$.
\[
F_X(x) =
\begin{cases}
0, & x<0,\\
1/8, & 0\le x<1,\\
4/8, & 1\le x<2,\\
7/8, & 2\le x<3,\\
1, & x\ge 3.
\end{cases}
\]
1. The cdf describes a distribution.  Since $X$ and $Y$ have the same distribution, they will have the same cdf.  The only difference would be labeling; we would call the cdf of $Y$, $F_Y$, and the argument of this function would typically (but not necessarily) be denoted $y$.

```


(ref:cap-binomial-cdf-illustration) Illustration of the pmf (left) and the cdf (right) of $X$, the number of heads in 3 flips of a fair coin.  The possible values of $X$ are 0, 1, 2, 3.  The pmf on the left displays the probabilities of these values, $p_X(x) = \IP(X=x)$. The cdf on the right displays $F_X(x)=\IP(X\le x)$. The cdf is flat between possible values, and jumps at the possible values, with the jumps sizes given by the pmf. (The corresponding distribution is the "Binomial(3, 0.5)" distribution.)




```{r binomial-cdf-illustration, echo=FALSE, fig.cap="(ref:cap-binomial-cdf-illustration)", out.width='50%', fig.show='hold'}

n = 3
p = 0.5
x = 0:3
xmin = -0.5
xmax = 3.5
px = dbinom(x, n, p)
Fx = pbinom(x, n, p)

plot(x, px, type='h', ylim=c(0, 1), xlim=c(xmin, xmax), ylab=expression(p[X](x)), yaxt='n', col="orange", lwd=2)
axis(2, (0:8)/8, cex.axis = 0.7)


plot(x, Fx, type='n', ylim=c(0, 1), xlim=c(xmin, xmax), ylab=expression(F[X](x)), yaxt='n')
axis(2, (0:8)/8, cex.axis = 0.7)
points(x, Fx, pch=19)
segments(x0=c(xmin, x), x1=c(x, xmax), y0=c(0, Fx), y1 = c(0, Fx))
segments(x0=x, x1=x, y0=c(0, Fx[-length(Fx)]), y1=Fx, lty=2, lwd = 2, col="orange") 
```



```{python}

Binomial(3, 0.5).cdf(2)

```

```{python}

Binomial(3, 0.5).cdf([-1, 0, 0.5, 0.99, 1, 1.1, 2.4, 2.9, 3, 3.1, 3.9999, 4, 10])

```


A few properties of cdfs

- A cdf is defined for all values of $x$, regardless if $x$ is a possible value of the RV.
- A cdf is a non-decreasing function^[This follows from the subset rule, since if $x\le \tilde{x}$ then $\{X\le x\}\subseteq\{X\le \tilde{x}\}$]: if $x \le \tilde{x}$ then $F_X(x)\le F_X(\tilde{x})$.
- A cdf approaches 0 as the input approaches $-\infty$: $\lim_{x\to-\infty}F_X(x) = 0$
- A cdf approaches 1 as the input approaches $\infty$: $\lim_{x\to\infty}F_X(x) = 1$
- The cdf of a *discrete* random variable is a step function.
  - The steps occur at the possible values of the random variable.
  - The height of a particular step corresponds to the probability of that value, given by the pmf.
- The cdf of a *continuous* random variable is a continuous function.
  - The cdf of a *continuous* random variable is obtained by integrating the pdf, so
  - The pdf of a *continuous* random variable is obtained by differentiating the cdf
  \[
  F_X' = f_X \qquad \text{if $X$ is continuous}
  \]
- For any random variable $X$ with cdf $F_X$
\[
F_X(b) - F_X(a) = \IP(a<X \le b)
\]
Whether the inequalities in the above event are strict ($<$) or not ($\le$) matters for discrete random variables, but not for continuous.

One advantage to using cdfs is that they are defined the same way ($F_X(x) = \IP(X\le x)$) for both continuous and discrete random variables.  So results stated in terms of cdfs apply for both discrete and continuous random variables.  This is a little more convenient than having two versions of every definition/result/proof: a statement for discrete RVs in terms of pmfs and a separate statement for continuous RVs in terms of pdfs.  The following definition is an example.



```{definition, samedist-def}

Random variables $X$ and $Y$ **have the same distribution** if their cdfs are the same, that is, if $F_X(u) = F_Y(u)$ for all^[Note that $u$ just represents a dummy variable, the argument of the two functions.  While we generally think of $x$ as the argument of $F_X$, that is just a convenient labeling.  Here we are checking for equality of two *functions*, so we need to use the same input for both.  That is, something like "$F_X(x) = F_Y(y)$" makes no sense because $x$ and $y$ represent different inputs.] $u\in\mathbb{R}$.

```

That is, two random variables have the same distribution if all the percentiles are the same.  While we generally think of two discrete random variables having the same distribution if they have the same pmf, and two continuous random variables having the same distribution if they have the same pdf, the above definition provides a consistent criteria for any two random variables to have the same distribution, regardless of type.


Multiple random variables defined on the same probability space have a joint cdf.


```{definition joint-cdf}

The **joint cdf** of random variables $(X,Y)$ defined on a probability space with probability measure $\IP$ is the function $F_{X,Y}:\reals^2\mapsto[0,1]$ defined by
\[
F_{X,Y}(x,y) = \IP(X\le x, Y\le y) \qquad \text{ for all } x,y
\]

```






## Quantile functions

Recall that the cdf fills in the following blank
for any given $x$: $x$ is the (blank) percentile.
The *quantile function* (essentially the inverse cdf^[If the cdf is a continuous function, then the quantile function is
    the inverse cdf. But the inverse of a cdf might not exist, if the cdf has
    jumps or flat spots. In particular, the inverse cdf does not exist
    for discrete random variables. So in general, the quantile function corresponding to cdf $F$ is defined as
    $Q(p) = \inf\{u:F(u)\ge p\}$.]) fills in the
following blank for a given $p$: the $p$th percentile is (blank)


```{example exponential-quantile}

Let $X$ have an Exponential(1) distribution.  Recall that the cdf is $F_X(x) = 1-e^{-x}, x>0$.

```

1. Find the median (i.e., 50th percentile). How does this appear in Figure \@ref(fig:log-uniform-density)?
1. Find the 63.2th percentile. How does this appear in Figure \@ref(fig:log-uniform-density)?
1. Find the 86.5th percentile. How does this appear in Figure \@ref(fig:log-uniform-density)?
1. Specify a function that returns the $p$th percentile for $p\in[0,1]$. How does this function relate to the spinner in Figure \@ref(fig:log-uniform-density)? 

```{solution exponential-quantile-sol}
to Example \@ref(exm:exponential-quantile)
```

```{asis, fold.chunk = TRUE}


1. Set $0.5 = \IP(X\le x) = 1-e^{-x}$ and solve to find $x=-\log(1-0.5)=\log(2)\approx0.693$ This is represented by the halfway point in the spinner in Figure \@ref(fig:log-uniform-density).
1. Set $0.632 = \IP(X\le x) = 1-e^{-x}$ and solve to find $x=-\log(1-0.632)\approx 1$. We see that the value 1 in Figure \@ref(fig:log-uniform-density) has an area below it of 63.2%.
1. Set $0.865 = \IP(X\le x) = 1-e^{-x}$ and solve to find $x=-\log(1-0.865)\approx 2$. We see that the value 2 in Figure \@ref(fig:log-uniform-density) has an area below it of 86.5%.
1. Set $p = \IP(X\le x) = 1-e^{-x}$ and solve for $x$ to find $x=-\log(1-p)$.  For a given $p$ --- the area of any region starting from 0 in the spinner in Figure \@ref(fig:log-uniform-density) --- $-\log(1-p)$ is the corresponding value on the axis of the spinner.

```


For named distributions, we can evaluate the quantile function in Symbulate using the `.quantile()` method.

```{python}

Exponential(1).quantile(0.5)

```

```{python}

Exponential(1).quantile([0.632, 0.865, 0.9999])

```


For a *continuous* random variable with cdf $F$, the **quantile function** is the function $Q:[0,1]\mapsto\mathbb{R}$ defined by $Q(p) = F^{-1}(p)$.

The quantile function can be used to create a spinner for a distribution.
Basically, the values on the outside boundary of the spinner are scaled based on the quantile function (which is determined by the cdf).
Intervals corresponding to regions of higher density ("more likely") values are stretched out on the spinner boundary; intervals corresponding regions of lower density ("less likely" values) are shrunk.

### Universality of the Uniform (One spinner to rule them all)

Recall how the spinner in Figure \@ref(fig:log-uniform-density) was constructed.  We started with the Uniform(0, 1) spinner with equally spaced increments, and applied the transformation $-\log(1-u)$, which "stretched" the intervals corresponding to higher probability and "shrunk" the intervals corresponding to lower probability.  The distribution that we ended up with was the Exponential(1) distribution with cdf $1-e^{-x}, x>0$.  Notice now that the transformation $-\log(1-u)$ corresponds to the quantile function of an Exponential(1) distribution.

Example \@ref(exm:exponential-quantile) provides an example of how to go backwards.  Starting from a cdf, we can construct the corresponding spinner by finding the quantile function, essentially the inverse cdf, and applying it to the equally spaced values on the Uniform(0, 1) spinner.  The quantile function will stretch/shrink the intervals just right to correspond to the probabilities given by the cdf.

This is the idea behind "universality of the uniform".  Basically, we can always start with a spinner that is "equally likely" to land in (0, 1) and suitably stretch/scale the axis around the spinner to construct a spinner corresponding to any distribution of interest.  The Uniform(0, 1) spinner returns a value in (0, 1); we obtain the value of the corresponding percentile from the quantile function.

**Universality of the Uniform.** Let $F$ be a cdf and $Q$ its corresponding quantile function.  Let $U$ have a Uniform(0, 1) distribution and define the random variable $X=Q(U)$.  Then the cdf of $X$ is $F$.

In the above, $U$ represents the result of the spin on the [0, 1] scale, and $Q(U)$ is the corresponding value on the stretched/shrunk scale.

We'll only prove the result assuming $F$ is a continuous, strictly increasing function, so that the quantile function is just the inverse of $F$, $Q(p) = F^{-1}(p)$.
\[
\IP(X \le x) = \IP(F^{-1}(U)\le x) = \IP(U\le F(x)) = F(x)
\]
The last step follows since $F(x)$ is just a number in [0, 1] and for $\IP(U\le u) = u$ for $0\le u\le 1$ since $U$ has a Uniform(0, 1) distribution.

To show that $\{F^{-1}(U)\le x\} = \{U\le F(x)\}$, draw a picture like the middle plot in the figure before Example \@ref(exm:exponential-cdf-calcs).
<!-- Consider $u\in[0, 1]$ and let $F^{-1}(u)=x$, so $F(x)=u$. -->
<!-- Since $F$ is increasing if $\tilde{x}<x$ then $F(\tilde{x})<F(x)$ -->


## Distributions of transformations of random variables {#cdf-method}


Recall that a function of a random variable is also a random variable. If $X$ is a random variable, then $Y=g(X)$ is also a random variable and so it has a probability distribution. Unless $g$ represents a linear rescaling, a transformation will change the shape of the distribution.  So the question is: what is the distribution of $g(X)$?  We'll focus on transformations of continuous random variables, in which case the key to answering the question is to work with cdfs.

```{example log-uniform-cdf-method}

Recall the example in Section \@ref(sim-nonlinear). Let $U$ be a random variable with a Uniform(0, 1) distribution, and let $X=-\log(1-U)$.  We approximated the distribution of $X$ via simulation, and we saw some example calculations in Example \@ref(exm:uniform-log-transform-calcs2) justifying why the distribution looks like it does.  Now we will derive the pdf of $X$.

```

1. Identify the possible values of $X$.  (We have done this already, but this should always be your first step.)
1. Let $F_X$ denote the cdf of $X$. Find $F_X(1)$.
1. Find $F_X(2)$.
1. Find the cdf $F_X(x)$.
1. Find the pdf $f_X(x)$.
1. Why should we not be surprised that $X=-\log(1-U)$ has cdf $F_X(x) = 1 - e^{-x}$?  Hint: what is the function $u\mapsto -\log(1-u)$ in this case? 

```{solution log-uniform-cdf-method-sol}
to Example \@ref(exm:log-uniform-cdf-method)
```

```{asis, fold.chunk = TRUE}


1. As always, first determine the range of possible values.  When $u=0$, $-\log(1-u)=0$, and as $u$ approaches 1, $-\log(1-u)$ approaches $\infty$; see the picture of the function below.  So $X$ takes values in $[0, \infty)$.
1. $F_X(1) = \IP(X \le 1)$, a probability statement involving $X$.  Since we know the distribution of $U$, we express the event $\{X \le 1\}$ as an equivalent event involving $U$.  
\[
\{X \le 1\} = \{-\log(1-U) \le 1\} = \{U \le 1 - e^{-1}\}
\]
The above follows since $-\log(1-u)\le 1$ if and only if $u\le 1-e^{-1}$; see Figure \@ref(fig:log-function-plot) below.  Therefore
\[
F_X(1) = \IP(X \le 1) = \IP(-\log(1-U)\le 1) = \IP(U\le 1-e^{-1})
\]
Now since $U$ has a Uniform(0, 1) distribution, $\IP(U\le u)=u$ for $0<u<1$.  The value $1-e^{-1}\approx 0.632$ is just a number between 0 and 1, so $\IP(U\le 1-e^{-1}) = 1-e^{-1}\approx 0.632$. Therefore $F_X(1)=1-e^{-1}\approx 0.632$.  (This is represented in Figure \@ref(fig:log-uniform-density) by the area of the region from 0 to 1, 63.2%.)
1. Similar to the previous part
\[
F_X(2) = \IP(X \le 2) = \IP(-\log(1-U)\le 2) = \IP(U\le 1-e^{-2}) = 1-e^{-2}\approx 0.865
\]
<!-- The above follows since $-\log(1-u)\le 2$ if and only if $u\le 1-e^{-2}$; see the picture below.  Now since $U$ has a Uniform(0, 1) distribution, $\IP(U\le u)=u$ for $0<u<1$.  The value $1-e^{-2}\approx 0.865$ is just a number between 0 and 1, so $\IP(U\le 1-e^{-2}) = 1-e^{-2}\approx 0.865$. Therefore $F_X(1)=1-e^{-2}\approx 0.865$.  (This is represented in Figure \@ref(fig:log-uniform-density) by the area of the region from 0 to 2, 86.5%.) -->
1. As suggested in the paragraph before the example, the key to finding the pdf is to work with cdfs.  We basically repeat the calculation in the previous steps, but for a generic $x$ instead of 1 or 2. Consider $0\le x<\infty$; we wish to find the cdf evaluated at $x$.
\[
F_X(x) = \IP(X \le x) = \IP(-\log(1-U)\le x) = \IP(U \le 1-e^{-x}) = 1-e^{-x}
\]
The above follows since, for $0<x<\infty$, $-\log(1-u)\le x$ if and only if $u\le 1-e^{-x}$; see Figure \@ref(fig:log-function-plot) below (illustrated for $x=1$).  Now since $U$ has a Uniform(0, 1) distribution, $\IP(U\le u)=u$ for $0<u<1$.  For a fixed $0<x<\infty$, the value $1-e^{-x}$ is just a number between 0 and 1, so $\IP(U\le 1-e^{-x}) = 1-e^{-x}$. Therefore $F_X(x)=1-e^{-x}, 0<x<\infty$.
1. Differentiate the cdf with respective to $x$ to find the pdf.
\[
f_X(x) = F'(x) =\frac{d}{dx}(1-e^{-x}) = e^{-x}, \qquad 0<x<\infty
\]
Thus we see that $X$ has the pdf in Example \@ref(exm:exponential-pdf).
1. The function $Q_X(u) = -\log(1-u)$ is the quantile function (inverse cdf) corresponding to the cdf $F_X(x) = 1-e^{-x}$.  Therefore, since $U$ has a Uniform(0, 1) distribution, the random variable $Q_X(U)$ will have cdf $F_X$ by universality of the Uniform.

```

(ref:cap-log-function-plot) A plot of the function $u\mapsto -\log(1-u)$.  The dotted lines illustrate that $-\log(1-u)\le 1$ if and only if $u\le 1-e^{-1}\approx 0.632$.

```{r, log-function-plot, echo = FALSE, fig.cap="(ref:cap-log-function-plot)"}

g0 = 1
x0 = 1-exp(-g0)
x = seq(0, 0.99999, 0.0001)
gx = -log(1-x)
plot(x, gx, type="l", xlim = c(0, 1), ylim = c(0, 5), xlab="u", ylab="-log(1-u)",xaxs="i",yaxs="i")
segments(x0=x0, y0=0, x1=x0, y1=g0, col="orange", lty=2, lwd=2)
segments(x0=0, y0=0, x1=0, y1=g0, col="skyblue", lty=1, lwd=10)
segments(x0=0, y0=g0, x1=x0, y1=g0, col="skyblue", lty=2, lwd=2)
segments(x0=0, y0=0, x1=x0, y1=0, col="orange", lty=1, lwd=10)

```


If $X$ is a continuous random variable whose distribution is known, the **cdf method** can be used to find the pdf of $Y=g(X)$

- Determine the possible values of $Y$. Let $y$ represent a generic possible value of $Y$.
- The cdf of $Y$ is $F_Y(y) = \IP(Y\le y) = \IP(g(X) \le y)$.
- Rearrange $\{g(X) \le y\}$ to get an event involving $X$. Warning: it is not always $\{X \le g^{-1}(y)\}$.  Sketching a picture of the function $g$ helps.
- Obtain an expression for the cdf of $Y$ which involves $F_X$ and some transformation of the value $y$. 
- Differentiate the expression for $F_Y(y)$ with respect to $y$, and use what is known about $F'_X = f_X$, to obtain the pdf of $Y$.  You will typically need to apply the chain rule when differentiating.

You will need to use information about $X$ at some point in the last step above.  You can either:

- Plug in the *cdf* of $X$ and then differentiate with respect to $y$.
- Differentiate with respect to $y$ and then plug in the *pdf* of $X$.

Either way gets you to the correct answer, but depending on the problem one way might be easier than the other.  We'll illustrate both methods in the next example.

```{example uniform-square-cdf-method}

Let $X$ be a random variable with a Uniform(-1, 1) distribution and Let $Y=X^2$. 

```

1. Sketch the pdf of $Y$.
1. Run a simulation to approximate the pdf of $Y$.
1. Find $F_Y(0.49)$.
1. Use the cdf method to find the pdf of $Y$.  Is the pdf consistent with your simulation results?




```{solution uniform-square-cdf-method-sol}
to Example \@ref(exm:uniform-square-cdf-method)
```

```{asis, fold.chunk = TRUE}



1. First the possible values: since $-1< X<1$ we have $0<Y<1$. The idea to the sketch is that squaring a number less than 1 in absolute value returns a smaller number.  So the transformation "pushes values towards 0" making the density higher near 0.  Consider the intervals $[0, 0.1]$ and $[0.9, 1]$ on the original scale; both intervals have probability 0.05 under the Uniform($-1, 1$) distribution.  On the squared scale, these intervals correspond to $[0, 0.01]$ and $[0.81, 1]$ respectively.  So the 0.05 probability is "squished" into $[0, 0.01]$, resulting in a greater height, while it is "spread out" over $[0.81, 1]$ resulting in a smaller height.  Remember: probability is represented by area.
1. See the simulation results below.  We see that the density is highest near 0 and lowest near 1.
1. Since $X$ can take negative values, we have to be careful; see Figure \@ref(fig:square-function-plot) below.
\[
\{Y \le 0.49\} = \{X^2 \le 0.49\} = \{-\sqrt{0.49} \le X \le \sqrt{0.49}\}
\]
Therefore, since $X$ has a Uniform($-1,1$) distribution,
\[
F_Y(0.49) = \IP(Y \le 0.49) = \IP(-0.7 \le X \le 0.7) = \frac{1.4}{2} = 0.7
\]
1. Fix $0<y<1$.  We now do the same calculation in the previous part in terms of a generic $y$, but it often helps to think of $y$ as a particular number first. 
    \begin{align*}
    F_Y(y) & = \IP(Y\le y)\\
    & = \IP(X^2\le y)\\
    & = \IP(-\sqrt{y}\le X\le \sqrt{y})\\
    & = F_X(\sqrt{y}) - F_X(-\sqrt{y})
    \end{align*}
    Note that the event of interest is *not* just $\{X\le \sqrt{y}\}$; see Figure \@ref(fig:square-function-plot) below. From here we can either

    use the cdf of $X$ and then differentiate, or differentiate and then use the pdf of $X$.  We'll illustrate both.
    (1) Using the Uniform(-1, 1) cdf, the interval $[-\sqrt{y}, \sqrt{y}]$ has length $2\sqrt{y}$, and the total length of $[-1, 1]$ is 2, so we have
    \[
    F_Y(y) = F_X(\sqrt{y}) - F_X(-\sqrt{y}) = \frac{2\sqrt{y}}{2} = \sqrt{y}
    \]
    Now differentiate with respect to the argument $y$ to obtain $f_Y(y) = \frac{1}{2\sqrt{y}}, 0<y<1$.
    (2) Differentiate both sides of $F_Y(y) = F_X(\sqrt{y}) - F_X(-\sqrt{y})$, with respect to $y$.  Differentiating the cdf $F_Y$ yields its pdf $f_Y$, and differentiating the cdf $F_X$ yields its pdf $f_X$.  But don't forget to use the chain rule when differentiating $F_X(\sqrt{y})$.
    \begin{align*}
    F_Y(y) & = F_X(\sqrt{y}) - F_X(-\sqrt{y})\\
        \Rightarrow \frac{d}{dy} F_Y(y) & = \frac{d}{dy}\left(F_X(\sqrt{y}) - F_X(-\sqrt{y})\right)\\
    \qquad f_Y(y) & = f_X(\sqrt{y})\frac{1}{2\sqrt{y}} - f_X(-\sqrt{y})\left(-\frac{1}{2\sqrt{y}}\right)\\
    &= \frac{1}{2\sqrt{y}}\left(f_X(\sqrt{y})+f_X(-\sqrt{y})\right)
    \end{align*}
    Since $X$ has a Uniform(-1, 1) distribution, its pdf is $f_X(x) = 1/2, -1<x<1$.  But for $0<y<1$, $\sqrt{y}$ and $-\sqrt{y}$ are just numbers in $[-1, 1]$, so $f_X(\sqrt{y})=1/2$ and $f_X(-\sqrt{y})=1/2$. Therefore,  $f_Y(y) = \frac{1}{2\sqrt{y}}, 0<y<1$.  The histogram of simulated values seems consistent with this shape.  (The density blows up at 0.)

```

(ref:cap-square-function-plot) A plot of the function $x\mapsto x^2$ for $-1<x<1$.  The dotted lines illustrate that $x^2\le 0.49$ if and only if $-\sqrt{0.49}\le x\le \sqrt{0.49}$.

```{r, square-function-plot, echo = FALSE, fig.cap="(ref:cap-square-function-plot)"}

g0 = 0.49
x0 = sqrt(g0)
x = seq(-1, 1, 0.0001)
gx = x^2
plot(x, gx, type="l", xlab="x", ylab=expression(x^2),xaxs="i",yaxs="i")
segments(x0=x0, y0=0, x1=x0, y1=g0, col="orange", lty=2, lwd=2)
segments(x0=-x0, y0=0, x1=-x0, y1=g0, col="orange", lty=2, lwd=2)
segments(x0=-1, y0=0, x1=-1, y1=g0, col="skyblue", lty=1, lwd=10)
segments(x0=-1, y0=g0, x1=x0, y1=g0, col="skyblue", lty=2, lwd=2)
segments(x0=-x0, y0=0, x1=x0, y1=0, col="orange", lty=1, lwd=10)

```

```{python}

X = RV(Uniform(-1, 1))
Y = X ** 2

Y.sim(10000).plot()

# plot the density
from numpy import *
y = linspace(0.001, 1, 1000)
plt.plot(y, 0.5 / sqrt(y), 'k-');
plt.ylim(0, 10);

plt.show()

```


### Transformations of multiple random variables

Cumulative distribution functions can also be used to derive the joint pdf of multiple random variables.  If $F_{X, Y}$ is the joint cdf of $X$ and $Y$ then the joint pdf of $X$ and $Y$ is

\[
f_{X, Y}(x, y) = \frac{\partial^2}{\partial x\partial y} F_{X, Y}(x, y)
\]

Remember: when taking a partial derivative with respect to one variable, treat the other variables like constants.

```{example, uniform-sum-max-joint-pdf}

Recall the example in Section \@ref(sim-transform-joint). Let $\IP$ be the probability space corresponding to two spins of the Uniform(1, 4) spinner, and let $X$ be the sum of the two spins, and $Y$ the larger spin (or the common value if a tie).

``` 



```{solution uniform-sum-max-joint-pdf-sol}
to Example \@ref(exm:uniform-sum-max-joint-pdf)
```


1. Let $F_{X, Y}$ denote the joint cdf of $X$ and $Y$. Find $F_{X, Y}(3.5, 2)$.
1. Find the joint cdf $F_{X, Y}$.
1. Find the joint pdf $f_{X, Y}$.

```{asis, fold.chunk = TRUE}

1. $F_{X, Y}(3.5, 2) = \IP(X \le 3.5, Y \le 2)$. The $(U_1, U_2)$ pairs take values in the square $[1, 4]\times[1, 4]$.  Figure \@ref(fig:uniform-sum-max-joint-pdf-event) illustrates the event $\{X \le 3.5, Y \le 2\}$.  The shaded region has area $(1)(1)-(1/2)(0.5)(0.5) = 0.875$.  Since $(U_1, U_2)$ pairs are uniformly distributed over the square region with area 9, $\IP(X \le 3.5, Y \le 2) = 0.875 / 9 = 0.0972$.
1. $F_{X, Y}(x, y) = \IP(X \le x, Y \le y)$. We repeat the calculation from the previous part with a generic $(x, y)$.  Let $(x, y)$ be a possible value of $(X, Y)$; that is, $2<x<8$, $1<y<4$, and $y + 1 < x < 2y$. The event $\{X \le x, Y \le y\}$ will have a shape like the one in Figure \@ref(fig:uniform-sum-max-joint-pdf-event), with area $(y-1)^2-(1/2)(2y-x)^2$.  Since $(U_1, U_2)$ pairs are uniformly distributed over the square region with area 9
    \[
    F_{X, Y}(x, y) = (1/9)\left((y-1)^2-(1/2)(2y-x)^2\right), \quad 2 < x < 8, 1 < y < 4, y + 1< x < 2y.
    \]
1. Differentiate the cdf with respect to both $x$ and $y$
    \begin{align*}
        F_{X, Y}(x, y) & = (1/9)\left((y-1)^2-(1/2)(2y-x)^2\right)\\
        \Rightarrow \frac{\partial}{\partial x}F_{X, Y}(x, y) & = \frac{\partial}{\partial x}(1/9)\left((y-1)^2-(1/2)(2y-x)^2\right)\\
        & = (1/9)(2y-x)\\
                \Rightarrow \frac{\partial^2}{\partial x\partial y}F_{X, Y}(x, y) & = \frac{\partial}{\partial y}(1/9)(2y-x)\\
        = 2/9
    \end{align*}
    Therefore
    \[
    f_{X, Y}(x, y) =
      \begin{cases}
    2/9, & 2<x<8,\; 1<y<4,\; x/2<y<x-1,\\
    0, & \text{otherwise}
    \end{cases}
    \]


```


(ref:cap-uniform-sum-max-joint-pdf-event) The event $\{X \le 3.2, Y \le 2\}$ for $X=U_1+U_2$, the sum, and $Y=\max(U_1, U_2)$, the max, of two spins $U_1, U_2$ of a Uniform(1, 4) spinner.


```{r uniform-sum-max-joint-pdf-event, echo=FALSE, fig.cap="(ref:cap-uniform-sum-max-joint-pdf-event)"}

dfA <- data.frame(x = c(2, 2, 1.5, 1, 1),
                 y = c(1, 1.5, 2, 2, 1),
                 v = c(1, 1, 1, 1, 1))

pA <- ggplot(data = dfA, aes(x = x, y = y)) +
  geom_polygon(fill="cornflowerblue", show.legend = FALSE) +
   geom_segment(aes(x = 2.5, y = 1, xend = 1, yend = 2.5), color = "orange", linetype = "dashed") +
  geom_segment(aes(x = 1, y = 2, xend = 2, yend = 2), color = "seagreen", linetype = "dotted") +
  geom_segment(aes(x = 2, y = 1, xend = 2, yend = 2), color = "seagreen", linetype = "dotted") +
  scale_x_continuous(limits = c(1, 4), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(1, 4), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression(U[1])) +
  ylab(expression(U[2])) +
  ggtitle("Event {U1 + U2 <= 3.5, max(U1, U2) <= 2} is shaded") +
  theme(plot.title = element_text(hjust = 0.5))

plot(pA)

# library(ggpubr)
# ggarrange(pA, pB, pC, pD, ncol = 2, nrow = 2)

```



```{example meeting-waiting-uniform-pdf}

Continuing Example \@ref(exm:meeting-waiting-uniform), let $R$ be the random variable representing Regina's arrival time in $[0, 1]$, and $Y$ for Cady. The random variable $T=\min(R, Y)$ represents the time in $(0, 1)$ at which the first person arrives. The random variable $W = |R - Y|$ represents the amount of time the first person to arrive waits for the second person to arrive.  

```

1. Let $F_W$ be the cdf of $W$.  Find $F_{W}(0.25)$.
1. Find the cdf $F_W$.
1. Find the pdf of $W$.  What does this tell you about the distribution of waiting times?
1. Let $F_T$ be the cdf of $T$.  Find $F_{T}(0.25)$.
1. Find the cdf $F_T$.
1. Find the pdf of $T$.  What does this tell you about the time of the first arrival?
1. Are $T$ and $W$ the same random variable?
1. Do $T$ and $W$ have the same distribution?


```{solution meeting-waiting-uniform-pdf-sol}

to Example \@ref(exm:meeting-waiting-uniform-pdf)

```

```{asis, fold.chunk = TRUE}

1. $F_W(0.25) = \IP(W \le 0.25)$.  We computed this in Example \@ref(exm:meeting-waiting-uniform); $F_W(0.25) = 1 - (1-0.25)^2$.  See the plot on the left in Figure \@ref(fig:meeting-waiting-uniform-pdf-plot) below.
1. We repeat the calculation in the previous part for a generic $w$ in $(0, 1)$.
    \[
    F_W(w) = 1 - (1 - w)^2, \quad 0 < w <1
    \]
1. Differentiate the cdf with respect to $w$.
    \[
    f_W(w) = 2(1 - w), \quad 0 < w <1
    \]
    Waiting time has highest density for short waiting times and lowest density for long waiting times.
1. $F_T(0.25) = \IP(T \le 0.25)$.  See the plot on the right in Figure \@ref(fig:meeting-waiting-uniform-pdf-plot) below. $\IP(T \le 0.25) = 1 - (1-0.25)^2$.
1. We repeat the calculation in the previous part for a generic $t$ in $(0, 1)$.
    \[
    F_T(t) = 1 - (1 - t)^2, \quad 0 < t <1
    \]
1. Differentiate the pdf with respect to $t$.
    \[
    f_T(t) = 2(1 - t), \quad 0 < t <1
    \]
    Time of first arrival has highest density near 0 and lowest density near 1.
1. No, $T$ and $W$ are not the same random variable.  For example, if they both arrive at time 0.5, then $T$ is 0.5 but $W$ is 0.
1. Yes, they do have the same distribution.  They have the same pdf (and cdf).

```

(ref:cap-meeting-waiting-uniform-pdf) Illustration of the events $\{W \le 0.25\}$ in Example \@ref(exm:meeting-waiting-uniform-pdf). The square represents the sample space $\Omega=[0,1]\times[0,1]$.


```{r meeting-waiting-uniform-pdf-plot, echo=FALSE, warning=FALSE, message=FALSE, fig.show="hold", out.width="50%", fig.cap="(ref:cap-meeting-waiting-uniform-pdf)"}


dfC <- data.frame(x = c(0, 0.25, 1, 1, 0.75, 0),
                 y = c(0, 0, 0.75, 1, 1, 0.25),
                 v = c(1, 1, 1, 1, 1, 1))

pC <- ggplot(data = dfC, aes(x = x, y = y)) +
  geom_polygon(fill="cornflowerblue", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression(R)) +
  ylab(expression(Y)) +
  ggtitle("Event {|R - Y| < 0.25}") +
  theme(plot.title = element_text(hjust = 0.5))

plot(pC)

dfB <- data.frame(x = c(0.25, 1, 1, 0.25),
                  y = c(0.25, 0.25, 1, 1),
                  v = c(1, 1, 1, 1))

pB <- ggplot(data = dfB, aes(x = x, y = y)) +
  geom_polygon(fill = "white", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(panel.background = element_rect(fill = "cornflowerblue",
                                    colour = "cornflowerblue")) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression(R)) +
  ylab(expression(Y)) +
  ggtitle("Event {min(R, Y) < 0.25}") +
  theme(plot.title = element_text(hjust = 0.5))
  


plot(pB)

```


```{python}

R, Y = RV(Uniform(0, 1) ** 2)

W = abs(R - Y)

T = (R & Y).apply(min)

W.sim(10000).plot()
T.sim(10000).plot()
plt.show()

```




<!-- ## Mixed discrete and continuous random variables -->

<!-- LATER -->

