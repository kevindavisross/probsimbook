# Introduction to Simulation {#simulation}

<!-- \newcommand{\IP}{\textrm{P}} -->
<!-- \newcommand{\IQ}{\textrm{Q}} -->
<!-- \newcommand{\E}{\textrm{E}} -->
<!-- \newcommand{\Var}{\textrm{Var}} -->
<!-- \newcommand{\SD}{\textrm{SD}} -->
<!-- \newcommand{\Cov}{\textrm{Cov}} -->
<!-- \newcommand{\Corr}{\textrm{Corr}} -->
<!-- \newcommand{\Xbar}{\bar{X}} -->
<!-- \newcommand{\Ybar}{\bar{X}} -->
<!-- \newcommand{\xbar}{\bar{x}} -->
<!-- \newcommand{\ybar}{\bar{y}} -->
<!-- \newcommand{\ind}{\textrm{I}} -->
<!-- \newcommand{\dd}{\text{DDWDDD}} -->
<!-- \newcommand{\ep}{\epsilon} -->
<!-- \newcommand{\reals}{\mathbb{R}} -->


A probability model of a random phenomenon consists of a sample space of possible outcomes, associated events and random variables, and a probability measure which specifies probabilities of events and determines distributions of random variables.  **Simulation** involves using a probability model to artificially recreate a random phenomenon, usually using a computer. Given a probability model, we can simulate outcomes, occurrences of events, and values of random variables, according to the specifications of the probability measure.

Recall from Section \@ref(rel-freq) that probabilities can be interpreted as long run relative frequencies.
Therefore the probability of an event can be approximated by simulating, according to the assumptions of the probability model, the random phenomenon a large number of times and computing the relative frequency of repetitions on which the event occurs.
Simulation can be used to approximate probabilities of events, distributions of random variables, long run averages, and other characteristics.

In general, a simulation involves the following steps.

1. **Set up.** Define a probability space, and related random variables and events. The probability measure encodes all the assumptions of the model, but the probability measure is often only specified indirectly.  The set up might be as simple as "flip a fair coin ten times and count the number of heads".  However, even when the set up can be described simply, translating the setup into computer code can be challenging^[In most situations we'll encounter in this book, the "simulate" and "summarize" steps are usually straightforward. In many other cases, these steps can be challenging. Complex set ups often require sophisticated methods, such MCMC (Markov Chain Monte Carlo) algorithms, to efficiently simulate realizations. Effectively summarizing high dimensional simulation output often requires the use of multivariate statistics.].
1. **Simulate.** Simulate --- according to the probability measure --- outcomes, occurrences of events, and values of random variables.
1. **Summarize.** Summarize simulation output in plots and summary statistics (relative frequencies, means, standard deviations, correlations, etc) to describe and approximate probabilities, distributions, and related characteristics.

We also often perform **sensitivity analysis** to investigate how results respond to changes in the assumptions or inputs of the simulation.

You might ask: if we have access to the probability measure, then why do we need simulation to approximate probabilities?
Can't we just compute them?
Remember that the probability measure is often only specified indirectly.
The probability measure represents the underlying assumptions under which probabilities of events of interest are determined.
But in most situations the probability measure does not provide an explicit formula for computing the probability of any particular event.
And in many cases, it is impossible to enumerate all possible outcomes. 
For example, a probabilistic model of a particular Atlantic Hurricane does not provide a mathematical formula for computing the probability that the hurricane makes landfall in the U.S.
Nor does the model provide a comprehensive list of the uncountably many possible paths of the hurricane.
Rather, the model reflects a set of assumptions under which possible paths can be simulated to approximate probabilities of events of interest.

We will see techniques for computing probabilities, but in many situations explicit computation is difficult.
Simulation is a powerful tool for investigating probability models and solving complex problems.

(ref:cap-hurricane-cone) Picture of a "hurricane cone of uncertainty" from the [August 28, 2019 *Washington Post* article "How the hurricane cone of uncertainty can be a cone of confusion, and what to do about it"](https://www.washingtonpost.com/weather/2019/08/28/how-hurricane-cone-uncertainty-can-be-cone-confusion-what-do-about-it/).

```{r hurricane-cone, echo=FALSE, fig.cap="(ref:cap-hurricane-cone)"}

knitr::include_graphics("_graphics/hurricane-cone.png")

```

## Tactile simulation: Boxes and spinners {#tactile}

While we generally use technology to conduct large scale simulations, it is helpful to first consider how we might conduct a simulation by hand using physical objects like coins, dice, cards, or spinners. 

Many random phenomena can be represented in terms of a **"box model^[Our use of "box models" is inspired by [@FPP].]"**

- Imagine a box containing "tickets" with labels.  Examples include:
  - Fair coin flip. 2 tickets: 1 labeled H and 1 labeled T
  - Free throw attempt of a 90\% free throw shooter.  10 tickets: 9 labeled "make" and 1 labeled "miss"
  - Card shuffling.  52 cards: each card with a pair of labels (face value, suit).
<!-- - Random digit dialing.  10 tickets: labeled 0 through 9 (corresponding to single digits). -->
- The tickets are shuffled in the box, some number are drawn out --- either *with replacement or without replacement* of the tickets before the next draw^["With replacement" always implies replacement at a uniformly random point in the box.  Think of "with replacement" as "with replacement and reshuffling" before the next draw.].
- In some cases, the order in which the tickets are drawn matters; in other cases the order is irrelevant.  For example, 
  - Dealing a 5 card poker hand: Select 5 cards without replacement, order does not matter
  - Random digit dialing: Select 4 cards with replacement from a box with tickets labeled 0 through 9 to represent the last 4 digits of a randomly selected phone number with a particular area code and exchange; order matters, e.g., 805-555-1212 is a different outcome than 805-555-2121.
- Then something is done with the tickets, typically to measure random variables of interest.  For example, you might flip a coin 10 times (by drawing from the H/T box 10 times with replacement) and count the number of H.

If the draws are made with replacement from a single box, we can think of a single circular "spinner" instead of a box, spun multiple times. For example:

- Fair coin flip. Spinner with half of the area corresponding to H and half T
- Free throw attempt of a 90\% free throw shooter.  Spinner with 90\% of the area corresponding to "make" and 10% "miss".
<!-- - Random digit dialing.  Spinner marked with digits 0-9, possibly with some digits more likely than others.  spun multiple times.  Depending on what regions you are trying to sample, you might have three spinners: one to generate area code, one to generate the next three digits, and one to generate the last four digits. -->


```{example dice-sim}

Let $X$ be the sum of two rolls of a fair four-sided die, and let $Y$ be the larger of the two rolls (or the common value if a tie).  Set up a box model and explain how you would use it to simulate a single realization of $(X, Y)$.  Could you use a spinner instead?
  
```
 

```{solution dice-sim-sol}

to Example \@ref(exm:dice-sim)

```

```{asis, fold.chunk = TRUE}

Use a box with four tickets, labeled 1, 2, 3, 4.  Draw two tickets with replacement.  Let $X$ be the sum of the two numbers drawn and $Y$ the larger of the two numbers drawn.

It's also possible to use a spinner with 4 sectors, corresponding to 1, 2, 3, 4, each with 25% of the total area; see Figure \@ref(fig:spinner-die).  Spin the spinner twice. Let $X$ be the sum of the two numbers spun and $Y$ the larger of the two numbers spun.

```

(ref:cap-spinner-die) Spinner corresponding to a single roll of a fair four-sided die.

```{r spinner-die, echo=FALSE, fig.cap="(ref:cap-spinner-die)", fig.width=8}

knitr::include_graphics("_graphics/spinner-die.png")

```

The spinner in Figure \@ref(fig:spinner-die) simulates the individual die rolls.  We will see later spinners for generating values of $X$, values of $Y$, and values of $(X, Y)$ pairs directly.

Note that we are able to simulate outcomes of the rolls and values of $X$ and $Y$ without defining the probability space in detail.  That is, we do not need to list all the possible outcomes and events and their probabilities. Instead, the probability space is defined implicitly via the specification to "roll a fair four-sided die twice" or "draw two tickets with replacement from a box with four tickets labeled 1, 2, 3, 4" or "spin the spinner in Figure \@ref(fig:spinner-die) twice". The random variables are defined by what is being measured for each outcome, the sum ($X$) and the max ($Y$) of the two draws or spins.

In Example \@ref(exm:dice-sim) we described how to simulate a single realization of $(X, Y)$; this is one "repetition" in the simulation.
A simulation general entails many repetitions.
When conducting simulations it is important to distinguish between what entails (1) one repetition of the simulation and its output, and (2) the simulation itself and output from many repetitions.



```{example matching-box-sim, name='Matching problem'}
Rocks labeled 1, 2, 3, 4, are placed at random in spots labeled 1, 2, 3, 4, with spot 1 the correct spot for rock 1, etc.
Suppose that the rocks are equally likely to be placed in any spot.
Let $Y$ be the number of rocks that are placed in the correct spot,
and let $C$ be the event that at least one rock is placed in the correct spot.
Describe how you would use a box model to simulate a single realization of $Y$ and of $C$.


```
 

```{solution matching-box-sim-sol}

to Example \@ref(exm:matching-box-sim)

```

```{asis, fold.chunk = TRUE}

Use a box with 4 tickets, labeled 1, 2, 3, 4.
Shuffle the tickets and draw all 4 *without* replacement and record the tickets drawn in order.
Let $Y$ be the number of tickets that match their spot in order.
(For example, if the tickets are drawn in the order 2431 then the realized value of $Y$ is 1 since only ticket 3 matches its spot in the order.)

Since $C=\{Y \ge 1\}$, event $C$ occurs if $Y\ge 1$ and does not occur if $Y=0$.
We could record the realization of event $C$ as "True" or "False".
We could also record the realization of $I_C$, the indicator random variable for event $C$, as 1 if $C$ occurs and 0 if $C$ does not occur.

``` 



### Exercises


1. Flip a fair coin 3 times and record the results in sequence.  Let $X$ be the number of flips that result in H. Let $Y$ be the number of flips that result in T, and let $Z$ be the length of the longest streak of H in a row (which could be 0 if all T or 1 if no H is followed by H). Describe how you would use a box model to simulate a single realization of $(X, Y, Z)$.

1. Two players, A and B, play a single game of [rock, paper, scissors (RPS)](https://en.wikipedia.org/wiki/Rock%E2%80%93paper%E2%80%93scissors).
One possible probability measure corresponds to each player being equally like to choose between rock, paper, and scissors, and the choices of the two players being independent.
Describe how you would use a box model to simulate a single realization of $A$, the event that player $A$ wins the game.
    
1. The probability measure in the previous exercise assumed equally likely outcomes.
However, [a recent rock, paper, scissors tournament conducted by FiveThirtyEight](https://fivethirtyeight.com/features/what-are-the-odds-world-cup-teams-play-each-other-twice/) suggests that the following might be a more reasonable probability measure reflecting how people actually play.
Suppose that the probability that player A throws rock is 0.319, and 0.402 for paper.
Suppose that the probability that player B throws rock is 0, and 0.75 for paper.
Also suppose that the probability that both players throw paper is 0.3015 and that both players throw scissors is 0.06975 (which corresponds to assuming that they make their choices independently).
Describe how you would use a box model to simulate a single realization of $A$, the event that player $A$ wins the game.


1. In Example \@ref(exm:collector-outcome), suppose we continue to purchase packages until we obtain a complete set of prizes and then we stop.
Suppose that each package is equally likely to contain any of the 3 prizes, regardless of the contents of other packages.
Let $A$ be the event that we purchase more than 10 packages, and let $Y$ be the number of packages than contain prize 1.
Describe how you would use a box model to simulate a single realization of $(\ind_A, Y)$.


## Relative frequencies


A simulation involves repeatedly artificially recreating the random phenomenon a large number of times and using the results to investigate properties of interest.
In particular, we can use simulation-based relative frequencies to approximate probabilities.
That is, the probability of event $A$ can be approximated by simulating, according to the assumptions corresponding to the probability measure $\IP$, the random phenomenon a large number of times and computing the relative frequency of $A$.

\[
{\small
\IP(A) \approx \frac{\text{number of repetitions on which $A$ occurs}}{\text{number of repetitions}}, \quad \text{for a large number of repetitions simulated according to $\IP$}
}
\]


In practice, many repetitions of a simulation are performed on a computer to approximate what happens in the "long run".
However, we often start by carrying out a few repetitions by hand to help make the process more concrete. 

```{example dice-sim-tactile}

Use a four-sided die (or a box or a spinner) and perform by hand 10 repetitions of the simulation in Example \@ref(exm:dice-sim).
(Yes, really do it.)
For each repetition, record the results of the first and second rolls (or draws or spins) and the values of $X$ and $Y$.
Based only on the results of your simulation, how would you approximate the following?
(Don't worry if the approximations are any good yet.)

```

1. $\IP(A)$, where $A$ is the event that the first roll is 3.
1. $\IP(X=6)$
1. $\IP(X \ge 6)$
1. $\IP(Y = 3)$
1. $\IP(Y \ge 3)$
1. $\IP(X=6, Y=3)$
1. $\IP(X\ge6, Y \ge 3)$


```{solution dice-sim-tactile-sol}

to Example \@ref(exm:dice-sim-tactile).  


```

```{r, echo = FALSE}

u1 = c(2, 1, 3, 4, 3, 3, 2, 2, 1, 3)
u2 = c(1, 1, 3, 3, 2, 4, 3, 4, 2, 4)
x = u1 + u2
y = pmax(u1, u2)
A = ifelse(u1 == 3, "True", "False")
IA = as.numeric(u1 == 3)

die_df = data.frame(1:10, u1, u2, x, y, A, IA)


```

```{asis, fold.chunk = TRUE}

See Table \@ref(tab:dice-sim-tactile-results) for the results of our simulation.

1. Approximate $\IP(A)$ by 4/10, the relative frequency of event $A$ in the simulation; that is, the proportion of repetitions where the first roll is 3.
1. Approximate $\IP(X=6)$ by 2/10, the proportion of repetitions where the sum is 6.
1. Approximate $\IP(X\ge 6)$ by 5/10, the proportion of repetitions where the sum is at least 6.
1. Approximate $\IP(Y=3)$ by 3/10, the proportion of repetitions where the max is 3.
1. Approximate $\IP(Y\ge 3)$ by 7/10, the proportion of repetitions where the max is at least 3.
1. Approximate $\IP(X=6, Y = 3)$ by 1/10, the proportion of repetitions where both the sum is 6 and the max is 3.
1. Approximate $\IP(X\ge 6, Y \ge 3)$ by 5/10, the proportion of repetitions where both the sum is at least 6 and the max is at least 3.
(Since $X\ge 6$ implies $Y\ge 3$, $\IP(X\ge 6, Y\ge 3) = \IP(X\ge 6)$.)

```


Table \@ref(tab:dice-sim-tactile-results) summarizes the results of 10 repetitions of the simulation in Example \@ref(exm:dice-sim-tactile).  Results vary naturally so your simulation results will be different, but the same ideas apply.

```{r, dice-sim-tactile-results, echo = FALSE}


knitr::kable(
  die_df, booktabs = TRUE,
  col.names = c("Repetition", "First roll", "Second roll", "X", "Y", "Event A occurs?", expression(I[A])),
  caption = "Results of 10 repetitions of two rolls of a fair four-sided die. X is the sum of the two rolls, Y is the maximum, and A is the event that the first roll is a 3."
)

```




Remember that it is important to distinguish between what entails (1) one repetition of the simulation and its output, and (2) the simulation itself and output from many repetitions.  When describing a simulation, refrain from making vague statements like "repeat this" or "do it again", because "this" or "it" could refer to different elements of the simulation. In the dice example, (1) rolling a die is repeated to generate a single $(X, Y)$ pair, and (2) the process of generating $(X, Y)$ pairs is repeated to obtain the simulation^[Do we perform "a simulation", or "many simulations"? Throughout, "a simulation" refers to the collection of results corresponding to repeatedly artificially recreating the random process.  "A repetition" refers to a single artificial recreation resulting in a single simulated outcome.  We perform a simulation which consists of many repetitions.] results.  That is, a single repetition involves an ordered pair of die rolls, resulting in an outcome $\omega$, and the values of the sum $X(\omega)$ and max $Y(\omega)$ are computed for the outcome $\omega$.  The process described in the previous sentence is repeated many times to generate many outcomes and $(X, Y)$ pairs according to the probability model.

Think of simulation results being organized in a table like Table \@ref(tab:dice-sim-tactile-results), where each row corresponds to a different repetition of the simulation and each column corresponds to a different random variable or event.  Remember that indicators are the bridge between events and random variables.  On each repetition of the simulation an event either occurs or not.  We could record the occurrence of an event as "True/False" for each repetition, or we could record the 1/0 value of the corresponding indicator random variable; see the last two columns in Table \@ref(tab:dice-sim-tactile-results) for an example.


```{example dice-sim-tactile-dist}
Continuing Example \@ref(exm:dice-sim-tactile), specify how you would use your simulation results (just the 10 repetitions) to approximate:

1. The distribution of $X$.
1. The joint distribution of $(X, Y)$.

```

```{solution dice-sim-tactile-dist-sol}

to Example \@ref(exm:dice-sim-tactile-dist).  


```


```{asis, fold.chunk = TRUE}

For discrete random variables like these we could make tables or plots summarizing the observed values of the random variables and their corresponding relative frequencies.

Summarizing our simulation results from Table \@ref(tab:dice-sim-tactile-results), the observed values of $X$ and corresponding relative frequencies are

| $x$ | Relative frequency |
|-----|-------------------:|
| 2   |               1/10 |
| 3   |               2/10 |
| 4   |                  0 |
| 5   |               2/10 |
| 6   |               2/10 |
| 7   |               3/10 |
| 8   |                  0 |
  
The above table^[We would typically only include the values observed in the simulation in the summary. However, we include 4 and 8 here because if we had performed more repetitions we would have observed these values.] represents an approximation of the distribution of $X$ (albeit a bad approximation; compare with Table \@ref(tab:dice-sum-dist-table).

To approximate the joint distribution of $(X, Y)$ we need to summarize the simulated $(X, Y)$ *pairs* and their relative frequencies, as in the following two-way table (compare with Table \@ref(tab:dice-joint-dist-twoway).

| $x, y$ |    1 |    2 |    3 |    4 |
|--------|-----:|-----:|-----:|-----:|
| 2      | 1/10 |    0 |    0 |    0 |
| 3      |    0 | 2/10 |    0 |    0 |
| 4      |    0 |    0 |    0 |    0 |
| 5      |    0 |    0 | 2/10 |    0 |
| 6      |    0 |    0 | 1/10 | 1/10 |
| 7      |    0 |    0 |    0 | 3/10 |
| 8      |    0 |    0 |    0 |    0 |

  
We could also visualize approximate distributions in plots like those in \@ref(fig:dice-sim-tactile-results-plot).

```

Simulation results are summarized in tables and plots. Figure \@ref(fig:dice-sim-tactile-results-plot) displays two plots summarizing the results in Table \@ref(tab:dice-sim-tactile-results). Each dot represents the results of one repetition; the plot on the left displays the simulated $(X, Y)$ pairs, and the plot on the right displays the simulated values of $X$ alone along with their frequencies. While this simulation only consists of 10 repetitions, a larger scale simulation and the summarization of results would follow the same process.


(ref:cap-dice-sim-tactile) Plot summaries of the simulation results in Table \@ref(tab:dice-sim-tactile-results) of 10 repetitions of two rolls of a fair four-sided die, where $X$ is the sum and $Y$ is the larger (or common value if a tie) of the two rolls.

```{r dice-sim-tactile-results-plot, echo = FALSE, fig.show = 'hold', out.width = '50%', fig.cap = "(ref:cap-dice-sim-tactile)"}

plot(x + c(0, 0, 0, 0, 0, 0.1, 0.1, 0, 0.1, -0.1), y,
     xlab = "X", ylab = "Y", xaxt = 'n', yaxt = 'n',
     xlim = c(1.5, 8.5), ylim = c(0.5, 4.5), xaxs = "i", yaxs = "i")
segments(x0 = 2.5:7.5, y0 = 0, x1 = 2.5:7.5, y1 = 5, lty = 3)
segments(x0 = 1.5, y0 = 0.5:4.5, x1 = 8.5, y1 = 0.5:4.5, lty = 3)
axis(1, at = 2:8, tck = 0)
axis(2, at = 1:4, tck = 0)


stripchart(x, method = "stack",
           xaxt = 'n', xlim = c(2, 8),
           offset = .5, at = .15, pch = 1, 
           xlab = "X", ylab = "Number of repetitions")
axis(1, at = 2:8)


```


You might have noticed that many of the simulated relative frequencies in Example \@ref(exm:dice-sim-tactile) provide terrible estimates of the corresponding probabilities.
For example, the true probability that the first roll is a 3 is $\IP(A) = 0.25$ while the simulated relative frequency is 0.4.
The problem is that the simulation only consisted of 10 repetitions.
Probabilities can be approximated by *long run* relatively frequencies, but 10 repetitions certainly doesn't qualify as the long run!
The more repetitions we perform the better our estimates should be.
But how many repetitions is sufficient?
And how accurate are the estimates?
We will address these issues in Section \@ref(moe).



### Exercises


1. Flip a fair coin 4 times and let $X$ be the number of H.

    a. Specify how to use a box model to simulate a single value of $X$, with tickets labeled H and T.
    a. Specify how to use a box model to simulate a single value of $X$, using tickets that are labeled with appropriate numbers (not H and T) and without counting.
    a. Specify how to use simulation to approximate $\IP(X = 3)$. 
    a. Specify how to use simulation to approximate the distribution of $X$.

1. In Example \@ref(exm:collector-outcome), suppose we continue to purchase packages until we obtain a complete set of prizes and then we stop.
Suppose that each package is equally likely to contain any of the 3 prizes, regardless of the contents of other packages.
Let $X$ be the number of packages we purchase, and let $Y$ be the number of packages than contain prize 1.
Describe how you would use a box model and simulation to approximate

    a. $\IP(X > 3Y)$.
    a. The distribution of $X$.
    a. The joint distribution of $(X, Y)$.



## Averages {#LRA}

On any single repetition of the simulation a particular event either occurs or not.  Summarizing simulation results for events involves simply counting the number of repetitions on which the event occurs and finding related proportions.

On the other hand, random variables typically take many possible values over the course of many repetitions.  We are still interested in relative frequencies of events, like $\{X=6\}$ and $\{Y \ge 3\}$ in the die example.  But for random variables we are also interested in their distributions which describe the possible values that the random variables can take and their relative likelihoods.  While the distribution contains all the information about a random variable, it is also useful to summarize some key features of a distribution. For example, probabilities of particular events concerning a random variable can be interpreted as long run relative frequencies.

One summary characteristic of a distribution is the **long run average value** of the random variable.  We can approximate the long run average value by simulating many values of the variable and computing the average (mean) in the usual way.

```{example dice-sim-tactile-ev}

Recall your tactile simulation from Example \@ref(exm:dice-sim-tactile). Based only on the results of your simulation, approximate the long run average value of each of the following.
(Don't worry if the approximations are any good yet.)

```

1. $X$
1. $Y$
1. $X^2$
1. $XY$


```{solution dice-sim-tactile-ev-sol}

to Example \@ref(exm:dice-sim-tactile-ev).  


```

```{r, echo = FALSE}

u1 = c(2, 1, 3, 4, 3, 3, 2, 2, 1, 3)
u2 = c(1, 1, 3, 3, 2, 4, 3, 4, 2, 4)
x = u1 + u2
y = pmax(u1, u2)

die_df = data.frame(1:10, u1, u2, x, y, x ^ 2, x * y)


```

<!-- NEED TO FIGURE OUT HOW TO SHOW/HIDE THE STUFF BELOW THAT HAS BOTH R AND TEXT -->

Our simulation results are in Table \@ref(tab:dice-sim-tactile-results-ev) below.

1. Approximate the long run average value of $X$ by summing the 10 simulated values of $X$ and dividing by 10.
\[
\frac{`r paste(x, collapse=" + ")`}{10} = `r round(mean(x), 3)`
\]
1. Approximate the long run average value of $Y$ by summing the 10 simulated values of $Y$ and dividing by 10.
\[
\frac{`r paste(y, collapse=" + ")`}{10} = `r round(mean(y), 3)`
\]
1. First, for each repetition square the value of $X$ to obtain the $X^2$ column. Then approximate the long run average value of $X^2$ by summing the 10 simulated values of $X^2$ and dividing by 10.
\[
\frac{`r paste(x ^ 2, collapse=" + ")`}{10} = `r round(mean(x ^ 2), 3)`
\]
<!-- paste(paste("(", x, ")^2", sep=""), collapse = "+") -->
<!-- expression(paste(x ^"2", sep="")) -->
1. First, for each repetition compute the product $XY$ to obtain the $XY$ column. Then approximate the long run average value of $XY$ by summing the 10 simulated values of $XY$ and dividing by 10.
\[
\frac{`r paste(x * y, collapse=" + ")`}{10} = `r round(mean(x * y), 3)`
\]
<!-- paste(paste("(",x, ")(", y, ")", sep=""), collapse = " + ") -->




We reproduce the results of our simulation in 
Table \@ref(tab:dice-sim-tactile-results-ev) with additional columns for $X^2$ and $XY$. Results vary naturally so your simulation results will be different, but the same ideas apply.

```{r, dice-sim-tactile-results-ev, echo = FALSE}


knitr::kable(
  die_df, booktabs = TRUE,
  col.names = c("Repetition", "First roll", "Second roll", "X", "Y", expression(X^2), "XY"),
  caption = "Results of 10 repetitions of two rolls of a fair four-sided die"
)

```


Of course, 10 repetitions is not enough to reliably approximate the *long run* average value.
But whether the average is based on 10 values or 10 million, an average is computed in the usual way: sum the values and divide by the number of values.

```{example dd-lra}
Donny Don't says: "Why bother creating columns for $X^2$ and $XY$? If I want to find the average value of $X^2$ I can just square the average value of $X$. For the average value of $XY$ I can just multiply the average value of $X$ and the average value of $Y$." Do you agree?  (Check to see if this works for your simulation results.) If not, explain why not.
```

```{solution dd-lra-sol}
to Example \@ref(exm:dd-lra)
```




It is easy to check that Donny is wrong just by inspecting the simulation results: `r round(mean(x), 3)`^2^ $\neq$ `r round(mean(x ^ 2), 3)`,
`r round(mean(x), 3)` $\times$ `r round(mean(y), 3)`  $\neq$ `r round(mean(x * y), 3)`.
<!-- $`r paste(round(mean(x), 3), sep = "")`^2 \neq `r paste(round(mean(x ^ 2), 3), sep = "")`$, $`r paste(round(mean(x), 3), sep = "")`\times `r paste(round(mean(y), 3), sep = "")`  \neq `r paste(round(mean(x * y), 3), sep = "")`$. -->
To see why, suppose we had just performed two repetitions, resulting in the first two rows of Table \@ref(tab:dice-sim-tactile-results-ev).
\[
\text{Average of $X^2$} = \frac{3^2 + 2^2}{2} =6.5 \neq 6.25= \left(\frac{3 + 2}{2}\right)^2=(\text{Average of $X$})^2
\]
Squaring first and then averaging (which yields 6.5) is not the same as averaging first and then squaring (which yields 6.25), essentially because $(3+2)^2\neq 3^2 + 2^2$.

Similarly,
\[
{\small
\text{Average of $XY$} = \frac{(3)(2) + (2)(1)}{2} =4 \neq 3.75= \left(\frac{3 + 2}{2}\right)\left(\frac{2 + 1}{2}\right)=(\text{Average of $X$})\times (\text{Average of $Y$})
}
\]
Multiplying first and then averaging (which yields 4) is not the same as averaging first and then multiplying (which yields 3.75), essentially because $(3)(2)+(2)(1)\neq(3+2)(2+1)$.



In general the order of transforming and averaging is not interchangeable.
Whether in the short run or the long run, in general
\begin{align*}
\text{Average of $g(X)$} & \neq g(\text{Average of $X$})\\
\text{Average of $g(X, Y)$} & \neq g(\text{Average of $X$}, \text{Average of $Y$})
\end{align*}

Many common mistakes in probability result from not heeding this principle, so we will introduce many related examples to help you practice your understanding.

The long run average value is just one characteristic of the distribution of a random variable.
We are also interested in percentiles, degree of variability, and quantities that measure relationships between random variables.
We will investigate these ideas in later sections.

Next we introduce a few useful properties of averages.

### Linearity of averages


```{example dice-sim-tactile-ev-linearity}

Recall your tactile simulation from Example \@ref(exm:dice-sim-tactile).
Let $U_1$ be the result of the first roll, and $U_2$ the result of the second.

```

1. Donny Don't says: "$X=U_1+U_2$, so I can find the average value of $X$ by finding the average value of $U_1$, the average value of $U_2$, and adding the two averages".  Do you agree? Explain.
1. Donny Don't says: "$U_1$ and $U_2$ have the same distribution, so they have the same average value, so I can find the average value of $X$ by multiplying the average value of $U_1$ by 2". Do you agree? Explain.
1. Donny Don't says: "$U_1$ and $U_2$ have the same distribution, so $X=U_1+U_2$ has the same distribution as $2U_1 = U_1 + U_1$". Do you agree?  Explain.



```{solution dice-sim-tactile-ev-linearity-sol}

to Example \@ref(exm:dice-sim-tactile-ev-linearity).  


```


<!-- NEED TO FIGURE OUT HOW TO SHOW/HIDE THE STUFF BELOW THAT HAS BOTH R AND TEXT -->

1. Donny is correct! Our simulation results are in Table \@ref(tab:dice-sim-tactile-results-ev).
The average value of $U_1$ is
\[
\frac{`r paste(u1, collapse=" + ")`}{10} = `r round(mean(u1), 3)`
\]
The average value of $U_2$ is
\[
\frac{`r paste(u2, collapse=" + ")`}{10} = `r round(mean(u2), 3)`
\]
The sum of these two values is equal to the average value of $X$.
To see why, suppose we had just performed two repetitions, resulting in the *last* two rows of Table \@ref(tab:dice-sim-tactile-results-ev).
\[
{\scriptsize
\text{Average of $(U_1+U_2)$} = \frac{(1 + 2) + (3 + 4)}{2} = 5 = 2 + 3= \left(\frac{1 + 3}{2}\right)+\left(\frac{2 + 4}{2}\right) = (\text{Average of $U_1$}) + (\text{Average of $U_1$}) 
}
\]
We discuss further below.
1. Donny is correct that $U_1$ and $U_2$ have the same distribution, and he has some good ideas about averages.
But we should remind Donny that a distribution represents the *long run* pattern of variability.
With only 10 repetitions, the results for $U_1$ will not necessarily follow the same pattern as those for $U_2$.
In our simulation, the average value of $U_1$ is `r mean(u1)` and the average value of $U_2$ is `r mean(u2)`.
Multiplying neither one of these numbers by 2 yields the average value of $X$.
    Donny would have been correct if he were talking about *long run* average values. Since $U_1$ and $U_2$ have the same distribution, the long run average value of $U_1$ is equal to the long run average value of $U_2$, and so the long run average value of $X$ is equal to the long run average value of $U_1$ multiplied by two.
1. Donny is not correct.  In particular, $X$ and $2U_1$ do not have the same possible values; for example, $X$ can be 3 but $2U_1$ cannot.
The long run average value is just one feature of a distribution.
Just because $X$ and $2U_1$ have the same long run average value does not necessarily mean they have the same full long run pattern of variability.
In particular, relationships between random variables will affect distributions of transformations of them.
$U_1$ and $U_2$ have the same marginal distribution, but the joint distribution of $(U_1, U_2)$ is not the same as that of $(U_1, U_1)$, and so the distribution of $U_1+U_2$ is not the same as that of $U_1+U_1$.



In general the order of transforming and averaging is not interchangeable.
However, the order is interchangeable for *linear* transformations.
If $X$ and $Y$ are random variables and $a$ and $b$ are non-random constants,
whether in the short run or the long run,
\begin{align*}
\text{Average of $a+bX$} & = a+b(\text{Average of $X$})\\
\text{Average of $X+Y$} & = \text{Average of $X$} +\text{Average of $Y$}
\end{align*}
These properties are referred to as **linearity of averages.**
Averaging involves adding and dividing.
Linear transformations involve only adding/subtracting and multiplying/dividing.
The ability to interchange the order of averaging and *linear* transformations follows simply from basic properties of arithmetic (commutative, associative, distributive).

Note that the average of the sum of $X$ and $Y$ is the sum of the average of $X$ and the average of $Y$ *regardless of the relationship between $X$ and $Y$.*
We will explore this idea in more detail later.

Remember that the long run average value is just one feature of a distribution.
There is much more to the long run pattern of variability of a random variable that just its average value.
Two random variables can have the same long run average value but very different distributions.

### Averages of indicator random variables

Recall that indicators are the bridge between events and random variables.
Indicators are also the bridge between relative frequencies and averages.


```{example dice-sim-tactile-ev-indicator}

Recall your tactile simulation from Example \@ref(exm:dice-sim-tactile).
Let $A$ be the event that the first roll is 3 and $\ind_A$ the corresponding indicator random variable.
Based only on the results of your simulation, approximate the long run average value of each of $\ind_A$.
What do you notice?

```



```{solution dice-sim-tactile-ev-indicator-sol}

to Example \@ref(exm:dice-sim-tactile-ev-indicator).  


```

```{r, echo = FALSE}

indA = as.numeric(u1 == 3)

```

<!-- NEED TO FIGURE OUT HOW TO SHOW/HIDE THE STUFF BELOW THAT HAS BOTH R AND TEXT -->

Our simulation results are in Table \@ref(tab:dice-sim-tactile-results-ev).
Approximate the long run average value of $\ind_A$ by summing the 10 simulated values of $\ind_A$ and dividing by 10.
\[
\frac{`r paste(indA, collapse=" + ")`}{10} = \frac{`r sum(indA)`}{10}
\]
The average of $\ind_A$ is the relative frequency of event $A$!
As we discussed in Section \@ref(indicators), when we sum the 1/0 values of $\ind_A$ we count the repetitions on which $A$ occurs.
That is, the numerator in the average calculation for $\ind_A$ is the frequency of event $A$, and dividing by the number of repetitions yields the relative frequency of event $A$. 



If $\ind_A$ is the indicator random variable of an event $A$, whether in the short run or the long run,
\begin{align*}
\text{Average of $\ind_A$} & = \text{Relative frequency of $A$}
\end{align*}


### Exercises

1. Flip a fair coin 4 times and let $X$ be the number of H.
Specify how you could use simulation to approximate the long run average value of

    a. $X$ 
    a. $X^2$
    a. $\ind_{\{X = 3\}}$
    a. $Y$, the number of tails.  How is the long run average of $Y$ related to the long run average value of $X$ when the coin is fair?  How is the long run average of $Y$ related to the long run average value of $X$ when the coin is *not* fair?
    a. $Z$, the length of the longest streak of H in a row (which could be 0 if all T or 1 if no H is followed by H). Also, without doing any calculations determine if the long run average value of $Z$ will be greater than, less than, or equal to the long run average value of $X$. 

1. In Example \@ref(exm:collector-outcome), suppose we continue to purchase packages until we obtain a complete set of prizes and then we stop.
Suppose that each package is equally likely to contain any of the 3 prizes, regardless of the contents of other packages.
Let $X$ be the number of packages we purchase, and let $Y$ be the number of packages than contain prize 1.
Specify how you could use simulation to approximate the long run average value

    a. $X$. Also, without doing any calculations determine if the long run average value of $X$ will be greater than, less than, or equal to 3. 
    a. $Y$. Also, without doing any calculations determine if the long run average value will be greater than, less than, or equal to the long run average value of $X$. 
    a. $XY$
    a. $\ind_{\{X > 3Y\}}$.

1. Let $X$ be the amount of rainfall in inches in a randomly selected day in a particular city. Suppose that the long run average value of $X$ is 0.5 inches.

    a. Let $W$ be the total amount of rainfall in inches in a randomly selected *week* in the city.  Find the long run average value of $W$.  Does it matter that the amount of rainfall might not be independent from day-to-day?
    a. Let $Y$ be the amount of rainfall in *millimeters* in a randomly selected day in the city (1 inch = 25.4 mm). Find the long run average of $Y$.  Is the distribution of $Y$ the same as the distribution of $X$?
    a. Suppose that it in the long run it rains on 10\% of days in the city. Let $A$ be the event that it rains in the city on a randomly selected day.  Find the long run average value of $\ind_A$.




## Computer simulation: Symbulate {#technology-intro}


**Note: some of the plots and tables in this and the following sections do not appear exactly as they would in Jupyter or Colab notebooks.  See the accompanying notebooks for a better representation of Symbulate output.**


We will perform computer simulations using the Python package Symbulate. The syntax of Symbulate mirrors the language of probability in that the primary objects in
Symbulate are the same as the primary components of a probability model: probability spaces, random variables, events.  Once these components are specified, Symbulate allows users to simulate many times from the 
probability model and summarize the results.


This section contains a brief introduction to Symbulate; many more examples can be found throughout the text or in the [Symbulate documentation](https://dlsun.github.io/symbulate/index.html). Remember to first import Symbulate during a Python session using the command


```{python, eval = FALSE}
from symbulate import *
```


Throughout Section \@ref(technology-intro) we illustrate Symbulate commands and output in the context of Example \@ref(exm:dice-sim) from Section \@ref(tactile).
Unless indicated otherwise, in this section $X$ represents the sum of two rolls of a fair four-sided die, and $Y$ represents the larger of the two rolls (or the common value if a tie).
We have already discussed the simulation process in this situation. Now we 
have to implement that process on a computer.

### Simulating outcomes

The following Symbulate code defines a probability space^[Technically, a probability space is a triple $(\Omega, \mathcal{F}, \IP)$. We primarily view a Symbulate probability space as a description of the probability model rather than an explicit specification of $\Omega$.  For example, we define a `BoxModel` instead of creating a set with all possible outcomes.  We tend to represent a probability space with `P`, even though this is a slight abuse of notation.] `P` for simulating the 16 equally likely ordered pairs of rolls via a box model. 

```{python dice-sym-boxmodel}

P = BoxModel([1, 2, 3, 4], size = 2, replace = True)

```

The above code tells Symbulate to draw 2 tickets (`size = 2`), with replacement^[The default argument for `replace` is `True`, so we could have just written `BoxModel([1, 2, 3, 4], size = 2)`.], from a box with tickets labeled 
1, 2, 3, and 4 (entered as the Python list `[1, 2, 3, 4]`). Each simulated outcome consists of an ordered^[There is an additional argument `order_matters` which defaults to `True`, but we could set it to false for unordered pairs.] pair of rolls.  The `sim(n)` command simulates `n` realizations of probability space outcomes (or events or random variables).

```{python}

P.sim(10)

```

### Simulating random variables

A Symbulate `RV` is specified by the probability space on which it is defined and the mapping function which defines it.  Recall that $X$ is the sum of the two dice rolls and $Y$ is the larger (max).

```{python dice-sym-rv}
X = RV(P, sum)
Y = RV(P, max)

```


The above code simply defines the random variables.  Since a random variable $X$ is a function, any `RV` can be called as a function^[The warning you get when you call a `RV` as a function just means that Symbulate is not going to check for you that the inputs to the function you used to define the `RV` actually match up with the outcomes of the probability space `P`.] to return its value $X(\omega)$ for a particular outcome $\omega$ in the probability space.

```{python}
omega = (3, 2)  # a pair of rolls
X(omega), Y(omega)

```



The following commands simulate 100 values of the random variable `Y` and store the results as `y`.  For consistency with standard probability notation^[We generally use names in our code that mirror and reinforce standard probability notation, e.g., uppercase letters near the end of the alphabet for random variables, with corresponding lowercase letters for their realized values.  Of course, these naming conventions are not necessary and you are welcome to use more descriptive names in your code.  For example, we could have named the probability space `DiceRolls` and the random variables `DiceSum` and `DiceMax` rather than `P, X, Y`.], the random variable itself is denoted with an uppercase letter `Y`, while the realized values of it are denoted with a lowercase letter `y`.





```{python}
y = Y.sim(100)
y # this just diplays y

```


### A few commands for summarizing simulation output

Values and their frequencies can be summarized using `tabulate`.

```{python}
y.tabulate()

```

By default, `tabulate` returns frequencies (counts). Adding the argument^["Normalize" is used in the sense of Section \@ref(consistency) and refers to rescaling the values so that they add up to 1 but the ratios are preserved.] `normalize = True` returns relative frequencies (proportions).  

```{python}
y.tabulate(normalize = True)

```

Methods like `sim` and `tabulate` can be chained together in a single line of code. 

```{python}
Y.sim(100).tabulate(normalize = True)

```

Each call to `sim` reruns the simulation to generate a new set of simulated values.  To perform multiple operations on a single set of simulated values, store the simulation results as a variable (like `y` above).  When running `Y.sim(100)` Symbulate simulates, in the background, outcomes from the probability space `P` and then computes `Y` for these outcomes; however, the outcomes themselves are not saved.  (We will soon see how to simulate multiple quantities simultaneously.)

The `tabulate` method provides a quick summary of the individual simulated values and their frequencies.  We can find frequencies of other events using the "count" functions:

- `count_eq(u)`: count *equal to* u
- `count_neq(u)`: count *not equal to* u
- `count_leq(u)`: count *less than or equal to* u
- `count_lt(u)`: count *less than* u
- `count_geq(u)`: count *greater than or equal to* u
- `count_gt(u)`: count *greater than* u
- `count`: count according to a specified True/False criteria. (See the code after Figure \@ref(fig:dice-tile-marginal) for an example.) Note: `count()` with no inputs to defaults to "count all".

```{python}
y.count()
```


```{python}
y.count_eq(3) / y.count()
```

```{python}
y.count_leq(3) / y.count()
```


Graphical summaries play an important role in describing distributions.
We can plot the 100 individual simulated values of $Y$ in a *rug plot*.

```{python}
y.plot('rug')
plt.show()
```

The rug plot emphasizes that realizations of the random varible $Y$ are numbers along a number line.  However, the rug plot does not adequately summarize the relative frequencies.  Instead, calling `.plot()` produces^[For discrete random variables `'impulse'` is the default plot type. Like `.tabulate()`, the `.plot()` method also has a `normalize` argument; the default is `normalize=True`.] an *impulse plot* which displays the simulated values and their relative frequencies.


```{python}

y.plot()
plt.show()

```

We will introduce several more plot types and commands for summarizing simulation output in the following sections.


### Approximating probabilities and distributions {#symbulate-distribution}

The theoretical distribution of $Y$ in the die rolling example is represented by Table \@ref(tab:dice-max-dist-table).  The plot above, based on only 100 simulated values, provides a poor approximation to the distribution of $Y$.
We often initially simulate a small number of repetitions to see what the simulation is doing and check that it is working properly.  However, in order to accurately approximate probabilities or distribution we simulate a large number of repetitions (usually thousands for our purposes).

Now we simulate 10,000 values of the random variable `Y` and summarize the simulation output to approximate the distribution of $Y$.  Since the simulation results below are stored as `y`, the same set of results is used to produce the table and the plot.  Compare the simulation results summarized in Figure \@ref(fig:dice-max-marginal-sim) with Table \@ref(tab:dice-max-dist-table). The results of 10000 repetitions provide a much better approximation to the true distribution of $Y$ than the results of just 100 repetitions.


(ref:cap-dice-max-marginal-sim) Simulation-based approximate distribution of $Y$, the larger (or common value if a tie) of two rolls of a fair four-sided die.

```{python}

y = Y.sim(10000)
y.tabulate()

```

```{python dice-max-marginal-sim, fig.cap = "(ref:cap-dice-max-marginal-sim)"}

y.plot()
plt.show()

```


Figure \@ref(fig:dice-sum-marginal-sim) contains a summary of a simulated $X$ values.  Compare with Table \@ref(tab:dice-sum-dist-table).

(ref:cap-dice-sum-marginal-sim) Simulation-based approximate distribution of $X$, the sum of two rolls of a fair four-sided die.



```{python}

x = X.sim(10000)
x.tabulate(normalize = True)

```


```{python, dice-sum-marginal-sim, fig.cap = "(ref:cap-dice-sum-marginal-sim)"}

x.plot()
plt.show()

```

Since probabilities can be interpreted as long run relative frequencies, we can use simulate many repetitions and use the `count` functions to compute relative frequencies and approximate probabilities of events.



```{python}
y.count()
```


```{python}
y.count_eq(3) / y.count()
```


```{python}
y.count_leq(3) / y.count()
```


### Approximating long run averages



Recall that we have stored 10000 simulated values of $Y$ as `y`. We can approximate the long run average value of $Y$ by computing the average --- a.k.a., *mean* --- of the 10000 simulated values in the usual way: sum the 10000 simulated values stored in `y` and divide by 10000. Here are a few ways of computing the mean of the simulated values.

```{python}

y.sum() / 10000

```

```{python}

y.sum() / y.count()

```


```{python}

y.mean()

```

Similarly, the approximate long run average value of $X$ is

```{python}

x.mean()

```

These values represent the "balance points" in Figure \@ref(fig:dice-max-marginal-sim)  and Figure \@ref(fig:dice-sum-marginal-sim). We will discuss long run average values in more detail later.








### Simulating events {#symbulate-events}

So far, we have simulated values of random variables and used the results to approximate probabilities of related events.  Events can also be defined and simulated directly.
For programming reasons, events are enclosed in parentheses `()` rather than braces $\{\}$.  For example, we can define the event that the larger of the two rolls is less than 3, $A=\{Y<3\}$, as

```{python}
A = (Y < 3) # an event
```

We can use `sim` to simulate events.
A realization of an event is `True` if the event occurs for the simulated outcome,  or `False` if not.


```{python}
A.sim(10)

```



For logical equality use a double equal sign `==`.  For example, `(Y == 3)` represents the event $\{Y=3\}$.

```{python}
(Y == 3).sim(10000).tabulate()

```

Since event $A$ is defined in terms of $Y$, we can also first simulate values of $Y$, store the results, and then determine whether event $A$ occurs based on each of the simulated values of $Y$. (The following code uses the values of `y` we had simulated earlier, and so will not match the output of the code above.)

```{python}
(y == 3)
```

```{python}
(y == 3).count_eq(True) / y.count()
```

Python automatically treats `True` as 1 and `False` as 0, so the code `(Y == 3)` functions effectively both as the event itself and as the indicator random variable for the event.
In particular, we can count the number of repetitions on which the event occurs by *summing* the simulated values of the indicator.

```{python}
(y == 3).sum() / y.count()
```


### Simulating multiple random variables {#sym-joint}


We can simulate $(X, Y)$ pairs using^[Technically `&` joins two `RV`s together to form a random *vector*.  While we often interpret Symbulate `RV` as random variable, it really functions as random vector.] `&`.    We store the simulation output as `xy` to emphasize that `xy` contains pairs of values.

```{python}
xy = (X & Y).sim(10000)
xy

```

Pairs of values can also be tabulated.

```{python}
xy.tabulate()

```


```{python}
xy.tabulate(normalize = True)

```

Individual pairs can be plotted in a scatter plot, which is a two-dimensional analog of a rug plot.

```{python}
xy.plot()
plt.show()

```


The values can be "jittered" slightly, as below, so that points do not coincide.

```{python}
xy.plot(jitter = True)
plt.show()

```

Even with jittering, the scatter plot does not adequately represent relative frequencies.
The two-dimensional analog of an impulse plot is a *tile plot*. For two discrete variables, the `'tile'` plot type produces a tile plot (a.k.a. heat map) where rectangles represent the simulated pairs with their relative frequencies visualized on a color scale.


```{python, eval = FALSE}
xy.plot('tile')
plt.show()

```

(ref:cap-dice-tile) Tile plot visualization of the simulation-based approximate joint distribution of the sum ($X$) and larger ($Y$) of two rolls of a fair four-sided die.  Color intensity represents relative frequencies of pairs.


```{r, dice-tile, echo = FALSE, fig.cap = "(ref:cap-dice-tile)"}

knitr::include_graphics("_graphics/dice-tile.png")
```

We can add the impulse plot for each of $X$ and $Y$ in the margins of the tile plot using the `'marginal'` argument.  

```{python, eval = FALSE}
xy.plot(['tile', 'marginal'])
plt.show()

```

(ref:cap-dice-tile-marginal) Tile and impulse plot visualization of the simulation-based approximate joint and marginal distributions of the sum ($X$) and larger ($Y$) of two rolls of a fair four-sided die.




```{r, dice-tile-marginal, echo = FALSE, fig.cap = "(ref:cap-dice-tile-marginal)"}

knitr::include_graphics("_graphics/dice-tile-marginal.png")
```

Custom functions can be used with `count` to compute relative frequencies of events involving multiple random variables. Suppose we want to approximate $\IP(X<6, Y \ge 2)$.  We first define a Python function which takes as an input a pair `u = (u[0], u[1])` and returns `True` if `u[0] < 6` and `u[1] >= 2`.

```{python}

def is_x_lt_6_and_y_ge_2(u):
  if u[0] < 6 and u[1] >= 2:
    return True
  else:
    return False

```

Now we can use this function along with `count` to find the simulated relative frequency of the event $\{X <6, Y \ge 2\}$.
Remember that `xy` stores $(X, Y)$ pairs of values, so the first coordinate `xy[0]` represents values of $X$ and the second coordinate `xy[1]` represents values of $Y$.

```{python}
xy.count(is_x_lt_6_and_y_ge_2) / xy.count()
```

We could also count use Boolean logic; basically using indicators and the property $\ind_{\{X<6,Y\ge 2\}}=\ind_{\{X<6\}}\ind_{\{Y\ge 2\}}$.

```{python}
((xy[0] < 6) * (xy[1] >= 2)).count_eq(True) / xy.count()
```


### Simulating outcomes and random variables

Recall Table \@ref(tab:dice-sim-tactile-results) which displays both the outcomes of the two rolls and the corresponding values of $X$ and $Y$ for 10 repetitions.  Calling `(X & Y).sim(10)` as in the previous section produces results like those in the table, but only the values of `(X & Y)` are saved and displayed. The outcomes of the rolls are generated in the background but not saved. 

We can create a `RV` which returns the outcomes of the probability space^[You might try `(P & X).sim(10)`. But `P` is a probability space object, and `X` is an `RV` object, and `&` can only be used to join like objects together.  Much like in probability theory in general, in Symbulate the probability space plays a background role, and it is usually random variables we are interested in.].  The default mapping function for `RV` is the identity function, $g(u) = u$, so simulating values of `U = RV(P)` below returns the outcomes of the BoxModel `P` representing the outcome of the two rolls. 

```{python}
U = RV(P)
U.sim(10)
```

Now we can simulate and display the outcomes along with the values of $X$ and $Y$ using `&`.

```{python}
(U & X & Y).sim(10)
```


Because the probability space `P` returns pairs of values, `U = RV(P)` above defines a random vector.  The individual components^[The components can also be accessed using brackets.  `U1, U2 = RV(P)` is shorthand for  
`U = RV(P); U1 = U[0]; U2 = U[1]`. Python uses zero-based indexing, so 0 refers to the first component, 1 to the second, and so on.] of `U` can be "unpacked" as `U1, U2` in the following.  Here `U1` represents the result of the first roll and `U2` the second. 

```{python}
U1, U2 = RV(P)
(U1 & U2 & X & Y).sim(10)
```

### Transformations of random variables

Transformations of random variables are random variables.
If `X` is a Symbulate `RV` and `g` is a function, then `g(X)` is also a Symbulate `RV`.

For example, we can approximate the long run average value of $X^2$.

```{python}
(X ** 2).sim(10000).mean()
```

For many common functions, the syntax `g(X)` is sufficient.
Here `exp(u)` is the exponential function $g(u) = e^u$.

```{python}
(X & exp(X)).sim(10)
```

For user defined functions, the syntax is `X.apply(g)`.

```{python}
def g(u):
  return (u - 5) ** 2
  
Z = X.apply(g)

(X & Z).sim(10)

```

(We could have actually defined `Z = (X - 5) ** 2` here.
We'll see examples where the `apply` syntax is necessary later.)

We can also apply transformations of multiple `RV`s *defined on the same probability space*.
(We will look more closely at how Symbulate treats this "same probability space" issue later.)

For example, we can approximate the long run average value of $XY$.

```{python}
(X * Y).sim(10000).mean()
```

Recall that we defined $X$ via `X = RV(P, sum)`.
Defining random variables $U_1, U_2$ to represent the individual rolls, we can define $X=U_1 + U_2$.
Recall that we defined^[We can also define `U = RV(P)` and then `X = U.apply(sum)`.] `U1, U2 = RV(P)`.

```{python}
X = U1 + U2

X.sim(10000).tabulate(normalize = True)

```

Unfortunately `max(U1, U2)` does not work, but we can use the `apply` syntax.
Since we want to apply `max` to $(U_1, U_2)$ pairs, we must^[We can also define `U = RV(P)` and then `X = U.apply(max)`.] first join them together with `&`.

```{python}
Y  = (U1 & U2).apply(max)

Y.sim(10000).tabulate(normalize = True)

```


### Two "worlds" in Symbulate

In Section \@ref(symbulate-events) we saw that we could either 

1. Define the event `(Y < 3)` and simulate True/False realizations of it, or
1. Define the random variable `Y`, simulate values of it and store them as `y`, and then evaluate the event for each of the simulated values with `(y < 3)`.

These two methods illustrate the two "worlds" of Symbulate, which we call "random variable world" and "simulation world".
Operations like transformations can be performed in either world.
Think of random variable world as the "before" world and simulation world as the "after" world, by which we mean before/after the `sim` step.


Most of the transformations we have seen so far happened in random variable world.
For example, we have seen how to define the sum of two dice in random variable world in a few ways, e.g., via

```
P = BoxModel([1, 2, 3, 4], size = 2)
U = RV(P)

X = U.apply(sum)
```

The sum transformation is applied to define a new random variable `X`, before the `sim` step.
We could then call, e.g., `(U & X).sim(10000)`. 

We could also compute simulated values of the sum in simulation world as follows.

```
P = BoxModel([1, 2, 3, 4], size = 2)
U = RV(P)

u = U.sim(10000)

x = u.apply(sum)

```

The above code will simulate all the pairs of rolls first, store them as `u`, and then apply the sum to the simulated values.
That is, the sum transformation happens after the `sim` step.

While either world is "correct", we generally take the random variable world approach.
We do this mostly for consistency, but also to emphasize some of the probability concepts we'll encounter.
For example, the fact that a sum of random variables (defined on the same probability space) is also a random variable is a little more apparent in random variable world.
However, it is sometimes more convenient to code in simulation world; for example, if complicated transformations are required.


When working in random variable world, it only makes sense to transform random variables defined on the same probability space.
Likewise, in simulation world, it only makes sense to apply transformations to values generated in the same simulation, with a single `sim` step.
For example, the following code would return an error.

```
P = BoxModel([1, 2, 3, 4], size = 2)
U1, U2 = RV(P)

u1 = U1.sim(10000)
u2 = U2.sim(10000)

x = u1 + u2 # returns an error: "objects must come from the same simulation"

```

We'll discuss the reason for this error in more detail later.
In short, the "simulation" step should be implemented in a single call to `sim`.

### Brief summary of Symbulate commands

This section has only presented an introduction to simulations and Symbulate.
We will see many more scenarios and additional Symbulate commands throughout the book.



Many
scenarios require only a few lines of Symbulate code to set up, run,
analyze, and visualize. The following table comprises the
requisite Symbulate commands for a wide variety of situations.


  Command                    Function
  -------------------------- -----------------------------------------------------------------------------------------
  `BoxModel`                 Define a box model
  `ProbabilitySpace`         Define a custom probability space
  `RV`                       Define random variables, vectors, or processes
  `RandomProcess`            Define a discrete or continuous time stochastic process
  `apply`                    Apply transformations
  `[]` (brackets)            Access a component of a random vector, or a random process at a particular time
  `*` (and `**`)             Define independent probability spaces or distributions
  `AssumeIndependent`        Assume random variables or processes are independent
  `|` (vertical bar)          Condition on events
  `&`                        Join multiple random variables into a random vector
  `sim`                      Simulate outcomes, events, and random variables, vectors, and processes
  `tabulate`                 Tabulate simulated values
  `plot`                     Plot simulated values
  `filter` (and relatives)   Create subsets of simulated values (`filter_eq`, `filter_lt`, etc)
  `count` (and relatives)    Count simulated values which satisfy some critera (`count_eq`, `count_lt`, etc)
  Statistical summaries      `mean`, `median`, `sd`, `var`, `quantile`, `corr`, `cov`, etc.
  Common models              See later


While no previous experience with Python is required, it is also possible to incorporate Python programming with Symbulate code. In particular, Python functions or loops can be used: to define or transform random variables or stochastic processes, or to investigate the effects of changing parameter values.  Also, while many common plots are built in with the Symbulate `plot` function, the Matplotlib package can be used to create or customize plots.  Some of these features will be illustrated in the next section and more examples are found throughout the text.






### Exercises



1. Flip a fair coin 4 times and let $X$ be the number of H.

    a. Write the Symbulate code to define $X$ using a box model with tickets labeled H and T.  Simulate a few outcomes and a few values of $X$.
    a. Write the Symbulate code to define $X$ using a box model with tickets labeled 1 and 0 and without counting. Simulate a few outcomes together with their values of $X$.
    a. Write the Symbulate code to conduct a simulation and use the results to approximate $\IP(X = 3)$. 




## Approximating probabilities: Simulation margin of error {#moe}


The probability of an event can be approximated by simulating the random phenomenon a large number of times and computing the relative frequency of the event.  After enough repetitions we expect the simulated relative frequency to be *close to* the true probability, but there probably won't be an exact match.  Therefore, in addition to reporting the approximate probability, we should also provide a margin of error which indicates how close we think our simulated relative frequency is to the true probability.

Section \@ref(rel-freq) introduced the relative frequency interpretation in the context of flipping a fair coin.  After many flips of a fair coin, we expect the proportion of flips resulting in H to be close to 0.5.  But how many flips is enough?  And how "close" to 0.5? We'll investigate these questions now.


```{r, echo = FALSE, cache = TRUE, warning=FALSE, message=FALSE}

N = 100
n = 10000
p = 0.5
ci = 0.95
z = 2


phat = data.frame(phat = rbinom(N, n, p) / n)

phat = phat %>%
  mutate(ci_lb = phat - z * sqrt(p * (1 - p) / n),
         ci_ub = phat + z * sqrt(p * (1 - p) / n),
         good_ci = abs(phat - p) <= z * sqrt(p * (1 - p) / n))

plot_coin_phat_n1 <- ggplot(phat,
                            aes(x = phat,
                                color = good_ci,
                                fill = good_ci)) +
  geom_dotplot() +
  theme_classic() +
  theme(axis.line = element_line(color = "black"),
        legend.position = "none") +
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(breaks = p - seq(-3, 3, 0.5) * sqrt(p * (1 - p) / n)) +
  geom_vline(xintercept = p + c(-1, 1) * z * sqrt(p * (1 - p) / n),
             linetype = "dotted", color = "black", size = 1.5) +
  labs(x = "Proportion of heads")

num_good_ci_n1 <- phat %>% select(good_ci) %>% sum()

num_good_phat <- phat %>% filter(phat == p) %>% count()

```

Consider Figure \@ref(fig:coin-sim1) below.
Each dot represents a set of 10,000 fair coin flips.
There are 100 dots displayed, representing 100 different sets of 10,000 coin flips each.
For each set of flips, the proportion of the 10,000 flips which landed on head is recorded.
For example, if in one set 4973 out of 10,000 flips landed on heads, the proportion of heads is 0.4973.
The plot displays 100 such proportions; similar values have been "binned" together for plotting.
We see that `r num_good_ci_n1` of these 100 proportions are between 0.49 and 0.51, represented by the blue dots.
So if "between 0.49 and 0.51" is considered "close to 0.5", then yes, in 10000 coin flips we would expect^[In 10000 flips, the probability of heads on between 49\% and 51\% of flips is 0.956, so `r num_good_ci_n1` out of 100 provides a rough estimate of this probability. We will see how to compute such a probability later.] the proportion of heads to be close to 0.5.


(ref:cap-coin-sim1) Proportion of flips which are heads in 100 sets of **10,000** fair coin flips.  Each dot represents a set of **10,000** fair coin flips. In `r num_good_ci_n1` of these 100 sets the proportion of heads is between 0.49 and 0.51 (the blue dots).

```{r coin-sim1, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-coin-sim1)"}

print(plot_coin_phat_n1)

```

Our discussion of Figure \@ref(fig:coin-sim1) suggests that 0.01 might be an appropriate margin of error for a simulation based on 10,000 flips.
Suppose we perform a simulation of 10000 flips with 4973 landing on heads.
We could say "we estimate that the probability that a coin land on heads is equal to 0.4973".
But such a precise estimate would almost certainly be incorrect, due to natural variability in the simulation.
In fact, only `r num_good_phat` sets^[This is difficult to see in Figure \@ref(fig:coin-sim1) due to the binning. But we can at least tell from Figure \@ref(fig:coin-sim1) that at most a handful of the 100 sets resulted in a proportion of heads exactly equal to 0.5.] resulted in a proportion of heads exactly equal to the true probability of 0.5. 



A better statement would be "we estimate that the probability that a coin land on heads is 0.4973 *with a margin of error*^[Technically, we should say "a margin of error *for 95% confidence* of 0.01". We'll discuss "confidence" in a little more detail soon.] *of 0.01*".
This means that we estimate that the true probability of heads is within 0.01 of 0.4973.
In other words, we estimate that the true probability of heads is between 0.4873 and 0.5073, an interval whose endpoints are $0.4973 \pm 0.01$.
This interval estimate is "accurate" in the sense that the true probability of heads, 0.5, *is* between 0.4873 and 0.5073. 
By providing a margin of error, we have sacrificed a little precision --- "equal to 0.4973" versus "within 0.01 of 0.4973" --- to achieve greater accuracy.


Let's explore this idea of "accuracy" further.
Recall that Figure \@ref(fig:coin-sim1) displays the proportion of flips which landed on heads for 100 sets of 10000 flips each.
Suppose that for each of these sets we form an interval estimate of the probability that the coin lands on heads by adding/subtracting 0.01 from the simulated proportion, as we did for $0.4973 \pm 0.01$ in the previous paragraph.
Figure \@ref(fig:coin-sim1-ci) displays the results.
Even though the proportion of heads was equal to 0.5 in only `r num_good_phat` sets, in `r num_good_ci_n1` of these 100 sets (the blue dots/intervals) the corresponding interval contains 0.5, the true probability of heads.
For almost all of the sets, the interval formed via "relative frequency $\pm$ margin of error" provides an accurate estimate of the true probability.
However, not all the intervals contain the true probability, which is why we often qualify that our margin of error is for "95% confidence" or "95% accuracy".
We will see more about "confidence" soon.
In any case, the discussion so far, and the results in Figure \@ref(fig:coin-sim1) and Figure \@ref(fig:coin-sim1-ci), suggest that 0.01 is a reasonable choice for margin of error when estimating the probability that a coin lands on heads based on 10000 flips.



(ref:cap-coin-sim1-ci) Interval estimates of the probability of heads based on 100 sets of **10,000** fair coin flips.  Each dot represents the proportion of heads in a set of **10,000** fair coin flips. (The sets have been sorted based on their proportion of heads.) For each set an interval is obtained by adding/subtracting the margin of error of 0.01 from the proportion of heads. In `r num_good_ci_n1` of these 100 sets (the blue dots/intervals) the corresponding interval contains the true probability of heads (0.5, represented by the vertical black line). 

```{r coin-sim1-ci, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-coin-sim1-ci)"}

plot_coin_phat_n1_ci <- ggplot(phat %>%
                                 arrange(phat) %>%
                                 mutate(repetition = row_number()),
                               aes(x = phat,
                                   y = repetition,
                                   color = good_ci)) +
  geom_point() +
  geom_segment(aes(x = ci_lb, xend = ci_ub,
                   y = repetition, yend = repetition,
                   color = good_ci)) +
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
    geom_vline(xintercept = p, color = "black", size = 1.0) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "Proportion of heads, with margin of error",
       y = "Set")

print(plot_coin_phat_n1_ci)
```



```{r, echo = FALSE, cache = TRUE, warning=FALSE, message=FALSE}

N = 100
n = 1000000
p = 0.5
ci = 0.95
z = 2


phat = data.frame(phat = rbinom(N, n, p) / n)

phat = phat %>%
  mutate(ci_lb = phat - z * sqrt(p * (1 - p) / n),
         ci_ub = phat + z * sqrt(p * (1 - p) / n),
         good_ci = abs(phat - p) <= z * sqrt(p * (1 - p) / n))

plot_coin_phat_n2 <- ggplot(phat,
                            aes(x = phat,
                                color = good_ci,
                                fill = good_ci)) +
  geom_dotplot() +
  theme_classic() +
  theme(axis.line = element_line(color = "black"),
        legend.position = "none") +
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(breaks = p - seq(-3, 3, 0.5) * sqrt(p * (1 - p) / n)) +
  geom_vline(xintercept = p + c(-1, 1) * z * sqrt(p * (1 - p) / n),
             linetype = "dotted", color = "black", size = 1.5) +
  labs(x = "Proportion of heads")

num_good_ci_n2 <- phat %>% select(good_ci) %>% sum()

```





What if we want to be stricter about what qualifies as "close to 0.5"?
That is, what if a margin of error of 0.01 isn't good enough?
You might suspect that with even more flips we would expect to observe heads on even closer to 50\% of flips.  Indeed, this is the case.  Figure \@ref(fig:coin-sim2) displays the results of 100 sets of *1,000,000* fair coin flips.  The pattern seems similar to Figure \@ref(fig:coin-sim1) but pay close attention to the horizontal axis which covers a much shorter range of values than in the previous figure.  Now `r num_good_ci_n2` of the 100 proportions are between *0.499 and 0.501*. So in 1,000,000 flips we would expect^[In 1,000,000 flips, the probability of heads on between 49.9\% and 50.1\% of flips is 0.955, and `r num_good_ci_n2` out of 100 sets provides a rough estimate of this probability.] the proportion of heads to be between 0.499 and 0.501, pretty close to 0.5. 
This suggests that 0.001 might be an appropriate margin of error for a simulation based on 1,000,000 flips.


(ref:cap-coin-sim2) Proportion of flips which are heads in 100 sets of **1,000,000** fair coin flips. Each dot represents a set of **1,000,000** fair coin flips. In `r num_good_ci_n2` of these 100 sets the proportion of heads is between 0.499 and 0.501 (the blue dots).



```{r coin-sim2, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-coin-sim2)"}

print(plot_coin_phat_n2)

```



```{r, echo = FALSE, cache = TRUE, warning=FALSE, message=FALSE}

N = 100
n = 100000000
p = 0.5
ci = 0.95
z = 2


phat = data.frame(phat = rbinom(N, n, p) / n)

phat = phat %>%
  mutate(ci_lb = phat - z * sqrt(p * (1 - p) / n),
         ci_ub = phat + z * sqrt(p * (1 - p) / n),
         good_ci = abs(phat - p) <= z * sqrt(p * (1 - p) / n))

plot_coin_phat_n3 <- ggplot(phat,
                            aes(x = phat,
                                color = good_ci,
                                fill = good_ci)) +
  geom_dotplot() +
  theme_classic() +
  theme(axis.line = element_line(color = "black"),
        legend.position = "none") +
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(breaks = p - seq(-3, 3, 0.5) * sqrt(p * (1 - p) / n)) +
  geom_vline(xintercept = p + c(-1, 1) * z * sqrt(p * (1 - p) / n),
             linetype = "dotted", color = "black", size = 1.5) +
  labs(x = "Proportion of heads")

num_good_ci_n3 <- phat %>% select(good_ci) %>% sum()

```


What about even more flips?
In Figure \@ref(fig:coin-sim3) each dot represents a set of *100 million* flips.
The pattern seems similar to the previous figures, but again pay close attention the horizontal access which covers a smaller range of values.
Now `r num_good_ci_n3` of the 100 proportions are between *0.4999 and 0.5001*.
So in 100 million flips we would expect^[In 100 million flips, the probability of heads on between 49.99\% and 50.01\% of flips is 0.977, and `r num_good_ci_n3` out of 100 sets provides a rough estimate of this probability.] the proportion of heads to be between 0.4999 and 0.5001, pretty close to 0.5.
This suggests that 0.0001 might be an appropriate margin of error for a simulation based on 100,000,000 flips.


(ref:cap-coin-sim3) Proportion of flips which are heads in 100 sets of **100,000,000** fair coin flips. Each dot represents a set of **100,000,000** fair coin flips. In `r num_good_ci_n3` of these 100 sets the proportion of heads is between 0.4999 and 0.5001 (the blue dots).

```{r coin-sim3, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-coin-sim3)"}

print(plot_coin_phat_n3)

```


The previous figures illustrate that the more flips there are, the more likely it is that we observe a proportion of flips landing on heads close to 0.5.  We also see that with more flips we can refine our definition of "close to 0.5": increasing the number of flips by a factor of 100 (10,000 to 1,000,000 to 100,000,000) seems to give us an additional decimal place of precision ($0.5\pm0.01$ to $0.5\pm 0.001$ to $0.5\pm 0.0001$.)

### A closer look at margin of error

We will now carry out an analysis similar to the one above to investigate simulation margin of error and how it is influenced by the number of simulated values used to compute the relative frequency.  Continuing the dice example, suppose we want to estimate $p=\IP(X=6)$, the probability that the sum of two rolls of a fair four-sided equals six. We saw in Section \@ref(dist-intro) that the true probability is $p=3/16=0.1875$. 

We will perform a "meta-simulation". The process is as follows

1. Simulate two rolls of a fair four-sided die.  Compute the sum ($X$) and see if it is equal to 6.
1. Repeat step 1 to generate $n$ simulated values of the sum ($X$).  Compute the relative frequency of sixes: count the number of the $n$ simulated values equal to 6 and divide by $n$.  Denote this relative frequency $\hat{p}$ (read "p-hat").
1. Repeat step 2 a large number of times, recording the relative frequency $\hat{p}$ for each set of $n$ values.

Be sure to distinguish between steps 2 and 3.  A simulation will typically involve just steps 1 and 2, resulting in a single relative frequency based on $n$ simulated values.  Step 3 is the "meta" step; we see how this relative frequency varies from simulation to simulation to help us in determing an appropriate margin of error.  The important quantity in this analysis is $n$, the *number of simulated values used to compute the relative frequency* in a single simulation. We wish to see how $n$ impacts margin of error.  The number of simulations in step 3 just needs to be "large" enough to provide a clear picture of how the relative frequency varies from simulation to simulation.  The more the relative frequency varies from simulation to simulation, the larger the margin of error needs to be.

We can combine steps 1 and 2 of the meta-simulation to put it in the framework of the simulations from earlier in this chapter.  Namely, we can code the meta-simulation as a single simulation in which

- A sample space outcome represents $n$ values of the sum of two fair-four sided dice
- The main random variable of interest is the proportion of the $n$ values which are equal to 6.

Let's first consider $n=100$. The following Symbulate code defines the probability space corresponding to 100 values of the sum of two-fair four sided dice.  Notice the use of `apply` which functions much in the same way^[One difference between `RV` and `apply`: `apply` preserves the type of the input object.  That is, if `apply` is applied to a `ProbabilitySpace` then the output will be a `ProbabilitySpace`; if `apply` is applied to an `RV` then the output will be an `RV`.  In contrast, `RV` always creates an `RV`.] as `RV`.


```{python metasim-p}
n = 100
P = BoxModel([1, 2, 3, 4], size = 2).apply(sum) ** n
P.sim(5)

```

In the code above

- `BoxModel([1, 2, 3, 4], size = 2)` simulates two rolls of a fair four-sided die
- `.apply(sum)` computes the sum of the two rolls
- `** n` repeats the process `n` times to generate a set of `n` independent values, each value representing the sum of two rolls of a fair four-sided die
- `P.sim(5)` simulates 5 sets, each set consisting of `n` sums

Now we define the random variable which takes as an input a set of $n$ sums and returns the proportion of the $n$ sums which are equal to six.

```{python metasim-phat}

phat = RV(P, count_eq(6)) / n
phat.sim(5)

```

In the code above

- `phat` is an `RV` defined on the probability space `P`. Recall that an outcome of `P` is a set of `n` sums (and each sum is the sum of two rolls of a fair four-sided die).
- The function that defines the `RV` is `count.eq(6)`, which counts the number of values in the set that are equal to 6. We then^[Unfortunately, for techincal reasons, `RV(P, count_eq(6) / n)` will not work.  It is possible to divide by `n` within `RV` if we define a custom function `def rel_freq_six(x): return x.count_eq(6) / n`
and then define `RV(P, rel_freq_six)`.] divide by `n`, the total number of values in the set, to get the relative frequency.  (Remember that a transformation of a random variable is also a random variable.)
- `phat.sim(5)` generates 5 simulated values of the relative frequency `phat`.  Each simulated value of `phat` is the relative frequency of sixes in `n` sums of two rolls of a fair four-sided die.


Now we simulate and summarize a large number of values of `phat`.
We'll simulate 100 values for illustration (as we did in Figures \@ref(fig:coin-sim1), \@ref(fig:coin-sim2), and \@ref(fig:coin-sim3)).
Be sure not to confuse 100 with `n`.
Remember, the important quantity is `n`, the number of simulated values used in computing each relative frequency.

```{python, eval = TRUE, cache = TRUE}
phat.sim(100).plot(type = "impulse", normalize = False)
plt.ylabel('Number of simulations');
plt.show()

```


We see that the 100 relative frequencies are roughly centered around the true probability 0.1875, but there is variability in the relative frequencies from simulation to simulation. From the range of values, we see that most relative frequencies are within about 0.08 or so from the true probability 0.1875. So a value around 0.08 seems like a reasonable value of the margin of error, but the actual value depends on what we mean by "most".  We can get a clearer picture if we run more simulations.  The following plot displays the results of 10000 simulations, each resulting in a value of $\hat{p}$.  Remember that each relative frequency is based on $n=100$ sums of two rolls.

```{python, eval = TRUE, cache = TRUE}
phats = phat.sim(10000)
phats.plot(type = "impulse", normalize = False)
plt.ylabel('Number of simulations');
plt.show()
```

Let's see how many of these 10000 simulated proportions are within 0.08 of the true probability 0.1875.




```{python, eval = TRUE, cache = TRUE}
1 - (phats.count_lt(0.1875 - 0.08) + phats.count_gt(0.1875 + 0.08)) / 10000

```

In roughly 95% or so of simulations, the simulated relative frequency was within 0.08 of the true probability.  So 0.08 seems like a reasonable margin of error for "95% confidence" or "95% accuracy".  However, a margin of error of 0.08 yields pretty imprecise estimates, ranging from about 0.10 to 0.27.  Can we keep the degree of accuracy at 95% but get a smaller margin of error, and hence a more precise estimate?  Yes, if we increase the number of repetitions used to compute the relative frequency.

Now we repeat the analysis, but with $n=10000$.  In this case, each relative frequency is computed based on 10000 independent values, each value representing a sum of two rolls of a fair four-sided die. As before we start with 100 simulated relative frequencies.

```{python, eval = TRUE, cache = TRUE}

n = 10000
P = BoxModel([1, 2, 3, 4], size = 2).apply(sum) ** n
phat = RV(P, count_eq(6)) / n

phats = phat.sim(100)
phats.plot(type = "impulse", normalize = False)
plt.ylabel('Number of simulations');
plt.show()

```

Again we see that the 100 relative frequencies are roughly centered around the true probability 0.1875, but there is less variability in the relative frequencies from simulation to simulation for $n=10000$ than for $n=100$.
Pay close attention to the horizontal axis.
From the range of values, we see that most relative frequencies are now within about 0.008 of the true probability 0.1875.

```{python, eval = TRUE, cache = TRUE}
1 - (phats.count_lt(0.1875 - 0.008) + phats.count_gt(0.1875 + 0.008)) / 100

```

As with $n=100$, running more than 100 simulations would give a clearer picture of how much the relative frequency based on $n=10000$ simulated values varies from simulation to simulation. But even with just 100 simulations, we see that a margin of error of about 0.008 is required for roughly 95% accuracy when $n=10000$, as opposed to 0.08 when $n=100$. As we observed in the coin flipping example earlier in this section, it appears that increasing $n$ by a factor of 100 yields an extra decimal place of precision. That is, increasing $n$ by a factor of 100 decreases the margin of error by a factor of $\sqrt{100}$.  In general, the margin of error is inversely related to $\sqrt{n}$.


(ref:cap-moe-compare) Comparison of margins of error for 95% confidence for the meta-simulations in this section.

```{r, moe-compare, echo = FALSE}
ns = 100 ^ (1:3)

compare_df = data.frame(c("A fair coin flip lands H",
                          "Two rolls of a fair four-sided die sum to 6"),
                        c(0.5, 0.1875),
                    round(t(cbind(2 * 0.5 / sqrt(ns), 2 * sqrt(3 / 16 * (1 - 3 / 16)) / sqrt(ns))), 4))

knitr::kable(
  compare_df,
  digits = 5,
  booktabs = TRUE,
  col.names = c("Probability that", "True value",
                "95% m.o.e. (n = 100)", "95% m.o.e. (n = 10000)",
                "95% m.o.e. (n = 1000000)"),
  caption = "(ref:cap-moe-compare)"
)

```


The two examples in this section illustrate that the margin of error also depends somewhat on the true probability. The margin of error required for 95% accuracy is larger when the true probability is 0.5 than when it is 0.1875.  It can be shown that when estimating a probability $p$ with a relative frequency based on $n$ simulated repetitions, the margin of error required for 95% confidence^[We will see
    the rationale behind this formula later. The factor 2
    comes from the fact that for a Normal distribution, about 95% of values
    are within 2 standard deviations of the mean. Technically, the
    factor 2 corresponds to 95% confidence only when a single
    probability is estimated. If multiple probabilities are estimated
    simultaneously, then alternative methods should be used,
    e.g., increasing the factor 2 using a
    [Bonferroni
    correction](https://en.wikipedia.org/wiki/Bonferroni_correction). For example, a multiple of 5 rather than 2 produces very conservative error bounds for 95% confidence even when many probabilities are being estimated.] is
\[
2\frac{\sqrt{p(1-p)}}{\sqrt{n}}
\]
For a given $n$, the above quantity is maximized when $p$ is 0.5.  Since $p$ is usually unknown --- the reason for performing the simulation is to approximate it --- we plug in 0.5 for a somewhat conservative margin of error of $1/\sqrt{n}$.

For a fixed $n$, there is a tradeoff between accuracy and precision.
The factor 2 in the margin of error formula above corresponds to 95% accuracy.
Greater accuracy would require a larger factor, and a larger margin of error, resulting in a wider --- that is, less precise --- interval.
For example, 99% confidence requires a factor of roughly 2.6 instead of 2, resulting in an interval that is roughly 30 percent wider.
The confidence level does matter, but the primary influencer of margin of error is $n$, the number of repetitions on which the relative frequency is based.
Regardless of confidence level, the margin of error is on the order of magnitude of $1/\sqrt{n}$.

In summary, **the margin of error when approximating a probability based on a simulated relative frequency is roughly on the order \(1/\sqrt{n}\), where \(n\) is the number of independently simulated values used to calculate the relative frequency.** Warning: alternative methods are necessary when the actual probability being estimated is very close to 0 or to 1.


A probability is a theoretical long run relative frequency.
A probability can be approximated by a relative frequency from a large number of simulated repetitions, but there is some simulation margin of error.
Likewise, the average value of $X$  after a large number of simulated repetitions is only an approximation to the theoretical long run average value of $X$.
The margin of error is also on the order of $1/\sqrt{n}$ where $n$ is the number of simulated values used to compute the average.
We will explore margins of error for long run averages  in more detail later.

Pay attention to the wording: $n$ is the number of independently^[In all the situations in this book the values will be simulated independently.  However, there are many simulation methods where this is not true, most notably MCMC (Markov Chain Monte Carlo) methods. The margin of error needs to be adjusted to reflect any dependence between simulated values.] simulated values *used to calculate the relative frequency*.
This is not necessarily the number of simulated values.
For example, suppose we use simulation to approximate the probability that the larger of two rolls of a fair four-sided die is 4 *when the sum is equal to 6.*
We might start by simulating 10000 pairs of rolls.
But the sum would be equal to 6 in only about 1875 pairs, and it is only these pairs that would be used to compute the relative frequency that the larger roll is 4 to approximate the probability of interest.
The appropriate margin of error is roughly $1/\sqrt{1875} \approx 0.023$.
Compared to 0.01 (based on the original 10000 repetitions) the margin of error of 0.023 results in intervals that are 130 percent wider.
Carefully identifying the number of values *used to calculate the relative frequency* is especially important when determining appropriate simulation margins of error for approximating *conditional probabilities*, which we'll discuss in more detail later.



### Approximating multiple probabilities

When using simulation to estimate a single probability, the primary influencer of margin of error is $n$, the number of repetitions on which the relative frequency is based.
It doesn't matter as much whether we use, say, 95% versus 99% confidence.
That is, it doesn't matter too much whether we compute our margin of error using
\[
2\frac{\sqrt{p(1-p)}}{\sqrt{n}},
\]
with a multiple of 2 for 95% confidence, or if we replace 2 by 2.6 for 99% confidence.
(Remember, we can plug in 0.5 for the unknown $p$ for a conservative margin of error.)
A margin of error based on 95% or 99% (or another confidence level in the neighborhood) provides a reasonably accurate estimate of the probability. 
However, using simulation to *approximate multiple probabilities simultaneously* requires a little more care with the confidence level.


In the previous section we used simulation to estimate $\IP(X=6)$.
Now suppose we want to approximate the *distribution* of the random variable $X$, the sum of two rolls of a fair four-sided equals six.
We could run a simulation like the one in Section \@ref(symbulate-distribution) to obtain results like those in Figure \@ref(fig:dice-sum-marginal-sim) and the table before it.
Each of the relative frequencies in the table is an approximation of the true probability, and so each of the relative frequencies should have a margin of error, say 0.01 for a simulation based on 10000 repetitions.
Thus, the simulation results yield a *collection* of seven interval estimates, an interval estimate of $\IP(X = x)$ for each value of $x = 2,3,4,5,6,7,8$.
Each interval in the collection either contains the respective true probability or not.
The question is then: In what percent of simulations will *every* interval in the collection contain the respective true probability?

Figure \@ref(fig:multiple-ci1) summarizes the results of 100 simulations.
Each simulation consists of 10000 repetitions, with results similar to those in Figure \@ref(fig:dice-sum-marginal-sim) and the table before it.
Each simulation is represented by a row in Figure \@ref(fig:multiple-ci1), consisting of seven 95% interval estimates, one for each value of $x$.
Each panel represents a different values of $x$; for each value of $x$, around 95 out of the 100 simulations yield estimates that contain the true probability $\IP(X=x)$, represented by the vertical line.


```{r echo=FALSE, warning=FALSE, message=FALSE}
x = 2:8
p_vec = c(1, 2, 3, 4, 3, 2, 1) / 16

n = 10000
N = 100

zmax = 5
z = 2

sim = data.frame(t(rmultinom(N, n, p_vec) / n))
names(sim) = paste("x=", x, sep = "")

good_ci = NULL

for (i in 1:length(x)) {
  good_ci_i = (abs(sim[, i] - p_vec[i]) < z * sqrt(p_vec[i] * (1 - p_vec[i]) / n))
  good_ci <- good_ci %>% bind_cols(good_ci_i)
}

n_good_ci = apply(good_ci, 1, sum)
n_good_sets = sum(n_good_ci == length(x))

sim <- sim %>%
  mutate(n_good_ci = n_good_ci) %>%
  arrange(n_good_ci) %>%
  mutate(repetition = row_number())

phat_lims = c(max(0, min(p_vec - zmax * sqrt(p_vec * (1 - p_vec) / n))),
              min(1, max(p_vec + zmax * sqrt(p_vec * (1 - p_vec) / n))))

plot_i = list()

for (i in 1:length(x)) {
  
  p = p_vec[i]
  
  sim_i = sim %>%
    select(i, repetition) %>%
    rename(phat = 1) %>%
    mutate(ci_lb = pmax(0, phat - z * sqrt(p * (1 - p) / n)),
           ci_ub = pmin(1, phat + z * sqrt(p * (1 - p) / n)),
           good_ci = (abs(phat - p) <= z * sqrt(p * (1 - p) / n)))
  
  p <- ggplot(sim_i, aes(x = phat, y = repetition, color = good_ci)) +
    geom_point() +
    geom_segment(aes(x = ci_lb, xend = ci_ub,
                     y = repetition, yend = repetition,
                     color = good_ci)) +
    # scale_x_continuous(limits = phat_lims) + 
  scale_color_manual(
    values = c("orange", "skyblue"),
    aesthetics = c("color", "fill")
  ) +
        geom_vline(xintercept = p, color = "black", size = 1.0) +
    labs(title = paste("x =", x[i]),
         x = "",
         y = "Simulation") +
    theme_classic() +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5))
        # geom_rect(aes(xmin = 0,xmax=1,ymin=0,ymax=n_good_sets),fill="black",alpha=0.2)
  
  if (i > 1) {
    p <- p +
      theme(axis.title.y = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank(),
            axis.line.y = element_blank())
  }
  
  plot_i[[i]] = p
  
}
```



(ref:cap-multiple-ci1) Results of 100 simulations. Each simulation yields a collection of seven 95% confidence intervals.


```{r multiple-ci1, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-multiple-ci1)"}

do.call(grid.arrange, c(plot_i, nrow = 1))

```


However, let's zoom in on the bottom of Figure \@ref(fig:multiple-ci1).
Figure \@ref(fig:multiple-ci2) displays the results of the `r N - n_good_sets` simulations at the bottom of Figure \@ref(fig:multiple-ci1).
Look carefully row by row in Figure \@ref(fig:multiple-ci2); in each of these simulations at least one of the seven intervals in the collection does not contain the true probability.
In other words, *every* interval in the collection contains the respective true probability in only `r n_good_sets` of the 100 simulations (the other simulations in Figure \@ref(fig:multiple-ci1).)
While we have 95 percent confidence in our interval estimate of $\IP(X = x)$ for any single $x$, we only have around `r n_good_sets` percent confidence in our approximate *distribution* of $X$.
Our confidence "grade" has gone from A range (95 percent) to C range (`r n_good_sets` percent).

(ref:cap-multiple-ci2) Subset of `r N - n_good_sets` simulations from Figure \@ref(fig:multiple-ci1). In each of these simulations, at least one 95% confidence interval does not contain the respective true probability.


```{r multiple-ci2, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-multiple-ci2)"}

plot_i2 = list()

for (i in 1:length(x)) {
  p = plot_i[[i]]
  p <- p +
    ylim(0, N - n_good_sets)
  plot_i2[[i]] = p

}

do.call(grid.arrange, c(plot_i2, nrow = 1))


```


When approximating multiple probabilities based on a single simulation, such as when approximating a distribution, margins of error and interval estimates need to be adjusted to obtain *simultaneous* 95% confidence.
The easiest way to do this is to make all of the intervals in the collection wider.

There are many procedures for adjusting a collection of interval estimates to achieve simultaneous confidence; we won't get into any technical details.
As a very rough, but simple and typically conservative rule, when approximating many probabilities based on a single simulation, we recommend making the margin of error twice as large as when approximating a single probability.
That is, **use a margin of error of
$2/\sqrt{n}$ (rather than $1/\sqrt{n}$) to achieve simultaneous 95% confidence when approximating many probabilities based on a single simulation.**



### Beware a false sense of precision

Why don't we always run something like one trillion repetitions so that our margin of error is tiny?
There is a cost to simulating and storing more repetitions in terms of computational time and memory.
Also, remember that simulating one trillion repetitions doesn't guarantee that the margin of error is actually based on anywhere close to one trillion repetitions, especially when conditioning on a low probability event.

Most importantly, keep in mind that any probability model is based on a series of assumptions and these assumptions are not satisfied exactly by the random phenomenon.
A precise estimate of a probability *under the assumptions of the model* is not necessarily a comparably precise estimate of the true probability.
Reporting probability estimates out to many decimal places conveys a false sense of precision and should typically be avoided.



For example, the probability that any particular coin lands on heads is probably not 0.5 *exactly*.
But any difference between the true probability of the coin landing on heads and 0.5 is likely not large enough to be practically meaningful^[There is actually [some evidence](https://www.stat.berkeley.edu/~aldous/Real-World/coin_tosses.html) that a coin flip is slightly more likely to land the same way it started; e.g, a coin that starts H up is more likely to land H up.  But the tendency is small.].
That is, assuming the coin is fair is a reasonable model.

Suppose we assume that the probability that a coin lands on heads is exactly 0.5 and that the results of different flips are independent.
If we flip the coin 1000 times the probability that it lands on heads at most 490 times is 0.2739864.
(We will see a formula for computing this value later.)
If we were to simulate one trillion repetitions (each consisting of 1000 flips) to estimate this probability then our margin of error would be 0.000001; we could expect accuracy out to the sixth decimal place. 
However, reporting the probability with so many decimal places is somewhat disingenuous.
If the probability that the coin lands on heads were 0.50001, then the probability of at most 490 heads in 1000 flips would be 0.2737757.
If the probability that the coin lands on heads were 0.5001, then the probability of at most 490 heads in 1000 flips would be 0.2718834.
Reporting our approximate probability as something like 0.2739864 $\pm$ 0.000001 says more about the precision in our *assumption* that the coin is fair than it does about the true probability that the coin lands on heads at most 490 times in 1000 flips.
A more honest conclusion would result from running 10000 repetitions and reporting our approximate probability as something like 0.27 $\pm$ 0.01.
Such a conclusion reflects more genuinely that there's some "wiggle room" in our assumptions, and that any probability computed according to our model is at best a reasonable approximation of the "true" probability. 




For most of the situations we'll encounter in this book, estimating a probability to within 0.01 of its true value will be sufficient for practical purposes, and so basing approximations on 10000 simulated values will be appropriate.  Of course, there are real situations where probabilities need to be estimated much more precisely, e.g., the probability that a bridge will collapse.  Such situations require more intensive methods.

 

## A more interesting example: Matching problem {#sim-matching-n}

Dice rolling provides a simple scenario for us to introduce ideas, but it's not very exciting.
Now we'll apply ideas from this section to investigate a more interesting problem: the matching problem.
We'll also see how we can combine Symulate and Python code.

Our version is from [FiveThirtyEight](https://fivethirtyeight.com/features/everythings-mixed-up-can-you-sort-it-all-out/).
A geology museum in California has $n$ different rocks sitting in a row on a shelf, with labels on the shelf telling what type of rock each is.
An earthquake hits and the rocks all fall off the shelf and get mixed up.
A janitor comes in and, wanting to clean the floor, puts the rocks back on the shelf in uniformly random order; each rock is equally likely to be placed in any spot.


Let $Y$ be the number of rocks put in the correct spot (i.e., the number of matches).
We considered the case $n=4$ in Example \@ref(exm:matching-probspace), where we found the distribution of $Y$; see Table \@ref(tab:matching-probspace-table).
When $n=4$ the probability of at least one match is 0.625.

When $n=4$ we could enumerate the $4!=24$ possible outcomes, but such a strategy is not feasible for a general $n$.
For example, if $n=60$ then there are $60! \approx 10^{82}$ possible outcomes, which is about the total number of atoms in the observable universe.
Therefore we need other strategies to investigate the problem.

We'll use simulation to investigate, for general $n$:

- the probability that at least one rock is placed in the correct spot (i.e., the probability of at least one match)
- the long run average number of rocks placed in the correct spot (i.e., the long run average number of matches)
- how these quantities depend on $n$.


Before proceeding, stop to think: what do you expect?
How do you expect the probability of at least one match to depend on $n$?
Will the probability increase, decrease, or stay about the same as $n$ gets larger?
What about the long run average number of matches?
It's always a good idea to think about a problem and make some initial guesses before just jumping into calculations or simulations.

```{example, matching-sim-describe, name='Matching problem'}

For a given $n$, describe in detail how you would use simulation to approximate:
  
1. The distribution of $Y$
1. The probability of at least one match.
1. The long run average number of matches

```


```{solution matching-sim-describe-sol}
to Example \@ref(exm:matching-sim-describe)

```





```{asis, fold.chunk = TRUE}

We could use a box model.

- The box has $n$ tickets, labeled $1, 2, \ldots, n$, one for each rock.
- An outcome is simulated by selecting $n$ tickets from the box *without* replacement and recording their order, e.g., (2, 1, 3, 4) if $n=4$.
- Let $y$ be the number of matches for the simulated outcome, e.g., $y=2$ for outcome (2, 1, 3, 4).
- The above two steps consist of one repetition, yielding one realized value of the random variable $Y$.
- Repeat these steps many times to obtain many simulated values of $Y$.

1. Summarize the simulated values of $Y$ and their relative frequencies to approximate the distribution of $Y$.
1. Count the number of repetitions on which $Y>0$ and divide by the total number of repetitions to approximate $\IP(Y>0)$, the probability of at least one match.
1. Compute the average of the simulated $Y$ values --- sum all simulated $Y$ values and divide by the number of simulated $Y$ values --- to approximate the long run average value of $Y$.

```

We'll start by coding a simulation for $n=4$.
We can compare our simulation results to the analytical results to check that our simulation process is working correctly.
Since Python uses zero-based indexing, we label the rocks $0, 1, \ldots, n-1$.

```{python}

n = 4

labels = list(range(n)) # list of labels [0, ..., n-1]
labels

```


Now we define the box model and simulate a few outcomes.  Note that `replace = False`.

```{python}

P = BoxModel(labels, size = n, replace = False)

P.sim(5)

```

We simulate many outcomes to check that they are roughly equally likely.

```{python}

P.sim(24000).tabulate()

```


Remember that a random variable $Y$ is a function whose inputs are the sample space outcomes.
In this example the function "counts matches", so would like to define $Y$ as `Y = RV(P, count_matches)`.
Unfortunately, such a function isn't built in like `sum` or `max`, but we can write a custom `count_matches` Python function ourselves.
The `count_matches` function below starts a counter at 0 and then goes spot-by-spot through each spot in the outcome and increments the counter by 1 any time there is a match.
Don't worry too much about the Python syntax yet.
What's important is that we have defined a *function* that we can use to define a random variable.


```{python}

def count_matches(omega):
    count = 0
    for i in range(0, n, 1):
        if omega[i] == labels[i]:
            count += 1
    return count
  
omega = (1, 0, 2, 3) # an example outcome, with 2 matches
count_matches(omega) # the function evaluated for the example outcome

```


Now we can use the function `count_matches` to define a Symbulate `RV` just like we have used `sum` or `max`.

```{python}

Y = RV(P, count_matches)

Y((1, 0, 2, 3))

```

We can simulate many values of $Y$ and use the simulated values to approximate the distribution of $Y$, the probability of at least one match, and the long run average value of $Y$.

```{python}

y = Y.sim(10000)

y.tabulate(normalize = True)

```

```{python}

y.count_gt(0) / y.count()

```

```{python}

y.mean()

```

The simulated distribution of $Y$ is close to the true distribution in Table \@ref(tab:matching-probspace-table); in particular, the simulated values are within the margin of error (about 0.01-0.02 for 10000 simulated values) of the true values.
We also see that the long run average value of $Y$ is around 1.


It appears that our simulation is working properly for $n=4$.
To investigate a different value of $n$, we simply need to revise the line `n=4`.
Because we want to investigate many values of $n$, we wrap all the above code in a Python function which takes $n$ as an input and outputs our objects of interest.

```{python}

def matching_sim(n):
    labels = list(range(n))
    def count_matches(omega):
        count = 0
        for i in range(0, n, 1):
            if omega[i] == labels[i]:
                count += 1
        return count
    
    P = BoxModel(labels, size = n, replace = False)
    Y = RV(P, count_matches)
    
    y = Y.sim(10000)
    
    plt.figure()
    y.plot('impulse')
    plt.show()
    
    return y.count_gt(0) / y.count(), y.mean()

```

For example, for $n=4$ we simply call

```{python}

matching_sim(4)

```

Now we can easily investigate different values of $n$. For example, for $n=10$ we see that the probability of at least one match is around 0.63 and the long run average number of matches is around 1.

```{python}

matching_sim(10)

```

We can use a for loop to automate the process of changing the value of $n$, running the simulation, and recording the results.
If `ns` is the list of $n$ values of interest we basically just need to run

```
for n in ns:
    matching_sim(n)
```

In Python we can also use list comprehension

```
[matching_sim(n) for n in ns]
```

The table below summarizes the simulation results for $n=4, \ldots, 10$.
The first line defines the values of $n$, and the second line implements the for loop.
The `tabulate` code just adds a little formatting to the table.
Note that we have temporarily redefined `matching_sim` to remove the lines that produced the plot, but we have not displayed the revised code here.
(We will get bring the plot back soon.)


```{python, echo = FALSE}

def matching_sim(n):
    labels = list(range(n))
    def count_matches(omega):
        count = 0
        for i in range(0, n, 1):
            if omega[i] == labels[i]:
                count += 1
        return count
    
    P = BoxModel(labels, size = n, replace = False)
    Y = RV(P, count_matches)
    
    y = Y.sim(10000)
    
    # plt.figure()
    # y.plot()
    # plt.show()
    
    return y.count_gt(0) / y.count(), y.mean()

```


```{python}

ns = list(range(4, 11, 1))

results = [matching_sim(n) for n in ns]

print(tabulate({'n': ns,
                'P(Y > 0), LRA': results},
               headers = 'keys', floatfmt=".3f"))
               
```

Stop and look at the table; what do you notice?
How do the probability of at least one match and the long run average value depend on $n$?
They don't!
Well, maybe they do, but they don't appear to change very much with $n$ after we take into account simulation margin of error^[The simulation margin of error for a single probability of at least one match is about 0.01 based on 10000 simulated values. We haven't discussed simulation margins of error for long run averages yet. In this case, the simulation margin of error for a single long run average based on 10000 simulated values is about 0.02.] of about 0.01-0.02.
It appears that regardless of the value of $n$, the probability of at least one match is around 0.63 and the long run average number of matches is around 1.

If we're interested in more values of $n$, we just repeat the same process with a longer list of `ns`.
The code below uses [Matplotlib](https://matplotlib.org/) to create a plot of the probability of at least one match and the long run average number of matches versus $n$.
While there is some natural simulation variability, we see that the probability of at least one match (about 0.63) and the long run average value (about 1) basically do not depend on $n$!

```{python, cache = TRUE}

ns = list(range(4, 101, 1))

results = [matching_sim(n) for n in ns]

plt.figure()
plt.plot(ns, results)
plt.legend(['P(Y > 0)', 'LRA'])
plt.xlabel('n')
plt.ylabel('value')
plt.show()

```

What about the distribution of $Y$?
Clicking on the picture below will launch a Colab notebook that contains code for investigating how the distribution of $Y$ depends on $n$.
The basic simulation code is identical to what we have already seen.
The notebook adds a few lines to create a Jupyter widget, which produces an interactive plot (with some additional formatting) of the distribution with a slider; you can change the slider to see how the distribution of $Y$ changes with $n$.
Take a few minutes to play with the slider; what do you see?

<script src="https://gist.github.com/kevindavisross/b6b6adeb13c3b10f1fa82c5261d7a931.js"></script>

You should see that unless $n$ is really small (like 4 or 5) the distribution of $Y$ is basically the same for any value of $n$!
And the distribution appears to follow a distribution called the Poisson(1) distribution.
In particular, the probability of exactly 0 matches is approximately equal to the probability of exactly 1 match.


Summarizing, our simulation investigation of the matching problem reveals that, unless $n$ is really small,

- the probability of at least one match does not depend on $n$, and is approximately 0.632.
- the long run average number of matches is approximately 1
- the distribution of the number of matches is approximately the same for all values of $n$
- the distribution of the number of matches is approximately the Poisson(1) distribution.

We will investigate these observations further later.
For now, marvel at the fact that no matter if there are 10 or 10 thousand or 10 million rocks and spots, there is about a 63% probability that at least one rock will be in the correct spot.
Amazing!

### Summary

- We can use Python code to
    - define functions to define random variables
    - define for loops to investigate changing parameters
    - customize plots produced by the Symbulate `plot` command (add axis labels, legends, etc)
    - summarize results of multiple simulations in tables and Matplotlib plots (e.g., for different values of problem parameters)
- Simulation provides an effective way for investigation probability problems
- Probability problems can have surprising results!
