# Simulating Distributions {#simulation2}

<!-- \newcommand{\IP}{\textrm{P}} -->
<!-- \newcommand{\IQ}{\textrm{Q}} -->
<!-- \newcommand{\E}{\textrm{E}} -->
<!-- \newcommand{\Var}{\textrm{Var}} -->
<!-- \newcommand{\SD}{\textrm{SD}} -->
<!-- \newcommand{\Cov}{\textrm{Cov}} -->
<!-- \newcommand{\Corr}{\textrm{Corr}} -->
<!-- \newcommand{\Xbar}{\bar{X}} -->
<!-- \newcommand{\Ybar}{\bar{X}} -->
<!-- \newcommand{\xbar}{\bar{x}} -->
<!-- \newcommand{\ybar}{\bar{y}} -->
<!-- \newcommand{\ind}{\textrm{I}} -->
<!-- \newcommand{\dd}{\text{DDWDDD}} -->
<!-- \newcommand{\ep}{\epsilon} -->
<!-- \newcommand{\reals}{\mathbb{R}} -->

In this chapter we will investigate some examples that further illustrate properties of discrete and continuous random variables and their distributions, the simulation process, and Symbulate code.

Recall that the **(probability) distribution** of a random variable specifies the possible values of the random variable and a way of determining corresponding probabilities.
The distribution of a random variable specifies the long run pattern of variation of values of the random variable over many repetitions of the underlying random phenomenon. The distribution of a random variable ($X$) can be approximated by

- simulating an outcome of the underlying random phenomenon ($\omega$)
- observing the value of the random variable for that outcome ($(X(\omega)$)
- repeating this process many times
- then computing relative frequencies involving the simulated values of the random variable to approximate probabilities of events involving the random variable (e.g., $\IP(X\le x)$).


We will discuss distributions in more detail in the next chapter, including their mathematical representations and properties.
The examples in the current chapter provide an introduction to some of the ideas.
One key idea is that **any distribution can be represented by a spinner**.
For example, the spinner on the left in Figure \@ref(fig:die-three-spinners) corresponds to a single roll of a fair four-sided die. 


## Simulating equally likely outcomes {#symbulate-discrete-uniform}

Consider again two rolls of a fair four-sided and let $X$ be the sum and $Y$ the larger of the two rolls (or the common value if a tie).
In the previous chapter we used a box model to specify the probability space correspoding to pairs of rolls of a four-sided die.
When tickets are equally likely and sampled with replacement, a **Discrete Uniform** model can also be used.
Think of a `DiscreteUniform(a, b)` probability space corresponding to a spinner with sectors of equal area labeled with integer values from `a` to `b` (inclusive).
For example, the spinner in Figure \@ref(fig:spinner-die) corresponds to `DiscreteUniform(1, 4)`.

In the following, $U$ represents the result of a *single* roll of a fair four-sided die.  The default mapping function in `RV` is the identity, so `U = RV(P)` represents^[For technical reasons, Symbulate will not plot simulated outcomes from a `ProbabilitySpace`, only simulated realizations of an `RV`.  Essentially, as mentioned in earlier sections, probability space outcomes can be *anything* --- not necessarily numbers --- and so there are too many potential situations and plots for Symbulate to handle with a single `plot` command.  In contrast, simulated realizations of `RV` are numbers (or vectors of numbers) which facilitates plotting.  Plotting simulated outcomes from a probability space `P` with numerical outcomes can be achieved by first defining `X = RV(P)` and then simulating and plotting values of `X`.  That is, replace `P.sim(10000).plot()` with `RV(P).sim(10000).plot()`.  However, this only works if the outcomes of `P` are numerical.] $U(\omega) = \omega$.

```{python}
P = DiscreteUniform(a = 1, b = 4)
U = RV(P)

U.sim(10000).plot()
plt.show()
```

For two rolls the probability space corresponds to spinning the DiscreteUniform spinner twice, which is coded^[For now you can interpret`DiscreteUniform(a = 1, b = 4)` as a spinner with four quarters labeled 1, 2, 3, 4, and `** 2` as "spin the spinner twice".  In Python, `**` represents exponentiation; e.g., `2 ** 5 = 32`.  So `DiscreteUniform(a = 1, b = 4) ** 2` is equivalent to `DiscreteUniform(a = 1, b = 4) * DiscreteUniform(a = 1, b = 4)`. Future sections will reveal why the product `*` notation is natural for *independent* spins of spinners.] as `DiscreteUniform(a = 1, b = 4) ** 2`.   The first line below has the same effect^[`BoxModel` assumes equally likely tickets by default, but there are [options for non-equally likely cases](https://dlsun.github.io/symbulate/probspace.html#boxmodel).] as `BoxModel([1, 2, 3, 4], size = 2, replace = True)`.

```{python, eval = FALSE}
P = DiscreteUniform(a = 1, b = 4) ** 2
X = RV(P, sum)
Y = RV(P, max)

(X & Y).sim(10000).plot(['tile', 'marginal'])
plt.show()
```

(ref:cap-dice-tile-marginal2) Tile and impulse plot visualization of the simulation-based approximate joint and marginal distributions of the sum ($X$) and larger ($Y$) of two rolls of a fair four-sided die.




```{r, dice-tile-marginal2, echo = FALSE, fig.cap = "(ref:cap-dice-tile-marginal2)"}

knitr::include_graphics("_graphics/dice-tile-marginal.png")
```



## Non-equally likely outcomes: A weighted die



<!-- The spinner on the left in Figure \@ref(fig:die-three-spinners) corresponds to a single roll of a fair four-sided die. But what about a weighted die like the one in Example \@ref(exm:die-weighted)?  Before considering the weighted die, let's look at the fair die in Symbulate. -->

<!-- Let $U$ be the result of a *single* roll of a four-sided die.  Let $\IP$ be the probability measure corresponding to a *fair* die. `BoxModel` assumes equally likely outcomes by default, so calling `BoxModel([1, 2, 3, 4])` assumes a fair die.  The random variable $U$ is just the outcome of this roll, identified by the identity function $U(\omega) = \omega$. (Recall that a Symbulate `RV` is always defined in terms of a probability space and a function `RV(probspace, function)`.  The default function is the identity: $g(\omega) = \omega$.) -->

<!-- ```{python} -->

<!-- P = BoxModel([1, 2, 3, 4]) -->
<!-- U = RV(P) -->

<!-- U.sim(10000).plot() -->
<!-- plt.show() -->

<!-- ``` -->

<!-- The plot displays a simulation-based approximation to the distribution of $U$ according to the probability measure $\IP$.  We see that the four values are equally likely.  This distribution can be represented by the spinner on the left in Figure \@ref(fig:die-three-spinners). -->

In the previous section we considered a *fair* four-sided die. 
Now consider the weighted die in Example \@ref(exm:die-weighted): a single roll results in 1 with probability 1/10, 2 with probability 2/10, 3 with probability 3/10, and 4 with probability 4/10.
Let $\IQ$ be the probability measure corresponding to the assumption that the die is weighted as in Example \@ref(exm:die-weighted).
We can specify non-equally likely outcomes in `BoxModel` using the `probs` option.
The probability space `Q` in the following code corresponds to a single roll of the weighted die. 
Note that $U$ is still defined via the identity function $U(\omega) = \omega$. (Recall that a Symbulate `RV` is always defined in terms of a probability space and a function `RV(probspace, function)`.  The default function is the identity: $g(\omega) = \omega$.) 


```{python}

Q = BoxModel([1, 2, 3, 4], probs = [0.1, 0.2, 0.3, 0.4], size = 1)
U = RV(Q)

U.sim(10000).plot()
plt.show()

```

The plot displays a simulation-based approximation to the distribution of $U$, but now according to the probability measure $\IQ$.  This distribution can be represented by the middle spinner in Figure \@ref(fig:die-three-spinners).


<!-- (ref:cap-spinner-die-weighted) Spinner corresponding to a single roll of the weighted four-sided die in Example \@ref(exm:die-weighted). -->

<!-- ```{r spinner-die-weighted, echo=FALSE, fig.cap="(ref:cap-spinner-die-weighted)"} -->

<!-- knitr::include_graphics("_graphics/spinner-die-weighted.png") -->

<!-- ``` -->


In the two scenarios --- fair die versus weighted die ---

- the sample space is the same, $\Omega=\{1,2,3,4\}$, and
- the random variable is the same function, $U(\omega) = \omega$.

What changes is the probability measure, from $\IP$ (fair die) to $\IQ$ (weighted die). 
*Changing the probability measure changes the distribution of $U$.*

Another way to model a weighted die is with a box model with 10 tickets --- one ticket labeled 1, two tickets labeled 2, three tickets labeled 3, and four tickets labeled 4 --- from which a single ticket is drawn.  A `BoxModel` can be specified in this way using the following `{label: number of tickets with the label}` formulation^[Braces `{}` are used here because this defines a Python *dictionary*.  But don't confuse this code with set notation.]. This formulation is especially useful when multiple tickets are drawn from the box *without replacement*.

```{python}

Q = BoxModel({1: 1, 2: 2, 3: 3, 4: 4}, size = 1)
U = RV(Q)

U.sim(10000).plot()
plt.show()

```


### Summary


- Changing a probability measure changes distributions of random variables.
- Distributions can be represented by spinners.
- Box models can handle situations with non-equally likely outcomes.
In Symbulate, `BoxModel` has options like `probs` that can be used to specify probabilities of individual outcomes.

### Exercises

1. Recall Example \@ref(exm:dice-normalize). Write the Symbulate code to model this situation, in two ways.

    a. Using the `probs` option of `BoxModel`.
    a. Using the `{label: number of tickets with the label}` form of `BoxModel`.


## Simulating from distributions: rolling dice yet again


Consider yet again the sum $X$ and max $Y$ of two rolls of a fair four-sided die. (We promise we'll move on to more interesting examples soon!)
Section \@ref(dist-intro) discussed the joint and marginal probability distributions of $X$ and $Y$.
In Section \@ref(technology-intro) we simulated values of $X$ and $Y$ by first simulating dice rolls.  Now we'll see another way.

```{example dice-spinners-ex}
Construct a spinner representing each of the following.
```

1. The marginal distribution of $X$.
1. The marginal distribution of $Y$.
1. The joint distribution of $X$ and $Y$.

```{solution dice-spinners-ex-sol}
to Example \@ref(exm:dice-spinners-ex).
```


```{asis, fold.chunk = TRUE}

1. The spinner in Figure \@ref(fig:dice-spinners-sum) represents the marginal distribution of $X$; see Table \@ref(tab:dice-sum-dist-table).
1. The spinner in Figure \@ref(fig:dice-spinners-max) represents the marginal distribution of $Y$; see Table \@ref(tab:dice-max-dist-table).
1. The spinner in Figure \@ref(fig:dice-spinners-sum-max) represents the joint distribution of $X$ and $Y$.
For example, the spinner returns the pair $(4, 2)$ with probability 1/16 and the pair $(4, 3)$ with probability 2/16.
See Table \@ref(tab:dice-joint-dist-twoway) or Table \@ref(tab:dice-joint-dist-flat).
Remember, a joint distribution of two random variables is a distribution of pairs on values.

``` 

(ref:cap-dice-spinners-sum) Spinner representing the marginal distribution of $X$, the sum of two rolls of a fair four-sided die. 

```{r dice-spinners-sum, echo=FALSE, fig.cap="(ref:cap-dice-spinners-sum)"}

knitr::include_graphics(c(
  "_graphics/spinner-dice-sum-marginal.png"))

```


(ref:cap-dice-spinners-max) Spinner representing the marginal distribution of $Y$, the larger of two rolls of a fair four-sided die.

```{r dice-spinners-max, echo=FALSE, fig.cap="(ref:cap-dice-spinners-max)"}

knitr::include_graphics(c(
  "_graphics/spinner-dice-max-marginal.png"))

```

(ref:cap-dice-spinners-sum-max) Spinner representing the joint distribution of $X$ and $Y$, the sum and the larger of two rolls of a fair four-sided die.

```{r dice-spinners-sum-max, echo=FALSE, fig.cap="(ref:cap-dice-spinners-sum-max)"}

knitr::include_graphics(c(
  "_graphics/spinner-dice-sum-max.png"))

```
                      
<!-- ```{r dice-spinners, echo=FALSE, fig.cap="(ref:cap-dice-spinners)", out.width='33%', fig.show='hold'} -->

<!-- make_discrete_spinner <- function(x, p){ -->
<!--   xp <- data.frame(x, p) -->
<!--   cdf = c(0, cumsum(xp$p)) -->
<!--   plotp = (cdf[-1] + cdf[-length(cdf)]) / 2 -->
<!--   ggplot(xp, aes(x="", y=p, fill=x))+ -->
<!--     geom_bar(width = 1, stat = "identity", color="black", fill="white") +  -->
<!--     coord_polar("y", start=0) + -->
<!--     blank_theme + -->
<!--     # plot the possible values on the outside -->
<!--     scale_y_continuous(breaks = plotp, labels=xp$x) + -->
<!--     theme(axis.text.x=element_text(size=16, face="bold")) + -->
<!--     # plot the probabilities as percents inside -->
<!--     geom_text(aes(y = plotp, -->
<!--                   label = format(percent(round(p, 3)),0)), size=6) -->
<!-- } -->

<!-- x = 2:8 -->
<!-- px = c(1, 2, 3, 4, 3, 2, 1) / 16 -->
<!-- y = 1:4 -->
<!-- py = c(1, 3, 5, 7) / 16 -->


<!-- make_discrete_spinner(x, px) -->

<!-- make_discrete_spinner(y, py) -->

<!-- # make_discrete_spinner(xy, pxy) -->

<!-- ``` -->

In principle, there are always two ways of simulating a value $x$ of a random variable $X$.

1. **Simulate from the probability space.** Simulate an outcome $\omega$ from the underlying probability space and set $x = X(\omega)$.
1. **Simulate from the distribution.** Construct a spinner corresponding to the distribution of $X$ and spin it once to generate $x$.

The second method requires that the distribution of $X$ is known.
However, as we will see in many examples, *it is common to specify the distribution of a random variable directly without defining the underlying probability space*.
Recall the brown bag analogy in Section \@ref(rv-function) and the discussion at the end of Section \@ref(dist-intro).



```{example dice-marginal-sim-from-dist}
We have seen the Symbulate code for the "simulate from the probability space" method in the dice rolling example.
Now we consider the "simulate from the distribution method".

1. Write the Symbulate code to define $X$ by specifying its marginal distribution.
1. Write the Symbulate code to define $Y$ by specifying its marginal distribution.

```

```{solution dice-marginal-sim-from-dist-sol}

to Example \@ref(exm:dice-marginal-sim-from-dist).

```

```{asis, fold.chunk = TRUE}

We simulate a value of $X$ from its distribution by spinning the spinner in Figure \@ref(fig:dice-spinners-sum).
Similarly, we simulate a value of $Y$ from its distribution by spinning the spinner in Figure \@ref(fig:dice-spinners-max).
We can define a BoxModel corresponding to each of these spinners, and then define a random variable through the identify function.  Essentially, we define the random variable by specifying its distribution, rather specifying the underlying probability space.
Note that the default `size` argument in `BoxModel` is `size = 1`, so we have omitted it.



```

```{python}

X = RV(BoxModel([2, 3, 4, 5, 6, 7, 8], probs = [1 / 16, 2 / 16, 3 / 16, 4 / 16, 3 / 16, 2 / 16, 1 / 16]))

Y = RV(BoxModel([1, 2, 3, 4], probs = [1 / 16, 3 / 16, 5 / 16, 7 / 16]))
```

We can specify the distribution of a discrete random variable via a box model with one ticket for each possible value of the random variable, along with its corresponding probability.


The joint distribution in Table \@ref(tab:dice-joint-dist-twoway) corresponds to the spinner in Figure \@ref(fig:dice-spinners-sum-max).
We now have two ways to simulate an $(X, Y)$ pair that has the distribution in Table \@ref(tab:dice-joint-dist-twoway).

1. Simulate two rolls of a fair four sided die.  Let $X$ be the sum of the two values and let $Y$ be the larger of the two rolls (or the common value if a tie).
1. Spin the spinner in Figure \@ref(fig:dice-spinners-sum-max) once and record the resulting $(X, Y)$ pair.  (Recall that this spinner returns a pair of values.) Of course, this method requires that the joint distribution of $(X, Y)$ is known.


Below is the Symbulate code for simulating directly from the joint distribution in Table \@ref(tab:dice-joint-dist-twoway).   Note that the tickets in `BoxModel` correspond to the possible $(X, Y)$ pairs, which are not equally likely (even though the 16 pairs of rolls are).  We specify the probability of each ticket by using the `probs` option.  To generate a single $(X, Y)$ pair --- like spinning the spinner in Figure \@ref(fig:dice-spinners-sum-max) once --- we draw one ticket from the box of pairs; this is why^[By default `size = 1`, so we could have omitted it, but we include it for emphasis. Drawing 1 ticket from this box model returns an $(X, Y)$ *pair*.] `size = 1`.

```{python}

xy_pairs = [(2, 1), (3, 2), (4, 2), (4, 3), (5, 3), (5, 4), (6, 3), (6, 4), (7, 4), (8, 4)]
pxy = [1/16, 2/16, 1/16, 2/16, 2/16, 2/16, 1/16, 2/16, 2/16, 1/16]

P = BoxModel(xy_pairs, probs = pxy, size = 1)

P.sim(5)

```

We can now define random variables $X$ and $Y$. An outcome of `P` is a pair of values.  Recall that a Symbulate `RV` is always defined in terms of a probability space and a function `RV(probspace, function)`.  The default function is the identity: $g(\omega) = \omega$.  Therefore, `RV(P)` would just correspond to the pair of values generated by `P`. The sum $X$ corresponds to the first coordinate in the pair and the max $Y$ corresponds to the second.  We can define these random variables in Symbulate by "unpacking" the pair as in the following^[Since `P` returns pairs of outcomes, `Z = RV(P)` is a random *vector*.  Components of a vector can be indexed with brackets `[]`; e.g., the first component is `Z[0]` and the second is `Z[1]`.  (Remember: Python uses zero-based indexing.)  So the "unpacked" code is an equivalent but simpler version of `Z = RV(P); X = Z[0]; Y = Z[1]`.]

```{python}
X, Y = RV(P)
```

Then we can simulate many $(X, Y)$ pairs and summarize as we did in Section \@ref(sym-joint).


```{python}
(RV(P) & X & Y).sim(5)

```


<!-- ```{python} -->

<!-- xy = (X & Y).sim(16000) -->

<!-- plt.figure() -->
<!-- xy.plot(['tile', 'marginal']) -->
<!-- plt.show() -->
<!-- xy.tabulate() -->

<!-- ``` -->







```{example dd-dice-marginal-sim, name='(ref:ddwddd)'}

Donny says "Forget the joint distribution spinner in Figure \@ref(fig:dice-spinners-sum-max).
I can simulate an $(X, Y)$ pair just by spinning the spinner in Figure \@ref(fig:dice-spinners-sum) to generate $X$ and the one in Figure \@ref(fig:dice-spinners-max) to generate $Y$."
Is Donny correct?
If not, can you help him see why not?

```

```{solution dd-dice-marginal-sim-sol}
to Example \@ref(exm:dd-dice-marginal-sim)

```


```{asis, fold.chunk = TRUE}
 
Donny is not correct.
Yes, spinning the $X$ spinner in Figure \@ref(fig:dice-spinners-sum) will generate values of $X$ according to the proper marginal distribution, and similarly with Figure \@ref(fig:dice-spinners-max) and $Y$.
However, spinning each of the spinners will *not* produce $(X, Y)$ pairs with the correct *joint* distribution.
For example, Donny's method could produce $X=2$ and $Y=4$, which is not a possible $(X, Y)$ pair.
Donny's method treats the values of $X$ and $Y$ as if they were *independent*; the result of the $X$ spin would not change what could happen with the $Y$ spin (since the spins are physically independent).
However, the $X$ and $Y$ values are related.
For example, if $X=2$ then $Y$ must be 1; if $X=4$ then $Y$ must be 2 or 3; etc.
The joint distribution spinner in Figure \@ref(fig:dice-spinners-sum-max) correctly reflects the relationship between $X$ and $Y$.
But as we discussed in Section \@ref(dist-intro), in general you cannot recover the joint distribution from the marginal distributions, which is what Donny is attempting to do.

```

Donny's method corresponds to (1) rolling the die twice and summing to get $X$, then (2) rolling the die two more times and finding the larger roll to get $Y$.  Essentially, Donny is not using the same probability space for $X$ and $Y$, and therefore events involving both random variables cannot be studied.  In Symbulate, Donny's code --- *which would produce an error* --- would look like

```
X = RV(BoxModel([1, 2, 3, 4], size = 2), sum)
Y = RV(BoxModel([1, 2, 3, 4], size = 2), max)

(X & Y).sim(10000)

### Error: Events must be defined on same probability space.
```

In Donny's code, his random variables are defined on different probability spaces; one box model is used to generate the rolls for $X$ and a separate box model is used to generate the rolls for $Y$.  As we have mentioned a few times, random variables (and events) must all be defined on the same probability space^[If Donny *really* wanted to simulate two independent pairs of rolls, one to compute $X$ and one to compute $Y$, he would still need define the random variables on the same probability space, using `BoxModel([1, 2, 3, 4], size = 2) ** 2` for which an example outcome would be ((3, 2), (1, 1)).  Then he could define `X=RV(P)[0].apply(sum)` and `Y=RV(P)[1].apply(max)`.  But it's hard to justify why Donny would want to do this.].

```{example dd-dice-joint-sim, name='(ref:ddwddd)'}

Donny says "I see what you mean about needing the spinner in Figure \@ref(fig:dice-spinners-sum-max) to simulate $(X, Y)$ pairs.
So then forget the spinners in Figure \@ref(fig:dice-spinners-sum) and Figure \@ref(fig:dice-spinners-max).
If I want to simulate $X$ values, I could just spin the joint distribution spinner in Figure \@ref(fig:dice-spinners-sum-max) and ignore the $Y$ values."
Is Donny's method correct?
If not, can you help him see why not?

```

```{solution dd-dice-joint-sim-sol}
to Example \@ref(exm:dd-dice-joint-sim)

```

```{asis, fold.chunk = TRUE}

Donny is correct!
The joint distribution spinner in Figure \@ref(fig:dice-spinners-sum-max) correctly produces $(X, Y)$ pairs according to the joint distribution in Table \@ref(tab:dice-joint-dist-twoway).
Ignoring the $Y$ values is like "summing the rows" and only worrying about what happens in total for $X$.
For example, in the long run, 1/16 of spins will generate (4, 2) and 2/16 of spins will generate (4, 3), so ignoring the $y$ values, 3/16 of spins will return an $x$ value of 4.
From the joint distribution  you can always find the marginal distributions (e.g., by finding row and column totals).

Now, Donny's method does work, but it does require more work than necessary.
If we really only needed to simulate $X$ values, we only need the distribution of $X$ and not the joint distribution of $X$ and $Y$, so you could use the $X$ spinner in Figure \@ref(fig:dice-spinners-sum).
But if we wanted to study anything about the relationship between $X$ and $Y$ then we would need the joint distribution spinner.

```



### Summary


- There are two ways to simulate a value of a random variable.
    - Simulate an outcome from the underlying probability space and evaluate the random variable for the simulated outcome.
    - Identify the distribution of the random variable, and simulate a value from that distribution (e.g., by constructing a spinner).
- We can specify the distribution of a discrete random variable via a box model with one ticket for each possible value of the random variable, along with its corresponding probability.
- The joint distribution of two random variables can be represented by a spinner that returns pairs of values.
- To simulate an $(X, Y)$ pair it is, in general^[When $X$ and $Y$ are *independent* it is sufficient to simulate values of $X$ and $Y$ separately from their respective marginal distributions.  We will study independence in detail later.], *not* sufficient to simulate a value of $X$ from its marginal distribution and a value of $Y$ from its marginal distribution.  Instead, a pair $(X, Y)$ must be simulated from the joint distribution.


### Exercises



## Outcomes on a continuous scale: Uniform distributions {#sec-linear-rescaling}



```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(ref:cap-uniform-event)"}

dfA <- data.frame(x = c(0, 2, 1, 0),
                 y = c(0, 1, 1, 0),
                 v = c(1, 1, 1, 1))

pA <- ggplot(data = dfA, aes(x = x, y = y)) +
  geom_polygon(fill="cornflowerblue", show.legend = FALSE) +
  scale_x_continuous(limits = c(0, 2), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
 # theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression(X)) +
  ylab(expression(Y)) +
  ggtitle("Joint pdf of sum (X) and max (Y) of two rolls of a fair four-sided die") +
  theme(plot.title = element_text(hjust = 0.5)) # +
#  scale_fill_continuous(guide = "colourbar")



```

Uniform probability measures are the continuous analog of equally likely outcomes.  The standard uniform model is the Uniform(0, 1) distribution corresponding to the spinner in Figure \@ref(fig:uniform-spinner) which returns values between^[Why is the interval $[0, 1]$ the standard instead of some other range of values?  Because probabilities take values in $[0, 1]$.  We will see why this is useful in more detail later.] 0 and 1.  Recall that the values in the picture are rounded to two decimal places, but the spinner represents an idealized model where the spinner is infinitely precise so that any real number between 0 and 1 is a possible value. We assume that the (infinitely fine) needle is "equally likely" to land on any value between 0 and 1.

For example, if Regina is "equally likely" to arrive for lunch at any time between noon (time 0) and 1:00PM (time 1), then a Uniform(0, 1) distribution is an appropriate probability model for her arrival time.

The following Symbulate code defines a probability space representing the Uniform(0, 1) model, and a random variable equal to the result of a single spin: $U(\omega)=\omega$.  Recall that the default function used to define a Symbulate `RV` is the identity.


```{python}

P = Uniform(0, 1)
U = RV(P)

U.sim(10)

```

Notice the number of decimal places.  Remember that for a continuous random variable, any value in some uncountable interval is possible.  For the Uniform(0, 1) distribution, any value in the continuous interval between 0 and 1 is a distinct possible value: 0.25000000000... is different from 0.25000000001... is distinct from 0.2500000000000000000001... and so on.

The rug plot displays 100 simulated values of $U$.  Note that the values seem to be "evenly spread" between 0 and 1.


```{python}

u = U.sim(100)

u.plot('rug')
plt.show()

```

A **histogram** groups the observed values into "bins" and plots frequencies for each bin^[Symbulate chooses the number of bins automatically, but you can set the number of bins using the `bins` option, e.g., `.plot(bins = 10)`]. 

```{python}

u.plot(['rug', 'hist'], bins = 10, normalize = False)
plt.show()

```

A histogram is the Symbulate default plot for summarizing values on a continuous scale. Typically, in a histogram *areas* of bars represent relative frequencies; in which case the axis which represents the height of the bars is called "density".  It is recommended that the bins all have the same width so that the ratio of the heights of two different bars is equal to the ratio of their areas. That is, with equal bin widths, bars with the same height represent the same area/relative frequency.  Symbulate will always produces a histogram with equal bin widths.

Now we simulate many values of $U$.  We see that the bars all have roughly the same height, represented by the horizontal line, and hence the same area, though there are some natural fluctuations due to the randomness in the simulation.

```{python}

u = U.sim(10000)

u.plot()
Uniform(0, 1).plot() # plots the horizontal line of constant height
plt.show()

```

Recall that in a uniform probability model, the probability of an event is proportional to the size (length, area, volume) of the set comprising the event; two events with the same size will have the same probability.

\[
\IP(A) = \frac{|A|}{|\Omega|} = \frac{\text{size of } A}{\text{size of } \Omega} \qquad \text{if $\IP$ is a uniform probability measure}
\]

For example, if $U$ follows a Uniform(0, 1) distribution then $\IP(U < 0.25) = 0.25 = \IP(U > 0.75)$.  We can approximate $\IP(U < 0.25)$ and $\IP(U > 0.75)$ with their corresponding simulated relative frequencies, which are roughly equal to 0.25. (Remember that the simulation margin of error based on 10000 repetitions is roughly $1/\sqrt{10000} = 0.01$.)  That is, the *area* of the histogram corresponding to the region $U<0.25$ is approximately 0.25.



```{python}
u.plot()
plt.axvspan(0, 0.25, alpha = 0.5, color = 'gray'); # shades the plot
plt.show()

u.count_lt(0.25) / 10000

```

```{python}
u.plot()
plt.axvspan(0.75, 1, alpha = 0.5, color = 'gray'); # shades the plot
plt.show()

u.count_gt(0.75) / 10000

```

As discussed in Section \@ref(sec-uniform-prob), the probability that the continuous random variable $U$ equals any particular value (e.g., 0.25) precisely is 0.

```{python}

u.count_eq(0.25)

```

However, the probability that $U$ is "close to" a value is non-zero.  For example, the probability that $U$ is within 0.01 of 0.25 --- that is, that $U$ lies in the interval (0.24, 0.26) of length 0.02 --- is 0.02.  That is, $\IP(|U - 0.25| < 0.01) = 0.02$.

```{python}

abs(u - 0.25).count_lt(0.01) / 10000

```

We can approximate the long run average value of continuous random variables in the usual way: simulate many values and average.  We see that the simulated average is approximately 0.5, the "balance point" of the distribution.

```{python}

u.mean()

```


The standard uniform distribution, Uniform(0, 1), is a distribution on the interval $[0, 1]$. The uniform distribution on the interval $[a, b]$, for $a<b$, is called the Uniform($a$, $b$) distribution.

For example, suppose that $X$, the SAT Math score of a randomly selected student, follows a Uniform distribution on the interval $[200, 800]$.  (This is certainly NOT true, and we will consider a more realistic distribution later.)  Remember that we can define a random variable by specifying its distribution.





```{python}

X = RV(Uniform(200, 800))

x = X.sim(10000)

x.plot()
Uniform(200, 800).plot() # draws the horizontal line of constant height
plt.show()

x.count_lt(300) / 10000, x.count_lt(400) / 10000, x.count_lt(500) / 10000
```

The plots show that the values are roughly uniformly distributed between 200 and 800.  Recall from Section \@ref(sec-uniform-prob) that for a continuous uniform distribution in one-dimension, probability is a ratio of lengths.  For example, $\IP(X \le 300)$ $= \frac{300-200}{800-200}$ $= \frac{100}{600}\approx 0.167$.  About 17\% of values are between 200 and 300, about 17% between 300 and 400, and about 17% between 400 and 500.  Each of these intervals is length 100, and the total length of the interval of possible values is 600, so the theoretical probability for each interval is $100/600\approx 0.167$.  The long run average value is 500, the midpoint of the interval $[200, 800]$.

```{python}

x.mean()

```

Remember that in a histogram, *area* represents relative frequency.  The Uniform(200, 800) distribution covers a wider range of possible values than the Uniform(0, 1) distribution.  Notice how the values on the vertical density axis change to compensate for the longer range on the horizontal variable axis.  The histogram bars now all have a height of roughly $\frac{1}{800-200} = \frac{1}{600} = 0.00167$ (aside from natural simulation variability).  The total area of the rectangle with a height of $\frac{1}{800-200}$ and a base of $800-200$ is 1.

### Summary


- A histogram can be used to summarize the distribution of a random variable that takes values on a continuous scale.
- When plotting values on a continuous scale in a histogram, relative frequencies are represented by areas.
- For a Uniform($a$, $b$) distribution, the probability of a subinterval of $[a, b]$ is proportional to the length of the interval.


## Normal distributions {#sim-normal}

In the previous section we assumed a Uniform(200, 800) distribution for $X$, the SAT Math score of a single randomly selected student.  The corresponding spinner would be like the one in Figure \@ref(fig:uniform-spinner)  but now labeled with equally spaced values from 200 to 800 (instead of 0 to 1).  However, this would not lead to very realistic SAT scores.  The average SAT Math score is around 500, and a much higher percentage of students score closer to average than to the extreme scores of 200 or 800.

To simulate SAT Math scores, we might use a spinner like the following.  Notice that the values on the spinner axis are *not* equally spaced.  Even though only some values are displayed on the spinner axis, imagine this spinner represents an infinitely fine model where any value between 200 and 800 is possible^[Technically, for a Normal distribution, *any* real value is possible.  But values that are more than 3 or 4 standard deviations occur with small probability.].


(ref:cap-sat-normal-spinner) A spinner representing the "Normal(500, 100)" distribution. The spinner is duplicated on the right; the highlighted sectors illustrate the non-linearity of axis values and how this translates to non-uniform probabilities.


```{r sat-normal-spinner, echo=FALSE, fig.cap="(ref:cap-sat-normal-spinner)", out.width='50%', fig.show='hold'}

knitr::include_graphics(c("_graphics/spinner-normal-sat.png",
                          "_graphics/spinner-normal-sat-sectors.png"))

```



Since the axis values are not evenly spaced, different intervals of the same length will have different probabilities.  For example, the probability that this spinner lands on a value in the interval [400, 500] is about 0.341, but it is about 0.136 for the interval [300, 400]. 

Consider what the distribution of values simulated using this spinner would look like.

- About half of values would be below 500 and half above
- Because axis values near 500 are stretched out, values near 500 would occur with higher frequency than those near 200 or 800.
- The shape of the distribution would be symmetric about 500 since the axis spacing of values below 500 mirrors that for values above 500.
For example, about 34% of values would be between 400 and 500, and also 34% between 500 and 600.
- About 68% of values would be between 400 and 600.
- About 95% of values would be between 300 and 700.

And so on.  We could compute percentages for other intervals by measuring the areas of corresponding sectors on the spinner to complete the pattern of variability that values resulting from this spinner would follow.  This particular pattern is called a "Normal(500, 100)" distribution. Note that the arguments for a Normal distribution play a different role than those for a Uniform distribution.  In a Uniform($a, b$) distribution, $a$ represents the minimum possible value and $b$ the maximum.  In a Normal($\mu$, $\sigma$) distribution, $\mu$ represents the *long run mean* (a.k.a. long run average) and $\sigma$ the *standard deviation*.  We will discuss standard deviation in more detail soon.

As in the previous section we can define a random variable by specifying its distribution.


```{python}

X = RV(Normal(500, 100))

```

We can then simulate values.  Remember that the Normal distribution is only a model for the distribution of SAT Math scores.  In particular, a Normal distribution assumes values on a continuous scale.  Also, it is possible to see values outside of the range $[200, 800]$, though such values do not occur often.

```{python}

x = X.sim(100)
x
```

Plotting the values, we see that values near 500 occur more frequently than those near 200 or 800.

```{python}

x.plot('rug')
plt.show()

```

```{python, eval = FALSE, include = FALSE, echo = FALSE}

x.plot(['rug', 'hist'])
plt.show()

```

We now simulate many values.


(ref:cap-normal-sat-density) Histogram representing the approximate distribution of values simulated using the spinner in Figure \@ref(fig:sat-normal-spinner).  The smooth solid curve models the theoretical shape of the distribution of $X$, called the "Normal(500, 100)" distribution). 

```{python}

x = X.sim(10000)
x
```

We see that the histogram appears like it can be approximated by a smooth, "bell-shaped" curve, called a *Normal density*.

```{python normal-sat-density, fig.cap="(ref:cap-normal-sat-density)"}

x.plot() # plot the simulated values
Normal(500, 100).plot() # plot the smooth density curve
plt.show()

```

The parameter 500 represents the long run average (a.k.a. mean) value.  Calling `x.mean()` will compute an average as usual: sum the 10000 simulated values in `x` and divide by 10000.  This average should be close to 500.  The more simulated values included in the average, the closer we would expect the simulated average value to be to 500.

```{python}

x.mean()

```


### Summary


- Spinners with non-evenly spaced values can be used to generate values from non-Uniform distributions 
- Normal distributions are common models of situations where the pattern of variability follows a bell-shaped curve centered at the average value.



## Standard deviation {#sd}

The parameter 100 in the Normal(500, 100) distribution represents the standard deviation, which is a measure of overall degree of variability.  While the average is 500, the values vary about that average.  Many values are close to the average, but some are farther away.  The standard deviation measures, roughly, the average distance of the values from their mean.  Calling `x.sd()` will compute the standard deviation of the simulated values in `x`.

```{python}

x.sd()

```

Roughly, standard deviation measures the average distance from the mean: For each simulated value compute its absolute distance from the mean, and then average these distances.

```{python}

abs(x - x.mean())

```

```{python}

abs(x - x.mean()).mean()

```


Unfortunately, the above calculation yields roughly 80 rather than the value of roughly 100 that `x.sd()` returns. The above calculation illustrates the concept of standard deviation as average distance from the mean, but the actual calculation of standard deviation is a little more complicated. Technically, you must first *square* all the distances and  then average; the result is the *variance*.  The standard deviation is then  the square root of the variance.  The standard deviation is measured in the measurement units of the random variable.  For example, if the random variable is measured in inches, then standard deviation is also measured in inches, while variance is measured in square-inches. We will see the theory of variance and standard deviation later.

The following code shows the "long way" of computing standard deviation.
First, find the squared distance between each simulated value and the mean.

```{python}

(x - x.mean()) ** 2 

```

Then average the squared distances; this is the variance.
The average is computed in the usual way: sum all the values and divide by the number of values^[If you have some familiarity with statistics, you might have seen a formula for standard deviation that includes dividing by *one less* than the number values ($n-1$). Dividing by $n$ or $n-1$ could make a difference in a small sample of data.  However, we will always be interested in *long run* averages, and it typically won't make any practical difference whether we divide by say 10000 or 9999.].
But now each value included in the average is the squared deviation of `x` from the mean, rather than the value of `x` itself.

```{python}

((x - x.mean()) ** 2).mean()

```

Now take the square root to get the standard deviation.
The value is the same as the one returned by `x.sd()`.

```{python}

sqrt(((x - x.mean()) ** 2).mean())

```

For comparison, consider values from the Uniform(200, 800) distribution.  While the Uniform(200, 800) and Normal(500, 100) distributions have the same mean, the Uniform(200, 800) has a larger standard deviation than the Normal(500, 100) distribution.  In comparison to a Normal(500, 100) distribution, a Uniform(200, 800) distribution will give higher probability to ranges of values near the extremes of 200 and 800, as well as lower probability to ranges of values near 500.  Thus, there will be more values far from the mean of 500 and fewer values close, and so the average distance from the mean and hence standard deviation will be larger for the Uniform(200, 800) distribution than for the Normal(500, 100) distribution. 

```{python}

RV(Normal(500, 100)).sim(10000).plot()
RV(Uniform(200, 800)).sim(10000).plot()
plt.show()

```

In a Uniform(200, 800) distribution, values are "evenly spread" from 200 to 800, so distances from the mean are "evenly spread" from 0 (for 500) to 300 (for 200 and 800).  We might expect the standard deviation to be about 150; it turns out to be about 173.  While the "average distance" interpretation helps our conceptual understanding of standard deviation, the process of squaring the distances, then averaging, and then taking the square root makes guessing the actual value of standard deviation difficult.

```{python}
RV(Uniform(200, 800)).sim(10000).sd()
```


### Summary


- Variability is an essential feature of a distribution.  Standard deviation measures degree of variability in terms of, roughly, the average distance from the mean. More precisely
   - **Variance** is the long run average of the squared deviations from the mean
   - **Standard deviation** is the square root of the variance




<!-- ## Bivariate Normal; correlation -->

<!-- Start with uniform sum/max. -->

<!-- The BVN.  Then correlation. -->


## Transformations of random variables: Linear rescaling {#linear-rescaling}

A function of a random variable is a random variable: if $X$ is a random variable and $g$ is a function then $Y=g(X)$ is a random variable.  In general, the distribution of $g(X)$ will have a different shape than the distribution of $X$.  The exception is when $g$ is a linear rescaling.


A **linear rescaling** is a transformation of the form $g(u) = a +bu$, where $a$ (intercept) and $b$ (slope^[You might be familiar with "$mx+b$" where $b$ denotes the intercept. In Statistics, $b$ is often used to denote slope. For example, in R `abline(a = 32, b = 1.8)` draws a line with intercept 32 and slope 1.8.]) are constants.
For example, converting temperature from Celsius to Fahrenheit using $g(u) = 32 + 1.8u$ is a linear rescaling.

A linear rescaling "preserves relative interval length" in the following sense.

  - If interval A and interval B have the same length in the original measurement units, then the rescaled intervals A and B will have the same length in the rescaled units. For example, [0, 10] and [10, 20] Celsius, both length 10 degrees Celsius, correspond to [32, 50] and [50, 68] Fahrenheit, both length 18 degrees Fahrenheit.
  - If the ratio of the lengths of interval A and B is $r$ in the original measurement units, then the ratio of the lengths in the rescaled units is also $r$. For example, [10, 30] is twice as long as [0, 10] in Celsius; for the corresponding Fahrenheit intervals, [50, 86] is twice as long as [32, 50].

Think of a linear rescaling as just a relabeling of the variable axis.  

```{r, echo = FALSE}

knitr::include_graphics('_graphics/celsius-fahrenheit.png')

```

Suppose that $U$ has a Uniform(0, 1) distribution and define $X = 200 + 600 U$.  Then $X$ is a linear rescaling of $U$, and $X$ takes values in the interval [200, 800].  We can define and simulate values of $X$ in Symbulate.  Before looking at the results, sketch a plot of the distribution of $X$ and make an educated guess for its mean and standard deviation.

```{python}

U = RV(Uniform(0, 1))

X = 200 + 600 * U

(U & X).sim(10)

```


```{python}

X.sim(10000).plot()
plt.show()

```

We see that $X$ has a Uniform(200, 800) distribution.  The linear rescaling changes the range of possible values, but the general shape of the distribution is still Uniform.  We can see why by inspecting a few intervals on both the original and revised scale.

| Interval of $U$ values | Probability that $U$ lies in the interval | Interval of $X$ values | Probability that $X$ lies in the interval |
|-----------------------:|------------------------------------------:|-----------------------:|------------------------------------------:|
|             (0.0, 0.1) |                                       0.1 |             (200, 260) |                          $\frac{60}{600}$ |
|             (0.9, 1.0) |                                       0.1 |             (740, 800) |                          $\frac{60}{600}$ |
|             (0.0, 0.2) |                                       0.2 |             (200, 320) |                         $\frac{120}{600}$ |

We have seen previously that the long run average value of $U$ is 0.5, and of $X$ is 500.  These two values are related through the same formula mapping $U$ to $X$ values: $500 = 200 + 600\times 0.5$.

The standard deviation of $U$ is about 0.289, and of $X$ is about 173.

```{python}

print(U.sim(10000).sd(), X.sim(10000).sd())

```

The standard deviation of $X$ is 600 times the standard deviation of $U$.  Multiplying the $U$ values by 600 rescales the distance between the values.  Two values of $U$ that are 0.1 units apart correspond to two values of $X$ that are 60 units apart.  However, adding the constant 200 to all values just shifts the distribution and does affect degree of variability.


In general, if $U$ has a Uniform(0, 1) distribution then $X = a + (b-a)U$ has a Uniform($a$, $b$) distribution.  Therefore, we can essentially use the Uniform(0, 1) distribution to simulate values from any Uniform distribution.







```{example uniform-linear}

Let $\IP$ be the probabilty space corresponding to the Uniform(0, 1) spinner and let $U$ represent the result of a single spin.  Define $V=1-U$.

```

1. Does $V$ result from a linear rescaling of $U$?
1. What are the possible values of $V$?
1. Is $V$ the same random variable as $U$?
1. Find $\IP(U \le 0.1)$ and $\IP(V \le 0.1)$.
1. Sketch a plot of what the histogram of many simulated values of $V$ would look like.
1. Does $V$ have the same distribution as $U$?

```{solution uniform-linear-sol}
to Example \@ref(exm:uniform-linear)
```

```{asis, fold.chunk = TRUE}



1. Yes, $V$ result from the linear rescaling $u\mapsto 1-u$ (intercept of 1 and slope of $-1$.)
1. $V$ takes values in the interval [0,1].
Basically, this transformation just changes the direction of the spinner from clockwise to counterclockwise.
The axis on the usual spinner has values $u$ increasing clockwise from 0 to 1.  Applying the transformation $1-u$,  the values would decrease clockwise from 1 to 0.
1. No. $V$ and $U$ are different random variables.  If the spin lands on $\omega=0.1$, then $U(\omega)=0.1$ but $V(\omega)=0.9$.  $V$ and $U$ return different values for the same outcome; they are measuring different things.
1. $\IP(U \le 0.1) = 0.1$ and $\IP(V \le 0.1)=\IP(1-U \le 0.1) = \IP(U\ge 0.9) = 0.1$.  Note, however, that these are different events: $\{U \le 0.1\}=\{0 \le \omega \le 0.1\}$ while $\{V \le 0.1\}=\{0.9 \le \omega \le 1\}$.  But each is an interval of length 0.1 so they have the same probability according to the uniform probability measure.
1. Since $V$ is a linear rescaling of $U$, the shape of the histogram of simulated values of $V$ should be the same as that for $U$.  Also, the possible values of $V$ are the same as those for $U$.  So the histograms should look identical (aside from natural simulation variability).
1. Yes, $V$ has the same distribution as $U$.  While for any single outcome (spin), the values of $V$ and $U$ will be different, over many repetitions (spins) the pattern of variation of the $V$ values, as depicted in a histogram, will be identical to that of $U$.
```

```{python}

P = Uniform(0, 1)
U = RV(P)

V = 1 - U

V.sim(10000).plot()
plt.show()

```

Let's consider a non-uniform example.  The spinner below represents a Normal distribution with mean 0 and standard deviation 1.
Technically, with a Normal distribution any value in the interval $(-\infty, \infty)$ is possible.  However, for a Normal(0, 1) distribution, the probability that a value lies outside the interval $(-3, 3)$ is small.

```{r, echo = FALSE}

knitr::include_graphics('_graphics/spinner-normal.png')

```



Let $Z$ be a random variable with the Normal(0, 1) distribution.
If we simulate many values of $Z$:

- About half of the values would be below 0 and half above
- Because axis values near 0 are stretched out, simulated values near 0 would occur with higher frequency than those near -3 or 3.
- The shape of the distribution would be symmetric about 0 since the axis spacing of values below 0 mirrors that for values above 0.
For example, about 34% of values would be between -1 and 0, and also 34% between 0 and 1.
- About 68% of values would be between -1 and 1.
- About 95% of values would be between -2 and 2.

```{python}
Z = RV(Normal(0, 1))

z = Z.sim(10000)

z.plot()
Normal(0, 1).plot() # plot the density
plt.show()
```

The long run average value of $Z$ is 0 and the standard deviation is 1.

```{python}
z.count_lt(-1) / z.count(), z.mean(), z.sd()

```


Now consider the linear rescaling $X=500 + 100 Z$.  We see that $X$ has a Normal(500, 100) distribution.

```{python}

X = 500 + 100 * Z

(Z & X).sim(10)

```

```{python}
x = X.sim(10000)

x.plot()
Normal(500, 100).plot() # plot the density
plt.show()
```


```{python}
x.mean(), x.sd()

```

The linear rescaling changes the range of observed values; almost all of the values of $Z$ lie in the interval $(-3, 3)$ while almost all of the values of $X$ lie in the interval $(200, 800)$.  However, the distribution of $X$ still has the general Normal shape.  The means are related by the conversion formula: $500 = 500 + 100 \times 0$. Multiplying the values of $Z$  by 100 rescales the distance between values; two values of $Z$ that are 1 unit apart correspond to two values of $X$ that are 100 units apart.  However, adding the constant 500 to all the values just shifts the center of the distribution and does not affect variability.  Therefore, the standard deviation of $X$ is 100 times the standard deviation of $Z$.  

In general, if $Z$ has a Normal(0, 1) distribution then $X = \mu + \sigma Z$ has a Normal($\mu$, $\sigma$) distribution.  Therefore, we can essentially use the Normal(0, 1) distribution to simulate values from any Normal distribution.


```{example normal-sat-linear}

Suppose that $X$, the SAT Math score of a randomly selected student, follows a Normal(500, 100) distribution.  Randomly select a student and let $X$ be the student's SAT Math score.  Now have the selected student spin the Normal(0, 1) spinner.  Let $Z$ be the result of the spin and let $Y=500 + 100 Z$.

```

1. Is $Y$ the same random variable as $X$?
1. Does $Y$ have the same distribution as $X$?

```{solution normal-sat-linear-sol}
to Example \@ref(exm:normal-sat-linear)
```

```{asis, fold.chunk = TRUE}
1. No, these two random variables are measuring different things.  One is measuring SAT Math score; one is measuring what comes out of a spinner.  Taking the SAT and spinning a spinner are not the same thing.
1. Yes, they do have the same distribution. Repeating the process of randomly selecting a student and measuring SAT Math score will yield values that follow a Normal(500, 100) distribution.  Repeating the process of spinning the Normal(0, 1) spinner to get $Z$ and then setting $Y=500+100Z$ will also yield values that follow a Normal(500, 100) distribution.  Even though $X$ and $Y$ are different random variables they follow the same long run pattern of variability.
```



```{example dd-normal-negative}

Let $Z$ be a random variable with a Normal(0, 1) distribution. Consider $-Z$.

```

1. Donny Don't says that the distribution of $-Z$ will look like an "upside-down bell".
Is Donny correct?
If not, explain why not and describe the distribution of $-Z$.
1. Donny Don't says that the standard deviation of $-Z$ is -1.
Is Donny correct?
If not, explain why not and determine the standard deviation of $-Z$.



```{solution dd-normal-negative-sol}
to Example \@ref(exm:dd-normal-negative)
```

```{asis, fold.chunk = TRUE}

1. Donny is confusing a random variable with its distribution.
The values of the random variable are multiplied by -1, not the heights of the histogram.
Multiplying values of $Z$ by -1 rotates the "bell" about 0 horizontally (not vertically).
Since the Normal(0, 1) distribution is symmetric about 0, the distribution of $-Z$ is the same as the distribution of $Z$, Normal(0, 1).
1. Standard deviation cannot be negative.
Standard deviation measures average distance of values from the mean.
A $Z$ value of 1 yields a $-Z$ value of -1; in either case the value is 1 unit away from the mean.
Multiplying a random variable by $-1$ simply reflects the values horizontally about 0, but does not change distance from the mean^[If the mean were not 0, multiplying a random variable by $-1$  reflects the mean about 0 too, and distances from the mean are unaffected. For example, suppose $X$ has mean 1 so a value of 5 is 4 units away from the mean of 1. Then $-X$ has mean -1; a value of $X$ of 5 yields a value of $-X$ of -5, which is 4 units away from the mean of -1.].
In general, $X$ and $-X$ have the same standard deviation.
In this example we can say more: $Z$ and $-Z$ have the same distribution so they must have the same standard deviation, 1.


```


### Summary


- A linear rescaling is a transformation of the form $g(u) = a + bu$.
- A linear rescaling of a random variable does not change the basic shape of its distribution, just the range of possible values.
    - However, remember that the possible values are part of the distribution. So a linear rescaling does technically change the distribution, even if the basic shape is the same.  (For example, Normal(500, 100) and Normal(0, 1) are two different distributions.)
- A linear rescaling transforms the mean in the same way the individual values are transformed.
- Adding a constant to a random variable does not affect its standard deviation.
- Multiplying a random variable by a constant multiplies its standard deviation by the *absolute value* of the constant. 
- Whether in the short run or the long run,
\begin{align*}
\text{Average of $a+bX$} & = a+b(\text{Average of $X$})\\
\text{SD of $a+bX$} & = |b|(\text{SD of $X$})\\
\text{Variance of $a+bX$} & = b^2(\text{Variance of $X$})
\end{align*}
- If $U$ has a Uniform(0, 1) distribution then $X = a + (b-a)U$ has a Uniform($a$, $b$) distribution.
- If $Z$ has a Normal(0, 1) distribution then $X = \mu + \sigma Z$ has a Normal($\mu$, $\sigma$) distribution.
- Remember, do NOT confuse a random variable with its distribution.
  - The random variable is the numerical quantity being measured
  - The distribution is the long run pattern of variation of many observed values of the random variable

## Nonlinear transformations of random variables {#sim-nonlinear}


The preceding section illustrated that a linear rescaling does not change the shape of a distribution, only the range of possible values.  But what about a nonlinear transformation, like a logarithmic or square root transformation?  In contrast to a linear rescaling, a nonlinear rescaling does *not* preserve relative interval length, so we might expect that a nonlinear rescaling can change the shape of a distribution.  We'll investigate by considering the Uniform(0, 1) spinner and a logarithmic^[As in many other contexts and programming languages, in this text any reference to logarithms or $\log$ refers to natural (base $e$) logarithms.  In the instances we need to consider another base, we'll make that explicit.] transformation.



Let $\IP$ be the probabilty space corresponding to the Uniform(0, 1) spinner and let $U$ represent the result of a single spin.  Attempting the transformation $\log(U)$ leads to two minor technicalities.

- Since $U\in[0, 1]$, $\log(U)\le 0$.  To obtain positive values we consider $-\log(U)$, which takes values in $[0,\infty)$.
- Technically, applying $-\log(u)$ to the values on the axis of the Uniform(0, 1) spinner, the resulting values would decrease from $\infty$ to 0 clockwise.  To make the values start at 0 and increase to $\infty$ clockwise, we consider $-\log(1-U)$. (We saw in the previous section the transformation $u \to 1-u$ basically just changes direction from clockwise to counterclockwise.)

Therefore, it's a little more convenient to consider the random variable $X=-\log(1-U)$ which takes values in $[0,\infty)$.  Remember: a transformation of a random variable is a random variable.  Also, always be sure to identify the possible values that a random variable can take.

Before proceeding, try sketching a plot of the distribution of $X$.  (Just take a guess; you'll get a chance to make a more educated sketch soon.)

The following code defines $X$ and plots a few simulated values.  

```{python}

P = Uniform(0, 1)
U = RV(P)

X = -log(1 - U)

x = X.sim(100)

x.plot('rug')
plt.show()

```


Notice that values near 0 occur with higher frequency than larger values.  For example, there are many more simulated values of $X$ that lie in the interval $[0, 1]$ than in the interval $[3, 4]$, even though these intervals both have length 1.  Let's see why this is happening before simulating many values.

```{example uniform-log-transform-calcs}

For each of the intervals in the table below find the probability that $U$ lies in the interval, and identify the corresponding values of $X$.  (You should at least compute a few by hand to see what's happening, but you can use software to fill in the rest.)

```

```{r, echo = FALSE}

u1 = seq(0, 0.9, 0.1)
u2 = seq(0.1, 1, 0.1)

x1 = -log(1-u1)
x2 = -log(1-u2)

knitr::kable(
  data.frame(paste("(", u1, ", ", u2, ")", sep=""), rep("", 10), rep("", 10), rep("", 10), rep("", 10)),
  col.names = c('Interval of U', 'Length of U interval', 'Probability', 'Interval of X', 'Length of X interval'),
  digits = c(1,  1, 1, 3, 3)
)



```



```{solution uniform-log-transform-calcs-sol}
to Example \@ref(exm:uniform-log-transform-calcs)
```

Plug the endpoints into the conversion formula $u\mapsto -\log(1-u)$ to find the corresponding $X$ interval.  For example, the $U$ interval $(0.1, 0.2)$ corresponds to the $X$ interval $(-\log(1-0.1), -\log(1-0.2))$.  Since $U$ has a Uniform(0, 1) distribution the probability is just the length of the $U$ interval.

```{r, echo = FALSE}

u1 = seq(0, 0.9, 0.1)
u2 = seq(0.1, 1, 0.1)

x1 = -log(1-u1)
x2 = -log(1-u2)

knitr::kable(
  data.frame(paste("(", u1, ", ", u2, ")", sep=""), u2 - u1, u2-u1, paste("(", round(x1, 3), ", ", round(x2, 3), ")", sep=""), x2-x1),
  col.names = c('Interval of U', 'Length of U interval', 'Probability', 'Interval of X', 'Length of X interval'),
  digits = c(1,  1, 1, 3, 3)
)



```

We see that the logarithmic transformation does not preserve relative interval length.  Each of the original intervals of $U$ values has the same length, but the nonlinear logarithmic transformation "stretches out" these intervals in different ways.  The probability that $U$ lies in each of these intervals is 0.1.  As the transformation stretches the intervals, the 0.1 probability gets "spread" over different lengths of values.  Since probability/relative frequency is represented by area in a histogram, if two regions of differing length have the same area, then they must have different heights.    Thus the shape of the distribution of $X$ will not be Uniform.

The following example provides a similar illustration, but from the reverse perspective.


```{example uniform-log-transform-calcs2}

For each of the intervals of $X$ values in the table below identify the corresponding values of $U$, and then find the probability that $X$ lies in the interval.   (You should at least compute a few by hand to see what's happening, but you can use software to fill in the rest.)

```

```{r, echo = FALSE}

x1 = seq(0, 4.5, 0.5)
x2 = seq(0.5, 5, 0.5)

u1 = 1 - exp(-x1)
u2 = 1 - exp(-x2)

knitr::kable(
  data.frame(paste("(", x1, ", ", x2, ")", sep=""), rep("", 10), rep("", 10), rep("", 10), rep("", 10)),
  col.names = c('Interval of X', 'Length of X interval', 'Probability', 'Interval of U', 'Length of U interval'),
  digits = c(1,  1, 3, 3, 3)
)



```



```{solution uniform-log-transform-calcs2-sol}
to Example \@ref(exm:uniform-log-transform-calcs2)
```

The corresponding $U$ intervals are obtained by applying the inverse transformation $v\mapsto 1-e^{-v}$.  For example, the $X$ interval $(0.5, 1)$ corresponds to the $U$ interval $(1-e^{-0.5}, 1-e^{-1})$.

```{r, echo = FALSE}

x1 = seq(0, 4.5, 0.5)
x2 = seq(0.5, 5, 0.5)

u1 = 1 - exp(-x1)
u2 = 1 - exp(-x2)

knitr::kable(
  data.frame(paste("(", x1, ", ", x2, ")", sep=""), x2 - x1, u2-u1, paste("(", round(u1, 3), ", ", round(u2, 3), ")", sep=""), u2-u1),
  col.names = c('Interval of X', 'Length of X interval', 'Probability', 'Interval of U', 'Length of U interval'),
  digits = c(1,  1, 3, 3, 3)
)



```




Since $U$ has a Uniform(0, 1) distribution the probability is just the length of the $U$ interval. Each of the $X$ intervals has the same length but they correspond to intervals of differing length in the original $U$ scale, and hence intervals of different probability.  


Now we simulate many values of $X$ and summarize the results in a histogram.  But before proceeding, try again to sketch a plot of the distribution of $X$.  (You  should be able to make a much more educated sketch now.)


(ref:cap-log-uniform-density) Histogram representing the approximate distribution of $X=-\log(1-U)$, where $U$ has a Uniform(0, 1) distribution.  The smooth solid curve models the theoretical shape of the distribution of $X$, known as the Exponential(1) distribution.

```{python log-uniform-density, fig.cap="(ref:cap-log-uniform-density)"}

X.sim(10000).plot()
Exponential(1).plot() # overlays the smooth curve
plt.show()

```


<!-- It should be clear that the simulated values of $X$ do not follow a Uniform distribution.  Values near 0 occur with greater frequency than larger values.  The non-linear log transformation changes the shape of the distribution. -->

<!-- To get some intuition behind why the shape changes, consider the following illustration. Consider intervals in increments of 0.1, starting from 0, on the original [0, 1] scale.  These intervals each have length 0.1 and so each have probability 0.1 according to the uniform probability measure.  Now consider the corresponding transformed intervals. -->

<!-- - [0, 0.1] maps to^[Each of these values is obtained from the transformation $u\mapsto-\log(1-u)$, e.g. $-\log(1-0.1)\approx 0.105$.] [0, 0.105], an interval of length 0.105. -->
<!-- - [0.1, 0.2] maps to [0.105, 0.223], an interval of length 0.118. -->
<!-- - [0.2, 0.3] maps to [0.223, 0.357], an interval of length 0.134. -->
<!-- - [0.3, 0.4] maps to [0.357, 0.511], an interval of length 0.154. -->
<!-- - [0.4, 0.5] maps to [0.511, 0.693], an interval of length 0.182, and so on. -->


<!-- - [0, 1] corresponds to^[Each of these values is obtained by applying the inverse transformation $u\mapsto 1-e^{-u}$, e.g. $1-e^{-1}\approx 0.632$] [0, 0.632], and interval with probability 0.632. -->
<!-- - [1, 2] corresponds to [0.632, 0.865], and interval with probability 0.233. -->
<!-- - [2, 3] corresponds to [0.865, 0.950], and interval with probability 0.086. -->






<!-- Notice that the shape of the histogram depicting the simulated values of $X$ appears that it can be approximated by a smooth curve.  This smooth curve is an idealized model of what would happen in the long run if -->

<!--   - we kept simulating more and more values, and -->
<!--   - made the histogram bin widths smaller and smaller. -->

<!-- The following plot illustrates the results of 100,000 simulated values of $X$ summarized in a histogram with 1000 bins.  -->

Notice that the shape of the histogram depicting the simulated values of $X$ appears that it can be approximated by a smooth curve.  This smooth curve is called the Exponential(1) density.  We will see more properties of Exponential distributions later.


The following plots illustrate the results of Example \@ref(exm:uniform-log-transform-calcs) (plots on the left) and Example \@ref(exm:uniform-log-transform-calcs2) (plots on the right), and give some insight into the shape of the distribution in Figure \@ref(fig:log-uniform-density).


```{r, echo = FALSE, fig.show="hold", out.width="50%"}

set.seed(1)

u = runif(10000)

ubreaks = seq(0, 1, 0.1)

par(mfrow=c(2, 1))
hist(u, breaks = ubreaks, xlab = "u", xaxt='n', freq = FALSE, main = "U ~ Uniform(0, 1)")
axis(1, ubreaks)

x = -log(1 - u)

xbreaks = -log(1 - pmin(ubreaks, 0.99999))
hist(x, breaks = xbreaks, xlab = "x", xaxt='n', freq = FALSE, main = "X = -log(1 - U) where U ~ Uniform(0, 1)")
axis(1, round(xbreaks, 2))

# Version 2

par(mfrow=c(2, 1))

xbreaks = c(seq(0, 5, 0.5), 15)

hist(x, breaks = xbreaks, xlab = "x", xaxt='n', freq = FALSE, main = "X = -log(1 - U) where U ~ Uniform(0, 1)")
axis(1, round(xbreaks, 2))


ubreaks = 1 - exp(-xbreaks)

hist(u, breaks = ubreaks, xlab = "u", xaxt='n', freq = FALSE, main = "U ~ Uniform(0, 1)")
axis(1, round(ubreaks, 3))

```


For a linear rescaling, we could just plug the mean of the original variable into the conversion formula to find the mean of the transformed variable.  However, this will not work for nonlinear transformations.

```{python}

(U & X).sim(10000).mean()

```


We see that the average value of $U$ is about 0.5, the average value of $X$ is about 1, and $-\log(1 - 0.5) \neq 1$.  The nonlinear "stretching" of the axis makes some value relatively larger and others relatively smaller than they were on the original scale, which influences the average. Remember, in general: Average of $g(X)$ $\neq$ $g$(Average of $X$).

What about a spinner which generates values according to the distribution in Figure \@ref(fig:log-uniform-density)?  The "simulate from the probability space" method for simulating of $X$ values entailed

- Spinning the Uniform(0, 1) spinner to get a value $U$
- Setting $X=-\log(1-U)$

These two steps can be combined by relabeling the values on the axis of the spinner according to the transformation $u\mapsto -\log(1-u)$.  For example, replace 0.1 by $-\log(1-0.1)\approx 0.105$; replace 0.9 by $-\log(1-0.9)\approx 2.30$.  This transformation results in the spinner in Figure \@ref(fig:exponential-spinner).

(ref:cap-exponential-spinner) A spinner representing the distribution in Figure \@ref(fig:log-uniform-density) (the "Exponential(1)" distribution.).  The spinner is duplicated on the right; the highlighted sectors illustrate the non-linearity of axis values and how this translates to non-uniform probabilities.

```{r exponential-spinner, echo=FALSE, fig.cap="(ref:cap-exponential-spinner)", out.width='50%', fig.show='hold'}

knitr::include_graphics(c("_graphics/exponential-spinner.png", "_graphics/exponential-spinner-sectors.png"))

```


Pay special attention to the values on the axis; they do not increase in equal increments.  (As with the Uniform(0, 1) spinner, while only certain values are marked on the axis, we consider an idealized model in which any value in the continuous interval $[0, \infty)$ is a possible result of the spin.) The spinner on the right in Figure \@ref(fig:exponential-spinner) is the same as the one on the left, with the intervals [0, 1], [1, 2], and [2, 3] highlighted with their respective probabilities.  Putting a needle on this spinner that is "equally likely" to land anywhere on the axis, the needle will land in the interval [0, 1] with probability 0.632, in the interval [1, 2] with probability 0.233, etc.  Therefore, values generated using this spinner, which represents the "Exponential(1)" distribution, will follow the pattern in Figure \@ref(fig:log-uniform-density).  Figure \@ref(fig:exponential-simulation) illustrates this "simulate from a distribution" method; values of $X$ are generated directly from an Exponential(1) distribution, rather than first generating $U$ and then transforming.





(ref:cap-exponential-simulation) Simulated values from an Exponential(1) distribution, correspoding to the results of many spins of the spinner in Figure \@ref(fig:log-uniform-density).

```{python exponential-simulation, fig.cap="(ref:cap-exponential-simulation)"}

X = RV(Exponential(1))

X.sim(10000).plot()
Exponential(1).plot()
plt.show()

```



### Summary

- Remember: a transformation of a random variable, both mathematically and in Symbulate.
- Be sure to always specify the possible values a random variable can take.
- A nonlinear transformation of a random variable changes the shape of its distribution (as well as the possible values).
- The shape of the histogram of simulated continuous values can be approximated by a smooth curve.
- Spinners can be used to generate values from non-uniform distributions by applying nonlinear transformations to values on the spinner axis.
- In general, Average of $g(X)$ $\neq$ $g$(Average of $X$)


<!-- ### Continuous analog of rolling two dice {#uniform-sum-max} -->

## Transformations of multiple random variables {#sim-transform-joint}

Earlier in this chapter we studied the joint distribution of the sum and max of two fair-four sided dice rolls.  Now we consider a continuous analog.  Instead of rolling a die which is equally likely to take the values 1, 2, 3, 4, we spin a Uniform(1, 4) spinner that lands uniformly in the continuous interval $[1, 4]$.   Let $\IP$ be the probability space corresponding to two spins of the Uniform(1, 4) spinner, and let $X$ be the sum of the two spins, and $Y$ the larger spin (or the common value if a tie).  We saw that in Section \@ref(symbulate-discrete-uniform), we could model two rolls of a fair-four sided die using `DiscreteUniform(1, 4) ** 2`.  Similarly, we can model two spins of the Uniform(1, 4) spinner with `Uniform(1, 4) ** 2`.

We start by looking at the joint distribution of the two spins,  $(U_1, U_2)$, which take values in $[1, 4]\times[1, 4]$.

```{python}

P = Uniform(1, 4) ** 2
U1, U2 = RV(P)

u1u2 = (U1 & U2).sim(100)

```


```{python}

u1u2.plot()
plt.show()

```

We see that the $(U_1, U_2)$ pairs are roughly "evenly spread" throughout $[1, 4]\times [1, 4]$.  The scatterplot displays each individual pair.  We can summarize the distribution  of many pairs with a two-dimensional histogram.  To construct the histogram, the space of values $[1, 4]\times[1, 4]$ is chopped into rectangular bins and the relative frequency of pairs which fall within each bin is computed. In a histogram of a single variable, area represents relative frequency; in a histogram of two variables, volume represents relative frequency, with the height of each rectangular bin on a "density" scale represented by its color intensity.

```{python, warning = FALSE, error = TRUE, message = FALSE}

(U1 & U2).sim(10000).plot('hist')
plt.show()

```

Now we let $X$ be the sum and $Y$ the max of the two spins^[Remember that a probability space outcome corresponds to the pair of spins, so we can define random variables on this space as we have done.  We could also first define random variables `U1, U2 = RV(P)` corresponding to the individual spins, and then define the sum as `X = U1 + U2`.  For technical reasons the syntax for `max` is a little different: `Y = (U1 & U2).apply(max)`.].  First consider the possible values of $(X, Y)$. Marginally, $X$ takes values in $[2, 8]$ and $Y$ takes values in $[1, 4]$.  However, not every value in $[2, 8]\times [1, 4]$ is possible.  Before proceeding,  sketch a picture representing the possible values of $(X, Y)$ pairs.

- We must have  $Y \ge 0.5 X$, or equivalently, $X \le 2Y$. For example, if $X=4$ then $Y$ must be at least 2, because if the larger of the two spins were less than 2, then both spins must be less than 2, and the sum must be less than 4.
- We must have $Y \le X - 1$, or equivalently, $X \ge Y + 1$. For example, if $Y=3$, then one of the spins is 3 and the other one is at least 1, so the sum must be at least 4.

Therefore, the possible values of $(X, Y)$ lie in the set
\[
\{(x, y): 2\le x\le 8, 1 \le y\le 4, 0.5x \le y \le x-1\}
\]
which can also be written as $\{(x, y): 2\le x \le 8, 0.5 x\le y \le \min(4, x-1)\}$.  This set is represented by the triangular region in the plots below.

```{python}

P = Uniform(1, 4) ** 2

U = RV(P)
X = RV(P, sum)
Y = RV(P, max)

(U & X & Y).sim(100)

```

```{python}

(X & Y).sim(100).plot()
plt.show()

```


```{python, warning = FALSE, error = TRUE, message = FALSE}


(X & Y).sim(10000).plot('hist')
plt.show()

```

Compare the joint histogram above to the tile plot in Section \@ref(sym-joint).  In the dice rolling situation there are basically two cases.  Each $(X, Y)$ pair that correspond to a tie --- that is each $(X, Y)$ pair with $X = 2Y$ --- has probability 1/16.  Each of the other possible $(X, Y)$ pairs has probability 2/16. 

Back to the continuous analog, the histogram shows that $(X, Y)$ pairs are roughly uniformly distributed within the triangular region of possible values.  Consider a single $(X, Y)$ pair, say (0.8, 0.5).  There are two outcomes --- that is, pairs of spins --- for which $X=0.8, Y=0.5$, namely (0.5, 0.3) and (0.3, 0.5).  Like (0.8, 0.5), most of the possible $(X, Y)$ values correspond to exactly two outcomes.  The only ones that do not are the values with $Y = 0.5X$ that lie along the south/east border of the triangular region. The pairs $(X, 0.5X)$ only correspond to exactly one outcome.  For example, the only outcome corresponding to (6, 3) is the $(U_1, U_2)$ pair (3, 3); that is, the only way to have $X=6$ and $Y=3$ is to spin 3 on both spins.  In general, the event $\{Y = 0.5X\}$ is the same as the event that both spins are exactly the same, $\{U_1=U_2\}$. However, as discussed in Section \@ref(non-uniform-prob-measure), the probability that $U_1=U_2$ exactly is 0.  Therefore, we don't really need to worry about the ties as we did in the discrete case.  Excluding ties, roughly, each pair in the triangular region of possible $(X, Y)$ pairs corresponds to exactly two outcomes (pairs of spins), and since the outcomes are uniformly distributed (over $[1, 4]\times[1, 4]$) then the $(X, Y)$ pairs are also uniformly distributed (over the triangular region of possible values).

The plot below represents the joint distribution of $(X, Y)$.  This is really a three-dimensional plot.  The base is the triangular region which represents the possible $(X, Y)$ pairs.  There is a surface floating above this region which represents the density at each point.  For a single variable, the density is a smooth curve approximating the idealized shape of the histogram.  Likewise, for two variables, the density is a smooth surface approximating the idealized shape of the joint histogram. The height of this surface is depicted in the two-dimensional plot via the color intensity.  Since the $(X, Y)$ pairs are uniformly distributed over their range of possible values, the height of the surface and hence the color intensity is constant over the range of possible values, and the height is 0 (white) for impossible $(X, Y)$ pairs.  Careful: this plot is not the same as the ones in Section \@ref(non-uniform-prob-measure).  Those plots were just depicting events, and the color was just used to shade the region of interest. The plot below is depicting a joint distribution, and the color represents the height of the density surface at each $(X, Y)$ pair; white areas correspond to a height of 0.

(ref:cap-dice-continuous-sum-max-joint) Joint distribution of $X$ (sum) and $Y$ (max) of two spins of the Uniform(1, 4) spinner.  The triangular region represents the possible values of $(X, Y)$ the height of the density surface is constant over this region and 0 outside of the region.

```{r dice-continuous-sum-max-joint, echo = FALSE, fig.cap="(ref:cap-dice-continuous-sum-max-joint)"}

dfA <- data.frame(x = c(2, 8, 5, 2),
                 y = c(1, 4, 4, 1),
                 v = c(1, 1, 1, 1))

pA <- ggplot(data = dfA, aes(x = x, y = y)) +
  geom_polygon(fill="cornflowerblue", show.legend = FALSE) +
  scale_x_continuous(limits = c(2, 8), expand = c(0, 0)) + 
  scale_y_continuous(limits = c(1, 4), expand = c(0, 0)) +
  theme_classic() +
  theme(panel.border = element_rect(linetype = "solid", fill = NA)) +
  theme(plot.margin=unit(c(1,1,1,1),"cm")) +
  xlab(expression("X")) +
  ylab(expression("Y")) +
  theme(plot.title = element_text(hjust = 0.5))

plot(pA)

```


We now consider the marginal distributions of $X$ and $Y$.  Before proceeding, try to sketch the marginal distributions.


Here is a plot showing the joint histogram representing the joint distribution of $(X, Y)$, along with histograms representing each of the marginal distributions.

```{python, warning = FALSE, error = TRUE, message = FALSE}

(X & Y).sim(10000).plot(['hist', 'marginal'])
plt.show()

```

Let's look a little more closely at the marginal distribution of $X$.

(ref:cap-dice-continuous-sum-marginal) Histogram representing the marginal distribution of the sum ($X$) of two spins of the Uniform(1, 4) spinner.

```{python dice-continuous-sum-marginal, echo = FALSE, fig.cap="(ref:cap-dice-continuous-sum-marginal)"}

X.sim(10000).plot()
plt.show()

```


The marginal distribution of $X$ has highest density near 5 and lowest density near 2 and 8.  Intuitively, there is only one pair of spins --- (1, 1) --- for which the sum is 2; similarly for a sum of 8.  But there are many pairs for which the sum is 5: (2.5, 2.5), (3, 2), (2, 3), (1.2, 2.8), etc.  Recall that for the dice rolls, we could obtain the marginal distribution of $X$ by summing the joint distribution over all $Y$ values.  Similarly, we can find the marginal density of $X$ by aggregating over all possible values of $Y$.  For each possible value of $X$, "collapse" the joint histogram vertically over all possible values of $Y$.  Imagine that within the region of possible $(X, Y)$ pairs, the joint histogram is composed of stacks of blocks, one for each bin, each stack of the same height (because the values are uniformly distributed over the triangular region).  To get the marginal density for a particular $x$, take all the stacks corresponding to that $x$, for different values of $y$, and stack them on top of one another.  There will be the most stacks for $x$ values near 5  and the fewest stacks for $x$ values near 2 or 8.  In other words, the aggregated density along "vertical strips" is largest for the vertical strip for $x=5$.
(In this case, the joint distribution is uniform over the range of possible pairs, so the stacks all have the same height.  That won't be true in general, so the marginal distributions will depend both on the number of and the height of the "stacks".)



Similarly reasoning applies to find the marginal distribution of $Y$.  Now we find the marginal density for a particular $y$ value by collapsing/stacking the histogram horizontally over all possible value of $X$. We see that the density increases with values of $y$.  Intuitively, there is only one pair of spins, (1, 1), for which $Y=1$, but many pairs of spins for which $Y=4$, e.g., (1, 4), (4, 1), (4, 2), (2.5, 4), etc.


(ref:cap-dice-continuous-max-marginal) Histogram representing the marginal distribution of the larger ($Y$) of two spins of the Uniform(1, 4) spinner.

```{python dice-continuous-max-marginal, echo = FALSE, fig.cap="(ref:cap-dice-continuous-max-marginal)"}

Y.sim(10000).plot()
plt.show()

```

What about the long run averages?  The sums of the two spins is $X= U_1 + U_2$. The long run average of each of $U_1$ and $U_2$ is 2.5 (the midpoint of the interval [1, 4]).  We can see from its marginal distribution that the long run average of $X$ is 5.  Therefore, the average of the sum is the sum of averages.

```{python}

X.sim(10000).mean()

```


However, the average of $Y=\max(U_1, U_2)$ is 3, which is not $\max(2.5, 2.5)$.  Therefore, the average of the maximum is not the maximum of the averages.  Remember that in general, Average of $g(X, Y)$ $\neq$ $g$(Average of $X$, Average of $Y$).

```{python}

Y.sim(10000).mean()

```



Finally, observe that the plots in this section look like continuous versions of the plots for the dice rolling example earlier in the chapter. However, it took a little more work in this section to think about what the joint or marginal distributions might look like.  When studying continuous random variables, it is often helpful to think about how a discrete analog behaves.


```{python, eval = FALSE, include = FALSE,  echo=FALSE, out.width='50%', fig.show='hold'}

P = DiscreteUniform(1, 4) ** 2
X = RV(P, sum)
Y = RV(P, max)

(X & Y).sim(10000).plot(['tile', 'marginal'])
plt.show()

P = Uniform(1, 4) ** 2
X = RV(P, sum)
Y = RV(P, max)

(X & Y).sim(10000).plot(['hist', 'marginal'])
plt.show()

```



### Summary

- The joint distribution of values on a continuous scale can be visualized in a joint histogram.
- Remember to always identify possible values of random variables, including possible pairs in a joint distribution.
- The marginal distribution of a single random variable can be obtained from a joint distribution by aggregating/collapsing/stacking over the values of the other random variables.
- The average of a sum is the sum of the averages. Whether in the short run or the long run,
\begin{align*}
\text{Average of $X+Y$} & = \text{Average of $X$} +\text{Average of $Y$}
\end{align*}
- Whether in the short run or the long run, in general: Average of $g(X, Y)$ $\neq$ $g$(Average of $X$, Average of $Y$).
- When studying continuous random variables, it is often helpful to think about how a discrete analog behaves.

### Exercises

```{exercise}
Spin the Uniform(0, 1) spinner twice and let $U_1$ and $U_2$ be the result of the two spins.  Each of the following random variables takes values in the interval (-1, 1). (You should verify this.)

- \(V = 2 U_1 - 1\).
- \(W = 2U_1^2 - 1\). 
- \(U_1^2\) the square of \(U_1\).
- \(X = U_1 - U_2\).
- \(Y = 2\max(U_1, U_2) - 1\).
- \(Z = (2 U_1 - 1)^{1/3}\), where the cube root of a negative number is defined to be negative, e.g. \((-1/8)^{1/3} = -1/2\); \((-1)^{1/3} = -1\).


Match each of the random variables above with the feature below that best describes its distribution.
Each feature will be used exactly once.

- Density is uniform over $(-1, 1)$.
- Density is highest at -1 and lowest at 1.
- Density is highest at 0 and lowest at -1 and 1.
- Density is highest at 1 and lowest at -1.
- Density is highest at 1 and -1 and lowest at 0.

Hint: It helps to sketch plots and work out what happens for a few example intervals (e.g. (0, 0.1), (0.1, 0.2)).
You can also use simulation to see what happens, but try sketching a plot first to practice your understanding.
```



## Joint Normal Distributions {#sec-example-sat-both}



In previous sections we considered randomly selecting an SAT taker and measuring their Math score.  In particular, we assumed that SAT Math scores follow a Normal distribution with a mean of 500 and a standard deviation of 100.

Now consider randomly selecting an SAT taker and recording both their Math and Reading score.  Suppose we want to conduct an appropriate simulation.

```{example dd-bvn-spinner}

Donny Don't says: "That's easy; just spin the SAT spinner twice, once for Math and once for Reading."
Do you agree?
Explain your reasoning.

```


```{solution dd-bvn-spinner-sol}
to Example \@ref(exm:dd-bvn-spinner)
```

```{asis, fold.chunk = TRUE}

You should not agree with Donny, for two reasons.

- It's possible that the distribution of SAT Math scores follow a different pattern than SAT Reading scores.  So we might need one spinner to simulate a Math score, and a second spinner to simulate the Reading score.  (In reality, SAT Math and Reading scores do follow pretty similar distributions.  But it's possible that they could follow different distributions.)
- Furthermore, there is probably some relationship between scores.  It is plausible that students who do well on one test tend to do well on the other.  For example, students who score over 700 on Math are probably more likely to score above than below average on Reading.  If we simulate a pair of scores by spinning one spinner for Math and a separate spinner for Reading, then there will be no relationship between the scores because the spins are physically independent.

```

What we really need is a spinner that generates a pair of scores simultaneously to reflect their association.  This is a little harder to visualize, but we could imagine spinning a "globe" with lines of latitude corresponding to SAT Math score and lines of longitutde to SAT Reading score.  But this would not be a typical globe:

- The lines of latitude would not be equally spaced, since SAT Math scores are not equally likely.  (We have seen similar issues for one-dimensional spinners like the one in Figure \@ref(fig:sat-normal-spinner) with unequally spaced values around the outside.) Similarly for lines of longitude.
- The scale of the lines of latitude would not necessarily match the scale of the lines of longitude, since Math and Reading scores could follow difference distributions.  For example, the equator (average Math) might be 500 while the prime meridian (average Reading) might be 520.
- The "lines" would be tilted or squiggled to reflect the relationship between the scores.  For example, the region corresponding to Math scores near 700 and Reading scores near 700 would be larger than the region corresponding to Math scores near 700 but Reading scores near 200. 

So we would like a model that

- Simulates Math scores that follow a Normal distribution pattern, with some mean and some standard deviation.
- Simulates Reading scores that follow a Normal distribution pattern, with possibly a different mean and standard deviation.
- Reflects how strongly the scores are associated.

Such a model is called a "Bivariate Normal" distribution.  There are five parameters: the two means, the two standard deviations, and the *correlation* which reflects the strength of the association between the two scores.  Correlation is a number between $-1$ and $1$ that measures the degree of association, with correlation values closer to $1$ or $-1$ denoting the strongest association^[We will see later that correlation measures the strength of a *linear* association: the degree to which pairs of values of the two random variables tend to follow a straight line.].  We will study correlation in more detail later.

In Symbulate, a `BivariateNormal` probability space returns a pair of values; we let $X$ be the first coordinate (Math) and $Y$ the second (Reading).  We'll assume, as [suggested by this site](https://blog.prepscholar.com/sat-standard-deviation#targetText=Standard%20deviation%20tells%20you%2C%20on,either%20above%20or%20below%20it), that Math scores have mean 527 and standard deviation 107, Reading scores have mean 533 and standard deviation 100, and the pairs of scores have correlation 0.77.


```{python}
P = BivariateNormal(mean1 = 527, mean2 = 533, sd1 = 107, sd2 = 100, corr = 0.77)

X, Y = RV(P)

xy = (X & Y).sim(100)

xy

```


```{python}

xy.plot()
plt.show()


```


Notice the strong positive association; students who have high scores on one exam tend to have high scores on the other.  We can simulate lots of values and construct a joint histogram.

```{python, warning = FALSE, error = TRUE, message = FALSE}

(X & Y).sim(10000).plot('hist')
plt.show() 

```

Recall that in some of the previous examples the shapes of one-dimensional histograms could be approximated with a smooth density curve.   Similarly, a two-dimensional histogram can sometimes be approximated with a smooth density surface.  As with histograms, the height of the density surface at a particular $(X, Y)$ pair of values can be represented by color intensity.  A Normal distribution is a "bell-shaped curve"; a Bivariate Normal distribution is a "mound-shaped" curve --- imagine a pile of sand.  (Symbulate does not yet have the capability to display densities in a three-dimensional-like plot such as [this plot](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#/media/File:Multivariate_Gaussian.png).)

```{python}

(X & Y).sim(10000).plot('density')
plt.show() 

```


We can find marginal distributions by "aggregating/stacking/collapsing".  The SAT Math scores follow a Normal distribution with mean 527 and standard deviation 107, similarly for Reading.

```{python}

X.sim(10000).plot()
plt.show()

```


The value of correlation measures the strength of the association.  For example, with a correlation of 0.4 the association would not be nearly as strong.

```{python, warning = FALSE, error = TRUE, message = FALSE}

P = BivariateNormal(mean1 = 527, mean2 = 533, sd1 = 107, sd2 = 100, corr = 0.40)

X, Y = RV(P)

xy = (X & Y).sim(10000)

xy.plot('hist')
plt.show()

```



A negative correlation represents a negative association: large values of one variable tend to be associated with small values of the other.  (This would not be realistic for SAT scores.)


```{python, warning = FALSE, error = TRUE, message = FALSE}

P = BivariateNormal(mean1 = 527, mean2 = 533, sd1 = 107, sd2 = 100, corr = -0.77)

X, Y = RV(P)

xy = (X & Y).sim(10000)

xy.plot('hist')
plt.show()

```

Note that in all of the above cases, the marginal distribution of Math scores is the same; likewise for Reading scores.  But different correlations lead to different joint distributions.  Remember: it is not possible to simulate $(X, Y)$ pairs simply from the marginal distributions.


**Some lessons from this example.**

- "Mound-shaped" Bivariate Normal distributions are the two-dimensional analogs of Normal distributions.
- Correlation is a measure of the strength of the association between two random variables.
- Remember: it is not possible to simulate $(X, Y)$ pairs simply from the marginal distributions.



## One ~~ring~~ spinner to rule them all? {#univeral-spinner}

In the examples in this section we used different spinners to represent different distributions.  However, all of the examples assumed the same generic spinner: the needle was infinitely precise and "equally likely" to land on any value on the axis around the spinner. We modeled different distributions simply by changing the values on the axis.

Consider the standard continuous spinner in Figure \@ref(fig:uniform-spinner), corresponding to a Uniform(0, 1) distribution.  By relabeling the axes on this spinner, we could have constructed the spinners for any of the other examples.

For example, to obtain the spinner in the middle of Figure \@ref(fig:die-three-spinners), corresponding to a weighted four-sided die, start with the Uniform(0, 1) spinner and map

- The range (0, 0.1] to 1,
- The range (0.1, 0.3] to 2,
- The range (0.3, 0.6] to 3,
- The range (0.6, 1] to 4

Then the probability that the Uniform(0, 1) spinner lands in the range (0.3, 0.6] is 0.3, so the spinner resulting from this mapping would return a value of 3 with probability 0.3.  (The probability of the infinitely precise needle landing on a specific value like 0.3 (that is, $0.300000000\ldots$) is 0, so it doesn't really matter what we do with the endpoints of the intervals.)



For non-uniform values on a continuous scale, we could construct a spinner according to the distribution of interest by rescaling and stretching/shrinking the axis of the Uniform(0, 1) spinner to correspond to intervals of larger/smaller probability.  For example, if we want to simulate values according to the distribution illustrated in Figure \@ref(fig:log-uniform-density) we could start with the Uniform(0, 1) spinner and then transform the axis values $u \mapsto -\log(1-u)$ to obtain the spinner in Figure \@ref(fig:exponential-spinner).  As discussed in Section \@ref(sim-nonlinear), the spinner in Figure \@ref(fig:exponential-spinner) generates values which follow the distribution is Figure \@ref(fig:log-uniform-density).

In Section \@ref(sim-nonlinear) we started with the transformation $u\mapsto -\log(1-u)$ of the Uniform(0, 1) spinner and saw what distribution the transformed values followed via simulation.  But what about the reverse question: given a particular distribution, how do we find the transformation of Uniform(0, 1) that will generate values according to the specified distribution?  We will return to this question later.

The only example in this section where a Uniform(0, 1) spinner could not be used was the SAT example in Section \@ref(sec-example-sat-both), where we described a "globe" for simulating values.  However, we will see later that we actually can use a Uniform(0, 1) to generate a pair of SAT scores, but we will need to suitably transform the results of *two* spins. 


Through the examples in this chapter we have seen that, in principle, we can start with a Uniform(0, 1) spinner and via a suitable transformation of the axis --- and possibly multiple spins --- generate values according to any distribution of interest.  This is the idea behind what is sometimes referred to as "universality of the uniform", or what we like to call "one spinner to rule them all", and we will explore it further later.




